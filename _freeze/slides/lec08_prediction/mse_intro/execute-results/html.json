{
  "hash": "68444cade1f25708b59ea63eff8e4e05",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mean Squared Error: In and Out of Sample\"\nformat:\n  revealjs: default # For the web slides\n  #beamer: default      # To generate the PDF version\nexecute:\n  echo: false\n---\n\n\n::: {.cell}\n\n:::\n\n\n## \n\nReview of key concepts\n\n## \n\nUse data to learn a model. What does that mean?\n\n##\n\nBegin with some data. Assume a linear model.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## \n\nEstimate a linear model\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = simulated)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.138289 -0.069877  0.006674  0.056056  0.203069 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.18696    0.03969    4.71 0.000175 ***\nx            0.12455    0.01688    7.38 7.58e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09133 on 18 degrees of freedom\nMultiple R-squared:  0.7516,\tAdjusted R-squared:  0.7378 \nF-statistic: 54.47 on 1 and 18 DF,  p-value: 7.578e-07\n```\n\n\n:::\n:::\n\n\n##\n\nEstimate a linear model\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## \n\nThe model **learned** coefficient values from the data.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n##\n\nHow? It learned by minimizing the **sum of squared errors**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n##\n\nHow? It learned by minimizing the **sum of squared errors**\n\n$$\n\\begin{aligned}\n\\text{Sum of squared error:}\\qquad &\\sum_{i\\in\\text{learning}} \\left(y_i - \\hat{y}_i\\right) ^ 2 \\\\\n\\text{Mean squared error:}\\qquad &\\frac{1}{n_\\text{Learning}}\\sum_{i\\in\\text{learning}} \\left(y_i - \\hat{y}_i\\right)\n\\end{aligned}\n$$\n\n## \n\n* We estimated the line in `learning` data\n* Now we evaluate the estimated line in `testing` data\n\n##\n\n**Learning** data (used to estimate the line)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n##\n\n**Testing** data (used to evaluate the already-learned line)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## \n\nWhen will prediction errors in learning and testing data differ?\n\n##\n\nTrue model:\n\n$$\n\\text{E}\\left(Y\\mid\\vec{X}\\right) = X_1\\beta_1 + X_2\\beta_2 + ... + X_{10}\\beta_{10}\n$$\nwith $\\beta_1 = .9$, $\\beta_2 = 0.8$, \\dots, $\\beta_9 = 0.1$, $\\beta_{10} = 0$.\n\nWe observe $n = 300$ cases.\n\n## \n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n##\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n## Recap: Surprising facts\n\n* An estimated model picks up both\n     * **Signal.** True patterns linking $\\vec{X}$ to $Y$\n     * **Noise.** Random patterns particular to the learning data.\n     \n## Recap: Surprising facts\n\n* As you add predictors to the model\n     * error in learning data goes down\n     * error in testing data may go up\n\n**Why?** With many predictors, the noise may dominate the signal.\n\n## A use case for train and test: **Tuning parameters**\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_sample_split <- read_csv(\"https://soc114.github.io/assets/for_sample_split.csv\")\n```\n:::\n\n\n. . .\n\n* Predictors `x1` through `x100`\n* Outcome `y`\n* $n$ = 300 cases\n\n## A use case for train and test: **Tuning parameters**\n\nRecall penalized (ridge) regression. Chooses $\\vec\\beta$ to minimize\n\n$$\n\\begin{aligned}\n\\underbrace{\\sum_i\\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda \\sum_{j} \\beta_j^2}_\\text{Penalty Term}\n\\end{aligned}\n$$\n\nBut how to choose $\\lambda$?\n\n## A use case for train and test: **Tuning parameters**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\nX <- model.matrix(y ~ ., data = for_sample_split)\ny <- for_sample_split |> pull(y)\npenalized_regression <- cv.glmnet(x = X, y = y, alpha = 0)\n```\n:::\n\n\n## A use case for train and test: **Tuning parameters**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(penalized_regression)\n```\n\n::: {.cell-output-display}\n![](mse_intro_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n## Recap: Data-driven model selection\n\nWhen there are many candidate models, you can choose the one with the lowest out-of-sample mean squared error.\n\n\n\n",
    "supporting": [
      "mse_intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}