{
  "hash": "db1fe0a32808e3fc64dfc89a36387a1c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Confidence Intervals\"\n---\n\n> Topic for 1/14.\n\n> Here are [slides](../slides/bootstrap/bootstrap.pdf). Notation and ideas on this page loosely draw on [Efron \\& Hastie (2016)](https://hastie.su.domains/CASI/) Ch 10--11.\n\nAs researchers adopt algorithmic estimation methods for which analytical standard errors do not exist, methods to produce standard errors by resampling become all the more important. We will discuss the bootstrap for simple random samples and extensions to allow resampling-based standard error estimates in complex survey samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(foreach)\nset.seed(90095)\n```\n:::\n\n\n## A motivating problem\n\nOut of the population of baseball salaries on Opening Day 2023, imagine that we have a sample of 10 Dodger players.\n\n\n::: {.cell}\n\n:::\n\n\nWe calculate the mean salary among the sampled Dodgers to be \\$3.8 million. How much should we trust this estimate?\n\nFor the sake of discussion, we provide the following information.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  `Salary Among Sampled Dodgers`    Value\n  <chr>                             <dbl>\n1 sample_mean                    3829119.\n2 sample_standard_deviation      6357851.\n3 sample_size                         10 \n```\n\n\n:::\n:::\n\n\n## Classical inference\n\nTo know how confident to be in our sample-based estimate, we need to reason about why our sample-based estimate might differ from the true (but unknown) population parameter. Let $\\hat\\mu$ denote our estimate for the sample mean of $Y$.\n\n$$\\hat\\mu = \\frac{1}{n}\\sum_{i}Y_i$$\n\nAcross repeated samples from the population, the estimate $\\hat\\mu$ equals the population mean on average but differs in any particular sample due to random sampling variance. The sample variance of the mean has a known formula.\n\n$$\nV(\\hat\\mu) = V\\left(\\frac{1}{n}\\sum_i Y_i\\right) = \\frac{1}{n^2}\\sum_i V(Y_i) = \\frac{V(Y)}{n}\n$$\n\nThe sample-to-sample variance of $\\hat\\mu$ will be greater to the degree that $Y$ varies substantially across individuals in the population (larger $V(Y)$), and will be smaller to the degree that many individuals are included in the sample (larger $n$).\n\nYou might be more familiar with this equation expressed as the standard deviation of the estimator, sometimes referred to as the standard error, which is the square root of the variance of the estimator,\n\n$$\n\\text{SD}(\\hat\\mu) = \\sqrt{\\text{V}(\\hat\\mu)} = \\frac{\\text{SD}(Y)}{\\sqrt{n}}\n$$\nwhere $\\text{SD}()$ is the standard deviation of $Y$ across individuals in the population.\n\nFrom the Central Limit Theorem, we know that even if $Y$ is not Normally distributed the sample mean of $Y$ converges to a Normal distribution as the sample size grows. Because we have formulas for these parameters, we can write down a formula for that sampling distribution.\n\n$$\n\\hat\\mu \\rightarrow \\text{Normal}\\left(\\text{Mean} = \\text{E}(Y),\\quad \\text{SD} = \\frac{\\text{SD}(Y)}{\\sqrt{n}}\\right)\n$$\nThe graph below visualizes the sampling variability of the sample mean. Across repeated samples, the sample mean $\\hat\\mu$ is normally distributed about its true population value. The middle 95\\% of sample estimates $\\hat\\mu$ fall within a region that can be derived with known formulas,\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in annotate(geom = \"label\", x = 0, y = 0.2 * dnorm(0), label = \"'Middle\n95% of Sample-Based Estimates'~hat(mu)\", : Ignoring unknown parameters:\n`label.size`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](4_ci_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nwhere $\\Phi^{-1}()$ is the inverse CDF of the standard Normal distribution.\n\nYou might be concerned: can a Normal distribution be a good approximation when Dodger player salaries are highly right-skewed? After all this is the distribution of Dodger player salaries.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_ci_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nBut the *sample mean* among 10 sampled Dodgers is actually quite close to a normal sampling distribution. This is because of the Central Limit Theorem.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_ci_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### Plug-in estimators\n\nWe have a formula for the standard deviation of the sample mean, but it involves the term $\\text{SD}(Y)$ which is the unknown population standard deviation of $Y$. It is common to plug in the sample estimate of this value in order to arrive at a sample estimate of the standard deviation of the estimator.\n\n$$\n\\widehat{\\text{SD}}(\\hat\\mu) = \\frac{\\widehat{\\text{SD}}(Y)}{\\sqrt{n}} = \\sqrt{\\frac{\\frac{1}{n-1}\\sum_i (Y_i - \\bar{Y})^2}{n}}\n$$\n\nThe idea of a plug-in estimator may seem obvious, but soon we will see that the step at which plug-ins occur changes dramatically when we move to resampling methods for statistical inference.\n\n### Classical confidence intervals\n\nA 95\\% confidence interval $(\\hat\\mu_\\text{Lower},\\hat\\mu_\\text{Upper})$ is an interval that has the property that across repeated samples the probability that $\\hat\\mu_\\text{Lower} < \\mu < \\hat\\mu_\\text{Upper}$ is 0.95. One way to think about this is that two properties should hold: the probability that the lower limit is too high and the probability that the upper limit is too low are each 0.025.\n\n$$\n\\begin{aligned}\n\\text{P}(\\hat\\mu_\\text{Lower} > \\mu) &= .025 \\\\\n\\text{P}(\\hat\\mu_\\text{Upper} < \\mu) &= .025\n\\end{aligned}\n$$\n\nYou may know from statistics that a 95\\% confidence interval for the sample mean can be derived as follows.\n\n$$\n\\hat\\mu \\pm \\Phi^{-1}(.975)\\widehat{\\text{SD}}(\\hat\\mu)\n$$\nwhere $\\Phi^{-1}(.975)$ is the value 1.96 that you might look up in the back of a statistics textbook. We can show that these confidence limits have the desired properties. For example, taking the lower limit:\n\n$$\n\\begin{aligned}\n&\\text{P}\\left(\\hat\\mu_\\text{Lower} > \\mu\\right)\\\\\n&=\\text{P}\\left(\\hat\\mu - \\Phi^{-1}(.975)\\widehat{\\text{SD}}(\\hat\\mu) > \\mu\\right)\\\\\n&= \\text{P}\\left(\\hat\\mu - \\mu > \\Phi^{-1}(.975)\\widehat{\\text{SD}}(\\hat\\mu)\\right)\\\\\n&= \\text{P}\\left(\\frac{\\hat\\mu - \\mu}{\\widehat{\\text{SD}}(\\hat\\mu)} > \\Phi^{-1}(.975)\\right)\\\\\n&= .025\n\\end{aligned}\n$$\n\nwhere the last line holds because $\\frac{\\hat\\mu - \\mu}{\\text{SD}(\\hat\\mu)}$ follows a standard Normal distribution. The proof for the upper limit is similar.\n\nAcross repeated samples, a 95\\% confidence interval constructed in this way should contain the true mean 95\\% of the time. We can visualize this behavior by taking repeated samples of 10 Dodger players from our data.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_ci_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nIn this particular simulation, we have slight undercoverage and the upper confidence limit is often the one that is incorrect. These problems may arise because our asymptotic normality of mean salaries is an imperfect approximation at a sample size of $n = 10$.\n\n## Analytic vs computational inference procedures\n\nAnalytical confidence intervals (derived by math) are the default for many researchers. Yet the exercise above reveals some of their shortcomings. First, there is a lot of math! Second, despite the math we still relied on the plug-in principle: for unknown quantities such as $\\text{SD}(Y)$ we plug in sample-based estimates $\\widehat{\\text{SD}}(Y)$ and act as though these were known. Third, our results may still yield imperfect coverage because underlying assumptions may be only approximately met. For example, our confidence intervals may have undercovered because the asymptotics of the Central Limit Theorem are unreliable at $n = 10$.\n\nNow suppose you had a complicated data science approach, such as a predicted value $\\hat{Y}_{\\vec{x}}=\\hat{\\text{E}}(Y\\mid \\vec{X} = \\vec{x})$ from a LASSO regression. How would you place a confidence interval on that predicted value?\n\nComputational inference procedures take a different approach. These procedures focus on a generic estimator $s()$ applied to data. Instead of deriving properties of the estimator by math, computational approaches seek to simulate what would happen when $s()$ is applied to samples from the population, often by using a plug-in principle at an earlier step.\n\n## The estimator function $s()$\n\nAt the core of a resampling-based inference procedure is a broad sense of how our estimate comes to be. First, the world has some cumulative distribution function $F$ over data that could be generated. A particular sample $\\texttt{data}$ is drawn from the probability distribution of the world. The researcher then applies an **estimator function** $s()$ that takes in \\texttt{data} and returns an estimate $s(\\texttt{data})$.\n\n$$F\\rightarrow \\texttt{data} \\rightarrow s(\\texttt{data})$$\n\nIn our baseball example, the estimator function is the sample mean of the `salary` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator <- function(data) {\n  data |>\n    summarize(estimate = mean(salary)) |>\n    pull(estimate)\n}\n```\n:::\n\n\nWe would like to repeatedly simulate $\\texttt{data}$ from the world and see the performance of the estimator. But this is only possible in illustrations like the baseball example where the population data are known. When $F$ is unknown and we only see one $\\texttt{data}$, we need a new procedure.\n\n## The nonparametric bootstrap\n\nThe nonparametric bootstrap simulates repeated-sample behavior by a plug-in principle.\n\n1. Plug in the empirical distribution $\\hat{F}$ of our sample data as an estimate of the true distribution $F$ for the unobserved full population of data\n2. Generate a bootstrap sample $\\texttt{data}^*$ by sampling from our empirical data with replacement.\n3. Generate an estimate $s(\\texttt{data}^*)$ using the bootstrap data.\n4. Repeat steps (2) and (3) many times to generate a large number $B$ of bootstrap replicate estimates.\n\nVisually, this procedure is analogous to the above.\n\n$$\\hat{F}\\rightarrow \\texttt{data}^* \\rightarrow s(\\texttt{data}^*)$$\n\n## Nonparametric bootstrap standard errors\n\nIn classical statistics, the standard error of an estimator is typically a mathematical expression derived for that particular estimator and then estimated by the plug-in principle. For example, the standard error of the mean is $\\text{SD}(\\hat\\mu) = \\text{SD}(Y) / \\sqrt{n}$.\n\nWith the bootstrap, we avoid this altogether because we have $B$ bootstrap replicate estimates. We can estimate the standard deviation of the estimator over repeated samples by the empirical standard deviation across bootstrap replicates.\n\n$$\n\\widehat{\\text{SD}}(s) = \\frac{1}{B-1}\\sum_{r=1}^B \\bigg(s(\\texttt{data}^*_r) - s(\\texttt{data}^*_\\bullet)\\bigg)^2\n$$\nwhere $s(\\texttt{data}^*_\\bullet)$ is the mean of the estimate across the bootstrap samples. Note that just like the analytic standard errors, these have also relied on a plug-in principle: we plugged in the empirical distribution $\\hat{F}$ for the population distribution $F$ when generating bootstrap samples from our empirical data instead of actual samples from the population.\n\nIn our baseball example, we first load data and draw a sample of 10 Dodger players.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopulation <- read_csv(\"http://soc114.github.io/data/baseball_population.csv\")\nsample <- population |>\n  filter(team == \"L.A. Dodgers\") |>\n  sample_n(size = 10) |>\n  select(player, team, salary)\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n   player           team           salary\n   <chr>            <chr>           <dbl>\n 1 Barnes, Austin   L.A. Dodgers  3500000\n 2 Reyes, Alex*     L.A. Dodgers  1100000\n 3 Betts, Mookie    L.A. Dodgers 21158692\n 4 Vargas, Miguel   L.A. Dodgers   722500\n 5 May, Dustin      L.A. Dodgers  1675000\n 6 Bickford, Phil   L.A. Dodgers   740000\n 7 Jackson, Andre   L.A. Dodgers   722500\n 8 Thompson, Trayce L.A. Dodgers  1450000\n 9 Pepiot, Ryan*    L.A. Dodgers   722500\n10 Peralta, David   L.A. Dodgers  6500000\n```\n\n\n:::\n:::\n\n\nThe code below generates a bootstrap sample from these 10 players by sampling 10 players with replacement. You will see that some players in the original sample do not appear, and others appear more than once.\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_sample <- sample |>\n  slice_sample(prop = 1, replace = TRUE) |>\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n   player         team           salary\n   <chr>          <chr>           <dbl>\n 1 Betts, Mookie  L.A. Dodgers 21158692\n 2 Peralta, David L.A. Dodgers  6500000\n 3 Barnes, Austin L.A. Dodgers  3500000\n 4 Pepiot, Ryan*  L.A. Dodgers   722500\n 5 Jackson, Andre L.A. Dodgers   722500\n 6 May, Dustin    L.A. Dodgers  1675000\n 7 Reyes, Alex*   L.A. Dodgers  1100000\n 8 May, Dustin    L.A. Dodgers  1675000\n 9 Vargas, Miguel L.A. Dodgers   722500\n10 Peralta, David L.A. Dodgers  6500000\n```\n\n\n:::\n:::\n\n\nWe can apply our estimator function to this sample to get one bootstrap estimate.\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator <- function(data) {\n  data |>\n    summarize(estimate = mean(salary)) |>\n    pull(estimate)\n}\nestimator(one_sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4427619\n```\n\n\n:::\n:::\n\n\nThe code below carries out 500 bootstrap samples and estimates the sample mean in each one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_estimates <- foreach(r = 1:1000, .combine = \"c\") %do% {\n  sample |>\n    # Draw a bootstrap sample\n    slice_sample(prop = 1, replace = TRUE) |>\n    # Apply the estimator\n    estimator()\n}\n```\n:::\n\n\nThe figure below shows how that the bootstrap distribution of the estimator compares to the actual sampling distribution of the estimator (known in this case since the population is known).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_ci_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThe bootstrap distribution is more heaped on 10 distinct salary values: the particular 10 Dodger player salaries included in our sample. When the variable being summarized takes continuous values, it will generally be more discretized in the bootstrap setting because there are only the sample size $n$ distinct values instead of the population size $N$ of distinct values. Otherwise, the two distributions are similar.\n\nThe bootstrap estimate of the standard error in this case is\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_estimates |> sd()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1965073\n```\n\n\n:::\n:::\n\n\nwhich is 86% of the size of the theoretical standard error of 2.2969632\\times 10^{6}. Like all sample-based analogs to theoretical standard errors, the bootstrap estimate of the standard error can itself be sensitive to sampling variability.\n\n## Parallel processing\n\nThe bootstrap is a computer-age approach to inference: present-day computers are what make it possible to apply the estimator thousands of repeated times.\n\nParallel processing can make the bootstrap faster. Note that each bootstrap replicate is independent: the first bootstrap replicate results are not needed to carry out the second, and so on. If you had 5 or 10 computers, you could split the bootstrap samples across those computers and carry it out 5 or 10 times as fast!\n\nYou may be surprised that you likely do have several effectively independent computers (called cores) within your computer. The `doParallel` package makes it possible to use these cores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(doParallel)\n```\n:::\n\n\nWith the doParallel package loaded, you can see how many cores you have with the `detectCores()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndetectCores()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n:::\n\n\nIn my case, there are 12 cores on my computer. This means I can split my bootstrap over 8 parallel processors! First, initialize a cluster with these cores.\n\n::: {.cell}\n\n```{.r .cell-code}\ncl <- makeCluster(spec = detectCores())\n```\n:::\n\n\nThen tell your computer to use that computing cluster for parallel processing.\n\n::: {.cell}\n\n```{.r .cell-code}\nregisterDoParallel(cl)\n```\n:::\n\n\nFinally, you can carry out your `foreach` loop using parallel processing, with the `%dopar%` operator clarifying that the loop should be done with parallel processing. You will also need the `.packages` argument to pass to the computing cores any packages that are used within the procedure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_estimates <- foreach(\n  r = 1:1000, \n  .combine = \"c\", \n  .packages = c(\"tidyverse\")\n) %dopar% {\n  sample |>\n    # Draw a bootstrap sample\n    slice_sample(prop = 1, replace = TRUE) |>\n    # Apply the estimator\n    estimator()\n}\n```\n:::\n\n\nIn my case, parallel processing will make the entire procedure about 12 times faster. As computers have become faster with more cores, bootstrapping has become an increasingly feasible method of statistical inference.\n\n## Bootstrap confidence intervals\n\nThe discussion above has focused on using the bootstrap to estimate standard errors. There are many methods to construct confidence intervals using bootstrap procedures. Two of the most common are the Normal approximation method and the percentile method.\n\n### Normal approximation\n\nThe beginning of this page reviewed classical statistics in which we routinely rely on the Central Limit Theorem which ensures that sample mean estimators are asymptotically Normal. Likewise with the bootstrap, if we believe that $s(\\texttt{data})$ has a Normal sampling distribution, then we can construct a confidence interval by the Normal approximation with the bootstrap estimate of the standard error.\n\n$$\ns(\\texttt{data}) \\pm \\Phi^{-1}(.975)\\text{SD}\\big(s(\\text{data}^*)\\big)\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator(sample) + c(-1,1) * qnorm(.975) * sd(bootstrap_estimates)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]   77781.86 7580456.54\n```\n\n\n:::\n:::\n\n\n### Percentile method\n\nThe bootstrap also offers another way to calculate the confidence interval: the middle 95\\% of the bootstrap estimates. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(bootstrap_estimates, probs = c(.025, .975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   2.5%   97.5% \n1133706 8083426 \n```\n\n\n:::\n:::\n\n\n<!-- Should add more justification of percentile intervals. In what sense do they guarantee that the probability of the lower limit being above the truth is 2.5%? -->\n\nThe percentile method can work better than the Normal approximation method in cases where normality does not hold. For example, in the beginning of this page we used analytic intervals that seemed imperfect in part because the Central Limit Theorem had not adequately yielded normality at a sample size of 10.\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_ci_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n## Bootstrap for machine learning algorithms\n\nSuppose a researcher carries out the following procedure.\n\n1. Sample $n$ units from the population\n2. Learn an algorithm $\\hat{f}:\\vec{X}\\rightarrow Y$ to minimize squared error\n3. Report a prediction $\\hat{\\text{E}}(Y\\mid\\vec{X} = \\vec{x}) = \\hat{f}(\\vec{x})$\n\nHow would the researcher use the bootstrap to carry out this process?\n\n1. Draw a bootstrap sample $\\texttt{data}^*$ of size $n$\n2. Learn the algorithm $\\hat{f}^*$ in the bootstrap sample\n3. Store the bootstrap estimate $\\hat{f}^*(\\vec{x})$\n\nThen the researcher could create a confidence interval with either the Normal approximation or the percentile method. Note that the bootstrap confidence interval may have undercoverage if the estimator is biased; see the words of warning at the end of this page.\n\n## Discussion: What belongs in $s()$?\n\nIn each example, describe the steps the researcher might use to bootstrap this estimate while capturing all sources of uncertainty.\n\n1. A researcher first truncates the values of a skewed predictor variable $x$ at the 1st and 99th percentile. Then the researcher learns a regression model and reports $\\hat\\beta$.\n2. A researcher first uses cross-validation to select the tuning parameter $\\lambda$ for ridge regression. Then, they estimate ridge regression with the chosen $\\lambda$ value and make a prediction $\\hat{f}(\\vec{x})$ at some predictor value $\\vec{x}$ of interest.\n3. A researcher first learns a prediction function $\\hat{f}:\\vec{X}\\rightarrow Y$ and then sees which subgroup $\\vec{x}$ has the highest predicted value $\\hat{f}(\\vec{x})$, which the researcher reports.\n\n::: {.callout-note collapse=\"true\"}\n## Answers\n\nMany steps of the analysis involve uncertainty. It can be ideal to include them all in your bootstrap! Write your estimator function to take in your raw data and return an estimate. The estimator function would include steps like truncating predictors at sample quantiles, choosing tuning parameters, and choosing subgroups of interest on which to focus.\n:::\n\n## Beyond simple random samples\n\nThe bootstrap in its simplest form is designed for simple random samples. Straightforward generalizations make it possible to move beyond simple random samples to more complex sampling designs.\n\n### Stratified bootstrap\n\nSuppose we draw a sample of players stratified by team: 10 players per team. No matter which random sample is drawn, there will always be 10 Dodgers, 10 Angels, 10 Yankees, and so on. Stratified sampling is often a more efficient estimator than simple random sampling, and our estimator should reflect that!\n\nAs an example, suppose we have a stratified sample of 10 players per team.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstratified_sample <- population |>\n  group_by(team) |>\n  slice_sample(n = 10) |>\n  ungroup()\n```\n:::\n\n\nWe would generate a stratified bootstrap sample^[Called the \"multi-sample bootstrap\" in Efron \\& Hastie.] by stratifying by team, exactly as the data were generated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstratified_bootstrap_sample <- stratified_sample |>\n  group_by(team) |>\n  slice_sample(prop = 1, replace = T)\n```\n:::\n\n\nStratified bootstrapping can be important. Using our baseball example, suppose our estimator is the predicted mean salary of the Dodgers from a linear regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator <- function(data) {\n  ols <- lm(salary ~ team_past_salary, data = data)\n  to_predict <- population |> \n    filter(team == \"L.A. Dodgers\") |> \n    distinct(team_past_salary)\n  predicted <- predict(ols, newdata = to_predict)\n  return(predicted)\n}\n```\n:::\n\n\nWe get different estimates if we carry out simple vs stratified bootstrap sampling.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_ci_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n### Cluster bootstrap\n\nSuppose we draw a sample of players clustered by team: all players on 10 sampled teams. Clustered sampling is often less expensive than simple random sampling because it can be easier for the person carrying out the survey. This often comes at a cost of statistical efficiency.\n\nAs an example, suppose we have a clustered sample of 10 teams.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclustered_sample <- population |>\n  distinct(team) |>\n  slice_sample(n = 10) |>\n  left_join(population, by = join_by(team))\n```\n:::\n\n\nWe would generate a clustered bootstrap sample by resampling teams instead of players, exactly as the data were sampled.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_teams <- clustered_sample |>\n  distinct(team) |>\n  slice_sample(prop = 1, replace = T)\nclustered_bootstrap_sample <- foreach(i = 1:nrow(chosen_teams), .combine = \"rbind\") %do% {\n  chosen_teams[i,] |>\n    left_join(clustered_sample, by = join_by(team))\n}\n```\n:::\n\n\nAs before, we get different estimated standard errors if we carry out clustered bootstrap sampling vs standard bootstrap sampling.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](4_ci_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n### Complex survey samples\n\nMany surveys involve complex samples, such as samples stratified by state and then clustered in regions within states. Often the variables that define sampling strata or clusters are geographic, and therefore they are often redacted from the data made available to researchers due to privacy concerns.\n\nThankfully, many surveys make **replicate weights** available to researchers. The goal of replicate weights is to enable you to resample the data in a way that mimics the (hidden) ways in which the sample was originally drawn. The rest of this section walks through the use of replicate weights, first in a hypothetical example and then in real data.\n\nWhen we download data, we typically download a column of weights. For simplicity, suppose we are given a sample of four people. The `weight` column tells us how many people in the population each person represents. The `employed` column tells us whether each person employed.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n     name weight employed\n1    Luis      4        1\n2 William      1        0\n3   Susan      1        0\n4  Ayesha      4        1\n```\n\n\n:::\n:::\n\n\nIf we take an unweighted mean, we would conclude that only 50\\% of the population is employed. But with a weighted mean, we would conclude that 80\\% of the population is employed! This might be the case if the sample was designed to oversample people at a high risk of unemployment.\n\n| Estimator | Math | Example | Result |\n|:--|:--|:--|:--|\n| Unweighted mean | $=\\frac{\\sum_{i=1}^n Y_i}{n}$ | $=\\frac{1 + 0 + 0 + 1}{4}$ | = 50\\% employed |\n| Weighted mean | $=\\frac{\\sum_{i=1}^n w_iY_i}{\\sum_{i=1}^n w_i}$ | $=\\frac{4*1 + 1*0 + 1*0 + 4*1}{4 + 1 + 1 + 4}$ | = 80\\% employed |\n\nIn R, the [`weighted.mean(x, w)`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/weighted.mean.html) function will calculate weighted means where `x` is an argument for the outcome variable and `w` is an argument for the weight variable.\n\nWhen you face a complex survey sample, those who administer the survey might provide\n\n- a vector of $n$ weights for making a point estimate\n- a matrix of $n\\times k$ replicate weights for making standard errors\n\nBy providing $k$ different ways to up- and down-weight various observations, the replicate weights enable you to generate $k$ estimates that vary in a way that mimics how the estimator might vary if applied to different samples from the population. For instance, our employment sample might come with 3 replicate weights.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n     name weight employed repwt1 repwt2 repwt3\n1    Luis      4        1      3      5      3\n2 William      1        0      1      2      2\n3   Susan      1        0      3      1      1\n4  Ayesha      4        1      5      3      4\n```\n\n\n:::\n:::\n\n\nThe procedure to use replicate weights depends on how they are constructed. Often, it is relatively straightforward:\n\n- use `weight` to create a point estimate $\\hat\\tau$\n- use `repwt*` to generate $k$ replicate estimates $\\hat\\tau^*_1,\\dots,\\hat\\tau^*_k$\n- calculate the standard error of $\\hat\\tau$ using the replicate estimates $\\hat\\tau^*$. The formula will depend on how the replicate weights were constructed, but it will likely involve the standard deviation of the $\\hat\\tau^*$ multiplied by some factor\n- construct a confidence interval^[If we hypothetically drew many complex survey samples from the population in this way, an interval generated this way would contain the true population mean 95\\% of the time.] by a normal approximation\n$$(\\text{point estimate}) \\pm 1.96 * (\\text{standard error estimate})$$\n\nIn our concrete example, the point estimate is 80\\% employed. The replicate estimates are 0.67, 0.73, 0.70. Variation across the replicate estimates tells us something about how the estimate would vary across hypothetical repeated samples from the population.\n\n### Computational strategy for replicate weights\n\nUsing replicate weights can be computationally tricky! It becomes much easier if you write an `estimator()` function. Your function accepts two arguments\n\n- `data` is the `tibble` containing the data\n- `weight_name` is the name of a column containing the weight to be used (e.g., \"repwt1\")\n\n**Example.** If our estimator is the weighted mean of employment,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator <- function(data, weight_name) {\n  data |>\n    summarize(\n      estimate = weighted.mean(\n        x = employed,\n        # extract the weight column\n        w = sim_rep |> pull(weight_name)\n      )\n    ) |> \n    # extract the scalar estimate\n    pull(estimate)\n}\n```\n:::\n\n\nIn the code above, `sim_rep |> pull(weight_name)` takes the data frame `sim_rep` and extracts the weight variable that is named `weight_name`. There are other ways to do this also.\n\nWe can now apply our estimator to get a point estimate with the main sampling weight,\n\n::: {.cell}\n\n```{.r .cell-code}\nestimate <- estimator(data = sim_rep, weight_name = \"weight\")\n```\n:::\n\nwhich yields the point estimate 0.80. We can use the same function to produce the replicate estimates,\n\n::: {.cell}\n\n```{.r .cell-code}\nreplicate_estimates <- c(\n  estimator(data = sim_rep, weight_name = \"repwt1\"),\n  estimator(data = sim_rep, weight_name = \"repwt2\"),\n  estimator(data = sim_rep, weight_name = \"repwt3\")\n)\n```\n:::\n\nyielding the three estimates: 0.67, 0.73, 0.70. In real data, you will want to apply this in a loop because there may be dozens of replicate weights.\n\nThe standard error of the estimator will be some function of the replicate estimates, likely involving the standard deviation of the replicate estimates. Check with the data distributor for a formula for your case. Once you estimate the standard error, a 95\\% confidence interval can be constructed with a Normal approximation, as discussed above.\n\n### Application in the CPS\n\nStarting in 2005, the CPS-ASEC samples include 160 replicate weights. If you download replicate weights for many years, the file size will be enormous. We illustrate the use of replicate weights with a question that can be explored with only one year of data: among 25-year olds in 2023, how did the proportion holding four-year college degrees differ across those identifying as male and female?\n\nWe first load some packages, including the `foreach` package which will be helpful when looping through replicate weights.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(foreach)\n```\n:::\n\n\nTo answer our research question, we download 2023 CPS-ASEC data including the variables [`sex`](https://cps.ipums.org/cps-action/variables/SEX), [`educ`](https://cps.ipums.org/cps-action/variables/EDUC), [`age`](https://cps.ipums.org/cps-action/variables/AGE), the weight variable [`asecwt`](https://cps.ipums.org/cps-action/variables/ASECWT), and the replicate weights [`repwtp*`](https://cps.ipums.org/cps-action/variables/REPWTP).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncps_data <- read_dta(\"../data_raw/cps_00079.dta\")\n```\n:::\n\n\nWe then define an estimator to use with these data. It accepts a tibble `data` and a character `weight_name` identifying the name of the weight variable, and it returns a tibble with two columns: `sex` and `estimate` for the estimated proportion with a four-year degree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator <- function(data, weight_name) {\n  data |>\n    # Define focal_weight to hold the selected weight\n    mutate(focal_weight = data |> pull(weight_name)) |>\n    # Restrict to those age 25+\n    filter(age >= 25) |>\n    # Restrict to valid reports of education\n    filter(educ > 1 & educ < 999) |>\n    # Define a binary outcome: a four-year degree\n    mutate(college = educ >= 110) |>\n    # Estimate weighted means by sex\n    group_by(sex) |>\n    summarize(estimate = weighted.mean(\n      x = college,\n      w = focal_weight\n    ))\n}\n```\n:::\n\n\nWe produce a point estimate by applying that estimator with the `asecwt`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimate <- estimator(data = cps_data, weight_name = \"asecwt\")\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  sex        estimate\n  <dbl+lbl>     <dbl>\n1 1 [male]      0.369\n2 2 [female]    0.397\n```\n\n\n:::\n:::\n\n\nUsing the `foreach` package, we apply the estimator 160 times---once with each replicate weight---and use the argument `.combine = \"rbind\"` to stitch results together by rows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(foreach)\nreplicate_estimates <- foreach(r = 1:160, .combine = \"rbind\") %do% {\n  estimator(data = cps_data, weight_name = paste0(\"repwtp\",r))\n}\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 320 × 2\n   sex        estimate\n   <dbl+lbl>     <dbl>\n 1 1 [male]      0.368\n 2 2 [female]    0.396\n 3 1 [male]      0.371\n 4 2 [female]    0.400\n 5 1 [male]      0.371\n 6 2 [female]    0.397\n 7 1 [male]      0.369\n 8 2 [female]    0.397\n 9 1 [male]      0.370\n10 2 [female]    0.398\n# ℹ 310 more rows\n```\n\n\n:::\n:::\n\n\nWe estimate the standard error of our estimator by a formula\n$$\\text{StandardError}(\\hat\\tau) = \\sqrt{\\frac{4}{160}\\sum_{r=1}^{160}\\left(\\hat\\tau^*_r - \\hat\\tau\\right)^2}$$\nwhere the formula comes from the [survey documentation](https://cps.ipums.org/cps/repwt.shtml#q40). We carry out this procedure within groups defined by sex, since we are producing estimate for each sex.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandard_error <- replicate_estimates |>\n  # Denote replicate estimates as estimate_star\n  rename(estimate_star = estimate) |>\n  # Merge in the point estimate\n  left_join(estimate,\n            by = join_by(sex)) |>\n  # Carry out within groups defined by sex\n  group_by(sex) |>\n  # Apply the formula from survey documentation\n  summarize(standard_error = sqrt(4 / 160 * sum((estimate_star - estimate) ^ 2)))\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  sex        standard_error\n  <dbl+lbl>           <dbl>\n1 1 [male]          0.00280\n2 2 [female]        0.00291\n```\n\n\n:::\n:::\n\n\nFinally, we combine everything and construct a 95\\% confidence interval by a Normal approximation.\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- estimate |>\n  left_join(standard_error, by = \"sex\") |>\n  mutate(ci_min = estimate - 1.96 * standard_error,\n         ci_max = estimate + 1.96 * standard_error)\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  sex        estimate standard_error ci_min ci_max\n  <dbl+lbl>     <dbl>          <dbl>  <dbl>  <dbl>\n1 1 [male]      0.369        0.00280  0.364  0.375\n2 2 [female]    0.397        0.00291  0.391  0.403\n```\n\n\n:::\n:::\n\n\nWe use `ggplot()` to visualize the result.\n\n::: {.cell}\n\n```{.r .cell-code}\nresult |>\n  mutate(sex = as_factor(sex)) |>\n  ggplot(aes(\n    x = sex, \n    y = estimate,\n    ymin = ci_min, \n    ymax = ci_max,\n    label = scales::percent(estimate)\n  )) +\n  geom_errorbar(width = .2) +\n  geom_label() +\n  scale_x_discrete(\n    name = \"Sex\", \n    labels = str_to_title\n  ) +\n  scale_y_continuous(name = \"Proportion with 4-Year College Degree\") +\n  ggtitle(\n    \"Sex Disparities in College Completion\",\n    subtitle = \"Estimates from the 2023 CPS-ASEC among those age 25+\"\n  )\n```\n\n::: {.cell-output-display}\n![](4_ci_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n\nWe conclude that those identifying as female are more likely to hold a college degree. Because we can see the confidence intervals generated using the replicate weights, we are reasonably confident in the statistical precision of our point estimates.\n\n## A word of warning\n\nThe bootstrap is a powerful tool, but there are notable cases in which it fails.\n\nFirst, all frequentist confidence intervals that are based solely on sampling variance may suffer undercoverage if applied to biased estimators. For example, many machine learning algorithms induce bias through regularization. This means that even if we correctly approximate sampling variance, the center of our confidence intervals may be systematically misaligned from the true population parameter, yielding undercoverage.\n\nSecond, the bootstrap can exhibit unexpected performance with statistics such as the maximum or minimum value, since these statistics can be sensitive to a particular data point. Taking the maximum as an example, the value $\\text{max}(\\vec{y}^*)$ in a bootstrap sample will never be higher than $\\text{max}(\\vec{y})$ in the sample from which that bootstrap was drawn. The entire bootstrap distribution of $\\text{max}(\\vec{y}^*)$ will be at or below the original estimate of $\\text{max}(\\vec{y})$. Like the max or min, quantiles of $\\vec{y}$ can also lead to unexpected bootstrap behavior. Generally the bootstrap will have the best performance for statistics such as the mean for which no particular unit plays an especially determining role.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}