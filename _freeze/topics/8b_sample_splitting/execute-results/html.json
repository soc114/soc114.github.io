{
  "hash": "091e70ea8cf9487692c0732c004f794c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sample Splitting\"\n---\n\nA predictive model $\\hat{f}()$ is an input-output function:\n\n- the input is a set of features $\\vec{x}$\n- the output is a predicted outcome $\\hat{y} = \\hat{f}(\\vec{x})$\n\nIdeally, a prediction function works well on **new cases**: cases that were not used to learn the model. Sample splitting is a strategy to test a prediction function on out-of-sample cases.\n\nWe will use the `tidyverse` package as usual. In addition, we will use the `rsample` package to create a sample split. You may need to install `rsample` by running `install.packages(\"rsample\")` in your console.\n\n\n::: {.cell}\n\n:::\n\n\n## Simulated data example\n\nTo practice the mechanics of sample splitting, the data [for_sample_split.csv](../assets/for_sample_split.csv) contain simulated data with 100 predictors `x1`,$\\dots$,`x100` observed for $n = 300$ cases.\n\n\n::: {.cell}\n\n:::\n\nbefore applying that function to generate one sample.\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_sample_split <- read_csv(\"https://soc114.github.io/assets/for_sample_split.csv\")\n```\n:::\n\n\nWe will consider modeling `y` by linear regression with various subset of the `x` variables.\n\n## How to sample split\n\nTo create a sample split, use the `initial_split` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplitted <- initial_split(data = for_sample_split, prop = .5)\n```\n:::\n\n\nThis randomly splits the data into two equally-sized subgroups, `training(splitted)` and `testing(splitted)`.\n\n## Evaluate predictive performance\n\nWe can fit a model on the training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(y ~ x1, data = training(splitted))\n```\n:::\n\n\nWe can then make out-of-sample predictions in the testing set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted <- testing(splitted) |>\n  mutate(yhat = predict(model, newdata = testing(splitted)))\n```\n:::\n\n\nFinally, we can evaluate mean squared error in the testing set.\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted |>\n  mutate(\n    error = y - yhat\n  ) |>\n  summarize(\n    mean_squared_error = mean(error ^ 2)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  mean_squared_error\n               <dbl>\n1               2.21\n```\n\n\n:::\n:::\n\n\n## Comparing several models\n\nWe can do the above for several candidate models. For example, are predictions more accurate using only `x1` as a predictor, or using all available columns as predictors?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_simple <- lm(y ~ x1, data = training(splitted))\nmodel_complex <- lm(y ~ ., data = training(splitted))\n```\n:::\n\n\nWe can then make out-of-sample predictions in the testing set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted <- testing(splitted) |>\n  mutate(\n    yhat_simple = predict(model_simple, newdata = testing(splitted)),\n    yhat_complex = predict(model_complex, newdata = testing(splitted))\n  )\n```\n:::\n\n\nFinally, we can evaluate mean squared error in the testing set.\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted |>\n  mutate(\n    error_simple = y - yhat_simple,\n    error_complex = y - yhat_complex\n  ) |>\n  summarize(\n    mse_simple = mean(error_simple ^ 2),\n    mse_complex = mean(error_complex ^ 2),\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  mse_simple mse_complex\n       <dbl>       <dbl>\n1       2.21        2.43\n```\n\n\n:::\n:::\n\n\nBy the gold standard of out-of-sample prediction, the simple model is better than the complex model! This may be surprising, because in fact these data were generated such that the complex model is the true model. But, there are so few cases that the complex model does not perform well at this sample size.\n\n## Closing thoughts\n\nSample splitting is an art as much as a science. In particular applications, the gain from sample splitting is not always clear and must be balanced against the reduction in cases available for training. It is important to remember that out-of-sample prediction remains the gold standard, and sample splitting is one way to approximate that when only one sample is available.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}