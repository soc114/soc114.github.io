[
  {
    "objectID": "topics/data_transformation.html",
    "href": "topics/data_transformation.html",
    "title": "Data transformation",
    "section": "",
    "text": "This topic is covered on Jan 16. If you want to download a full .qmd instead of copying codes from the website, you can use data_transformation_exercise.qmd.\nThis exercise examines how income inequality has changed over time in the U.S. We will measure inequality by the 10th, 50th, and 90th percentiles of wage and salary income from 1962 to 2022.1 The goal is to produce a graph like this one.",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#prepare-your-r-environment",
    "href": "topics/data_transformation.html#prepare-your-r-environment",
    "title": "Data transformation",
    "section": "Prepare your R environment",
    "text": "Prepare your R environment\nIn RStudio, create a Quarto document. Save it in your working directory. Run the following code to load two packages we will use. If you do not have these packages, first run install.packages(\"tidyverse\") and install.packages(\"haven\") to install the packages.\n\nlibrary(tidyverse)\nlibrary(haven)",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#data-access",
    "href": "topics/data_transformation.html#data-access",
    "title": "Data transformation",
    "section": "Data access",
    "text": "Data access\nThis exercise uses data from the Current Population Survey, provided via IPUMS. We have two versions of the data:\n\nsimulated data made available to you via the course website\nactual data, for which you will register with the data provider\n\nIn lecture, we will use simulated data. In discussion, your TA will walk through the process to access the actual data. You will ultimately need to access the actual data for future assignments in this class.\n\nAccessing simulated data\nThe simulated data are designed to have the same statistical properties as the actual data. To access the simulated data, copy the following line of code into your R script and run it. This line loads the data and stores it in an object called cps_data.\n\n\ncps_data &lt;- read_dta(\"https://soc114.github.io/data/simulated_cps_data.dta\")\n\n\n\nAccessing actual data\nAccessing the actual data is important for future assignments. You may also use these data in your project. Here are instructions to access the data:\n\nRegister for an account at cps.ipums.org\nLog in\nClick “Get Data”\nAdd the following variables to your cart: incwage, educ, wkswork2, age, asecwt\nAdd the 1962–2023 ASEC samples to your cart. Exclude the basic monthly samples\nCreate a data extract\n\nSelect cases to only download people ages 30–45\nChoose to download in Stata (.dta) format\n\nSubmit your extract and download the data!\n\nStore your data in a working directory: a folder on your computer that will hold the data for this exercise. Load the data using the read_dta function in the haven package.\n\ncps_data &lt;- read_dta(\"your_downloaded_file_name.dta\")\n\n\n\n\n\n\n\nTip\n\n\n\n\nChange the file name to the name of the file you downloaded\nIf R says the file does not exist in your current working directory, you may need to set your working directory by clicking Session -&gt; Set Working Directory -&gt; To Source File Location on a Mac or Tools -&gt; Change Working Directory on Windows.",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#explore-the-data",
    "href": "topics/data_transformation.html#explore-the-data",
    "title": "Data transformation",
    "section": "Explore the data",
    "text": "Explore the data\nType cps_data in the console. Some columns such as educ have a numeric code and a label. The code is how IPUMS has stored the data. The label is what the code means. You can always find more documentation explaining the labels on the IPUMS-CPS website.",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#filter-to-cases-of-interest",
    "href": "topics/data_transformation.html#filter-to-cases-of-interest",
    "title": "Data transformation",
    "section": "filter() to cases of interest",
    "text": "filter() to cases of interest\n\nIn this step, you will use filter() to convert your cps_data object to a new object called filtered.\n\nThe filter() function keeps only rows in our dataset that correspond to those we want to study. The examples on the documentation page are especially helpful. The R4DS section is also helpful.\nHere are two ways to use filter() to restrict to people working 50+ weeks per year. One way is to call the filter() function and hand it two arguments\n\n.data = cps_data is the dataset\nyear == 1962 is a logical condition coded TRUE for observations in 1962\n\n\nfilter(.data = cps_data, year == 1962)\n\nThe result of this call is a tibble with only the observations from 1962. Another way to do the same operation is with the pipe operator |&gt;\n\ncps_data |&gt;\n  filter(year == 1962)\n\nThis approach begins with the data set cps_data. The pipe operator |&gt; hands this data set on as the first argument to the filter() function in the next line. As before, the second argument is the logical condition year == 1962.\nThe piping approach is often preferable because it reads like a sentence: begin with data, then filter to cases with a given condition. The pipe is also useful\nThe pipe operator |&gt; takes what is on the first line and hands it on as the first argument to the function in the next line. This reads in a sentence: begin with the cps_data tibble and then filter() to cases with year == 1962. The pipe can also string together many operations, with comments allowed between them:\n\ncps_data |&gt;\n  # Restrict to 1962\n  filter(year == 1962) |&gt;\n  # Restrict to ages 40-44\n  filter(age &gt;= 40 & age &lt;= 44)\n\nYour turn. Begin with the cps_data dataset. Filter to\n\npeople working 50+ weeks per year (check documentation for wkswork2)\nvalid report of incwage greater than 0 and less than 99999998\n\n\n\nShow the code answer\nfiltered &lt;- cps_data |&gt;\n  # Subset to cases working full year\n  filter(wkswork2 == 6) |&gt;\n  # Subset to cases with valid income\n  filter(incwage &gt; 0 & incwage &lt; 99999998)\n\n\n\n\n\n\n\n\nNote\n\n\n\nFiltering can be a dangerous business! For example, above we dropped people with missing values of income. But what if the lowest-income people refuse to answer the income question? We often have no choice but to filter to those with valid responses, but you should always read the documentation to be sure you understand who you are dropping and why.",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#group_by-and-summarize-for-subpopulation-summaries",
    "href": "topics/data_transformation.html#group_by-and-summarize-for-subpopulation-summaries",
    "title": "Data transformation",
    "section": "group_by() and summarize() for subpopulation summaries",
    "text": "group_by() and summarize() for subpopulation summaries\n\nIn this step, you will use group_by() and summarize() to convert your mutated object to a new object called summarized.\n\nEach row in our dataset is a person. We want a dataset where each row is a year. To get there, we will group our data by year and then summarize each group by a set of summary statistics.\n\nIntroducing summarize() with the sample mean\nTo see how summarize() works, let’s first summarize the sample mean income within each year. The input has one row per person. The result has one row per group. For each year, it records the sample mean income.\n\nfiltered |&gt;\n  group_by(year) |&gt;\n  summarize(mean_income = mean(incwage))\n\n# A tibble: 62 × 2\n    year mean_income\n   &lt;dbl&gt;       &lt;dbl&gt;\n 1  1962       6383.\n 2  1963       5831.\n 3  1964       6688.\n 4  1965       6066.\n 5  1966       6438.\n 6  1967       6745.\n 7  1968       7244.\n 8  1969       8465.\n 9  1970       9198.\n10  1971       8490.\n# ℹ 52 more rows\n\n\n\n\n\nUsing summarize() with weighted quantiles\nInstead of the mean, we plan to use three other summary statistics: the 10th, 50th, and 90th percentiles of income. We also want to incorporate the sampling weights provided with the Current Population Survey, in order to summarize the population instead of the sample.\nWe will use the wtd.quantile function to create weighted quantiles. This function is available in the Hmisc package. If you don’t have that package, install it with install.packages(\"Hmisc\"). Using the Hmisc package is tricky, because it has some functions with the same name as functions that we use in the tidyverse. Instead of loading the whole package, we will only load the functions we are using at the time we use them. Whenever we want to calculate a weighted quantile, we will call it with the code packagename::functionname() which in this case is Hmisc::wtd.quantile().\nThe wtd.quantile function will take three arguments:\n\nx is the variable to be summarized\nweights is the variable containing sampling weights\nprobs is the probability cutoffs for the quantiles. For the 10th, 50th, and 90th percentiles we want 0.1, 0.5, and 0.9.\n\nThe code below produces weighted quantile summaries.\n\nsummarized &lt;- filtered |&gt;\n  group_by(year) |&gt;\n  summarize(\n    p10 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.1),\n    p50 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.5),\n    p90 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.9),\n    .groups = \"drop\"\n  )",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#pivot_longer-to-reshape-data",
    "href": "topics/data_transformation.html#pivot_longer-to-reshape-data",
    "title": "Data transformation",
    "section": "pivot_longer() to reshape data",
    "text": "pivot_longer() to reshape data\n\nIn this step, you will use pivot_longer() to convert your summarized object to a new object called pivoted. We first explain why, then explain the task.\n\nWe ultimately want to make a ggplot() where income values are placed on the y-axis. We want to plot the 10th, 50th, and 90th percentiles along this axis, distinguished by color. We need them all in one colun! But currently, they are in three columns.\nHere is the task. How our data look:\n\n\n# A tibble: 62 × 4\n   year   p10   p50    p90\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  1962 1826. 4460. 11733.\n2  1963 1770. 4484. 11934.\n# ℹ 60 more rows\n\n\nHere we want our data to look:\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10       1826.\n2  1962 p50       4460.\n3  1962 p90      11733.\n4  1963 p10       1770.\n5  1963 p50       4484.\n6  1963 p90      11934.\n# ℹ 180 more rows\n\n\nThis way, we can use year for the x-axis, quantity for color, and value for the y-axis.\nUse pivot_longer() to change the first data frame to the second.\n\nUse the cols argument to tell it which columns will disappear\nUse the names_to argument to tell R that the names of those variables will be moved to a column called quantity\nUse the values_to argument to tell R that the values of those variables will be moved to a column called income\n\nIf you get stuck, see how we did it at the end of this page.\n\n\nShow the code answer\npivoted &lt;- summarized %&gt;%\n  pivot_longer(\n    cols = c(\"p10\",\"p50\",\"p90\"),\n    names_to = \"quantity\",\n    values_to = \"income\"\n  )",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#left_join-an-inflation-adjustment",
    "href": "topics/data_transformation.html#left_join-an-inflation-adjustment",
    "title": "Data transformation",
    "section": "left_join() an inflation adjustment",
    "text": "left_join() an inflation adjustment\n\nIn this step, you will use left_join() to merge in an inflation adjustment\n\nA dollar in 1962 bought a lot more than a dollar in 2022. We will adjust for inflation using the Consumer Price Index, which tracks the cost of a standard basket of market goods. We already took this index to create a file inflation.csv,\n\ninflation &lt;- read_csv(\"https://soc114.github.io/data/inflation.csv\")\n\n\n\n# A tibble: 62 × 2\n   year inflation_factor\n  &lt;dbl&gt;            &lt;dbl&gt;\n1  1962            10.1 \n2  1963             9.95\n3  1964             9.82\n# ℹ 59 more rows\n\n\nThe inflation_factor tells us that $1 in 1962 could buy about as much as $10.10 in 2023. To take a 1962 income and report it in 2023 dollars, we should multiple it by 10.1. We need to join our data\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10       1826.\n2  1962 p50       4460.\n3  1962 p90      11733.\n# ℹ 183 more rows\n\n\ntogether with inflation.csv by the linking variable year. Use left_join() to merge inflation_factor onto the dataset pivoted. Below is a hypothetical example for the structure.\n\n# Hypothetical example\njoined &lt;- data_A |&gt;\n  left_join(\n    data_B,\n    by = join_by(key_variable_in_A_and_B)\n  )\n\n\n\nShow the code answer\njoined &lt;- pivoted |&gt;\n  left_join(\n    inflation,\n    by = join_by(year)\n  )",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#mutate-to-adjust-for-inflation",
    "href": "topics/data_transformation.html#mutate-to-adjust-for-inflation",
    "title": "Data transformation",
    "section": "mutate() to adjust for inflation",
    "text": "mutate() to adjust for inflation\n\nIn this step, you will use mutate() to multiple income by the inflation_factor\n\nThe mutate() function modifies columns. It can overwrite existing columns or create new columns at the right of the data set. The new variable is some transformation of the old variables.\n\n# Hypothetical example\nold_data |&gt;\n  mutate(new_variable = old_variable_1 + old_variable_2)\n\nUse mutate() to modify income so that it takes the values income * inflation_factor.\n\n\nShow the code answer\nmutated &lt;- joined |&gt;\n  mutate(income = income * inflation_factor)",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#ggplot-to-visualize",
    "href": "topics/data_transformation.html#ggplot-to-visualize",
    "title": "Data transformation",
    "section": "ggplot() to visualize",
    "text": "ggplot() to visualize\nNow make a ggplot() where\n\nyear is on the x-axis\nincome is on the y-axis\nquantity is denoted by color\n\nDiscuss. What do you see in this plot?",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#all-together",
    "href": "topics/data_transformation.html#all-together",
    "title": "Data transformation",
    "section": "All together",
    "text": "All together\nPutting it all together, we have a pipeline that goes from data to the plot.\n\ncps_data |&gt;\n  # Subset to cases working full year\n  filter(wkswork2 == 6) |&gt;\n  # Subset to cases with valid income\n  filter(incwage &gt; 0 & incwage &lt; 99999998) |&gt;\n  # Produce summaries\n  group_by(year) |&gt;\n  summarize(\n    p10 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.1),\n    p50 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.5),\n    p90 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.9\n    ),\n    .groups = \"drop\"\n  ) |&gt;\n  pivot_longer(\n    cols = c(\"p10\",\"p50\",\"p90\"),\n    names_to = \"quantity\",\n    values_to = \"income\"\n  ) |&gt;\n  # Join data for inflation adjustment\n  left_join(\n    read_csv(\"https://soc114.github.io/data/inflation.csv\"),\n    by = join_by(year)\n  ) |&gt;\n  # Apply the inflation adjustment\n  mutate(income = income * inflation_factor) |&gt;\n  # Produce a ggplot\n  ggplot(aes(x = year, y = income, color = quantity)) +\n  geom_line() +\n  xlab(\"Year\") +\n  scale_y_continuous(name = \"Annual Wage and Salary Income\\n(2023 dollars)\",\n                     labels = scales::label_dollar()) +\n  scale_color_discrete(name = \"Percentile of\\nDistribution\",\n                       labels = function(x) paste0(gsub(\"p\",\"\",x),\"th\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#want-to-do-more",
    "href": "topics/data_transformation.html#want-to-do-more",
    "title": "Data transformation",
    "section": "Want to do more?",
    "text": "Want to do more?\nIf you have finished and want to do more, you could\n\nincorporate the educ variable in your plot. You might want to group by those who do and do not hold college degrees, perhaps using facet_grid()\ntry geom_histogram() for people’s incomes in a specific year\nexplore IPUMS-CPS for other variables and begin your own visualization",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#footnotes",
    "href": "topics/data_transformation.html#footnotes",
    "title": "Data transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to past TA Abby Sachar for designing the base of this exercise.↩︎",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/sampling.html",
    "href": "topics/sampling.html",
    "title": "Population Sampling",
    "section": "",
    "text": "This topic is covered on Jan 21 and 23.\nClaims about inequality are often claims about a population. Our data are typically only a sample! This module addresses the link between samples and populations.\nThis page covers two lectures.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#full-count-enumeration",
    "href": "topics/sampling.html#full-count-enumeration",
    "title": "Population Sampling",
    "section": "Full count enumeration",
    "text": "Full count enumeration\nWhat proportion of our class prefers to sit in the front of the room?\nWe answered this question in class using full count enumeration: list the entire target population and ask them the question. Full count enumeration is ideal because it removes all statistical sources of error. But in settings with a larger target population, the high cost of full count enumeration may be prohibitive.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#simple-random-sample",
    "href": "topics/sampling.html#simple-random-sample",
    "title": "Population Sampling",
    "section": "Simple random sample",
    "text": "Simple random sample\nWe carried out a simple random sample1 in class.\n\neveryone generated a random number between 0 and 1\nthose with values less than 0.1 were sampled\nour sample estimate was the proportion of those sampled to prefer the front of the room\n\nIn a simple random sample, each person in the population is sampled with equal probabilities. Because the probabilities are known, a simple random sample is a probability sample.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#unequal-probability-sample",
    "href": "topics/sampling.html#unequal-probability-sample",
    "title": "Population Sampling",
    "section": "Unequal probability sample",
    "text": "Unequal probability sample\nSuppose we want to make subgroup estimates:\n\nwhat proportion prefer the front, among those sitting in the first 3 rows?\nwhat proportion prefer the front, among those sitting in the back 17 rows?\n\nIn a simple random sample, we might only get a few or even zero people in the first 3 rows! To reduce the chance of this bad sample, we could draw an unequal probability sample:\n\nthose in rows 1–3 are selected with probability 0.5\nthose in rows 4–20 are selected with probability 0.1\n\nOur unequal probability sample will over-represent the first three rows, thus creating a large enough sample in this subgroup to yield precise estimates.\n\nHaving drawn an unequal probability sample, suppose we now want to estimate the class-wide proportion who prefer sitting in the front. We will have a problem: those who prefer the front may be more likely to sit there, and they are also sampled with a higher probability! Sample inclusion is related to the value of our outcome.\nBecause the sampling probabilities are known, we can correct for this by applying sampling weights, which for each person equals the inverse of the known probability of inclusion for that person.\nFor those in rows 1–3,\n\nwe sampled with probability 50%\non average 1 in every 2 people is sampled\neach person in the sample represents 2 people in the population\n\\(w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.5} = 2\\)\n\nFor those in rows 4–20,\n\nwe sampled with probability 10%\non average 1 in every 10 people is sampled\neach person in the sample represents 10 people in the population\n\\(w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.1} = 10\\)\n\nTo estimate the population mean, we can use the weighted sample mean,\n\\[\\frac{\\sum_i y_iw_i}{\\sum_i w_i}\\]",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#stratified-random-sample",
    "href": "topics/sampling.html#stratified-random-sample",
    "title": "Population Sampling",
    "section": "Stratified random sample",
    "text": "Stratified random sample\nWe could also draw a stratified random sample by first partitioning the population into subgroups (called strata) and then drawing samples within each subgroup. For instance,\n\nsample 10 of the 20 people in rows 1–3\nsample 10 of the 130 people in rows 4–17\n\nIn simple random or unequal probability sampling, it is always possible that by random chance we sample no one in the front of the room. Stratified random sampling rules this out: we know in advance how our sample will be balanced across the two strata.\n\n\n\n\n\n\nNote\n\n\n\nIn our real-data example at the end of this page, the Current Population Survey is stratified by state so that the Bureau of Labor Statistics knows in advance that they will gather a sufficient sample to estimate unemployment in each state.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#sec-cps",
    "href": "topics/sampling.html#sec-cps",
    "title": "Population Sampling",
    "section": "A real case: The Current Population Survey",
    "text": "A real case: The Current Population Survey\nEvery month, the Bureau of Labor Statistics in collaboration with the U.S. Census Bureau collects data on unemployment in the Current Population Survey (CPS). The CPS is a probability sample designed to estimate the unemployment rate in the U.S. and in each state.\nWe will be using the CPS in discussion. This video introduces the CPS and points you toward where you can access the data via IPUMS-CPS.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#sec-baseball",
    "href": "topics/sampling.html#sec-baseball",
    "title": "Population Sampling",
    "section": "Example: Baseball players",
    "text": "Example: Baseball players\nAs one example where full-count enumeration is possible, we will examine the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by USA Today and are available in baseball.csv.\n\nbaseball &lt;- read_csv(\"https://info3370.github.io/data/baseball.csv\")\n\n\n\n# A tibble: 944 × 4\n  player            team         position   salary\n  &lt;chr&gt;             &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;\n1 Scherzer, Max     N.Y. Mets    RHP      43333333\n2 Verlander, Justin N.Y. Mets    RHP      43333333\n3 Judge, Aaron      N.Y. Yankees OF       40000000\n4 Rendon, Anthony   L.A. Angels  3B       38571429\n5 Trout, Mike       L.A. Angels  OF       37116667\n# ℹ 939 more rows\n\n\nSalaries are high, and income inequality is also high among baseball players\n\n4% were paid the league minimum of $720,000\n53% were paid less than $2,000,000\nthe highest-paid players—Max Scherzer and Justin Verlander—each earned $43,333,333\nthe highest-paid half of players take home 92% of the total pay\n\n\n\n\n\n\n\n\n\n\nPay also varies widely across teams!\n\n\n\n\n\n\n\n\n\n\nConceptualize the sampling strategy\nSuppose you did not have the whole population. You still want to learn the population mean salary! How could you learn that in a sample of 60 out of the 944 players?\nBefore reading on, think through three questions:\n\nWhat would it mean to use each of these strategies?\n\n\na simple random sample of 60 players\na sample stratified by the 30 MLB teams\na sample clustered by the 30 MLB teams\n\n\nWhich strategies have advantages in terms of\n\n\nbeing least expensive?\nhaving the best statistical properties?\n\n\nGiven that you already have the population, how would you write some R code to carry out the sampling strategies? You might use sample_n() and possibly group_by().\n\n\n\nSampling strategies in code\nIn a simple random sample, we draw 60 players from the entire league. Each player’s probability of sample inclusion is \\(\\frac{60}{n}\\) where \\(n\\) is the number of players in the league (944).\n\nsimple_sample &lt;- function(population) {\n  population |&gt;\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 60 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Sample 60 players\n    sample_n(size = 60)\n}\n\nTo use this function, we give it the baseball data as the population and it returns a tibble containing a sample of 60 players.\n\nsimple_sample(population = baseball)\n\n# A tibble: 60 × 6\n   player          team         position   salary p_sampled sampling_weight\n   &lt;chr&gt;           &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1 Clase, Emmanuel Cleveland    RHP       1900000    0.0636            15.7\n 2 Kim, Ha-Seong   San Diego    2B        7000000    0.0636            15.7\n 3 Martin, Chris   Boston       RHP       8000000    0.0636            15.7\n 4 Urias, Julio    L.A. Dodgers LHP      14250000    0.0636            15.7\n 5 Wacha, Michael  San Diego    RHP       7500000    0.0636            15.7\n 6 Miller, Owen    Milwaukee    1B         731100    0.0636            15.7\n 7 Kuhnel, Joel    Cincinnati   RHP        730000    0.0636            15.7\n 8 Miranda, Jose   Minnesota    3B         727850    0.0636            15.7\n 9 Oviedo, Johan   Pittsburgh   RHP        735000    0.0636            15.7\n10 Fried, Max      Atlanta      LHP      13500000    0.0636            15.7\n# ℹ 50 more rows\n\n\nIn a stratified random sample by team, we sample 2 players on each of 30 teams.\nEach player’s probability of sample inclusion is \\(\\frac{2}{n}\\) where \\(n\\) is the number on that player’s team (which ranges from 28 to 35).\n\nstratified_sample &lt;- function(population) {\n  population |&gt;\n    # Draw sample within each team\n    group_by(team) |&gt;\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 2 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Within each team, sample 2 players\n    sample_n(size = 2) |&gt;\n    ungroup()\n}\n\nIn a sample clustered by team, we might first sample 3 teams and then sample 20 players on each sampled team. A clustered sample is often less costly, for example because you would only need to call up the front office of 3 teams instead of 30 teams.\nEach player’s probability of sample inclusion is P(Team Chosen) \\(\\times\\) P(Chosen Within Team) = \\(\\frac{3}{30}\\times\\frac{20}{n}\\) where \\(n\\) is the number on that player’s team (which ranges from 28 to 35).\n\nclustered_sample &lt;- function(population) {\n  \n  # First, sample 3 teams\n  sampled_teams &lt;- population |&gt;\n    # Make one row per team\n    distinct(team) |&gt;\n    # Sample 3 teams\n    sample_n(3) |&gt;\n    # Store those 3 team names in a vector\n    pull()\n  \n  # Then load data on those teams and sample 20 per team\n  population |&gt;\n    filter(team %in% sampled_teams) |&gt;\n    # Define sampling probability and weight\n    group_by(team) |&gt;\n    mutate(\n      p_sampled = (3 / 30) * (20 / n()),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Sample 20 players\n    sample_n(20) |&gt;\n    ungroup()\n}\n\n\n\nWeighted mean estimator\nGiven a sample, how do we estimate the population mean? The weighted mean estimator can also be placed in a function\n\nwe hand our sample to the function\nwe get a numeric estimate back\n\n\nestimator &lt;- function(sample) {\n  sample |&gt;\n    summarize(estimate = weighted.mean(\n      x = salary, \n      w = sampling_weight\n    )) |&gt;\n    pull(estimate)\n}\n\nHere is what it looks like to use the estimator.\n\nsample_example &lt;- simple_sample(population = baseball)\nestimator(sample = sample_example)\n\n[1] 4561528\n\n\nTry it for yourself! The true mean salary in the league is $4,965,481. How close do you come when you apply the estimator to a sample drawn by each strategy?\n\n\nEvaluating performance: Many samples\nWe might like to know something about performance across many repeated samples. The replicate function will carry out a set of code many times.\n\nsample_estimates &lt;- replicate(\n  n = 1000,\n  expr = {\n    a_sample &lt;- simple_sample(population = baseball)\n    estimator(sample = a_sample)\n  }\n)\n\nSimulate many samples. Which one is the best? Strategy A, B, or C?\n\n\n\nThe danger of one sample\nIn actual science, we typically have only one sample. Any estimate we produce from that sample involves some signal about the population quantities, and also some noise. Herein is the danger: researchers are very good at telling stories about why their sample evidence tells something about the population, even when it may be random noise. We illustrate this with an example.\nDoes salary differ between left- and right-handed pitchers? To address this question, I create a tibble with only the pitchers(those for whom the position variable takes the value LHP or RHP).\n\npitchers &lt;- baseball |&gt;\n  filter(position == \"LHP\" | position == \"RHP\")\n\nTo illustrate what can happen with a sample, we now draw a sample. Let’s first set our computer’s random number seed so we get the same sample each time.\n\nset.seed(1599)\n\nThen draw a sample of 40 pitchers\n\npitchers_sample &lt;- pitchers |&gt;\n  sample_n(size = 40)\n\nand examine the mean difference in salary.\n\npitchers_sample |&gt;\n  group_by(position) |&gt;\n  summarize(salary_mean = mean(salary))\n\n# A tibble: 2 × 2\n  position salary_mean\n  &lt;chr&gt;          &lt;dbl&gt;\n1 LHP         9677309.\n2 RHP         3182428.\n\n\nThe left-handed pitchers make millions of dollars more per year! You can probably tell many stories why this might be the case. Maybe left-handed pitchers are needed by all teams, and there just aren’t many available because so few people are left-handed!\nWhat happens if we repeat this process many times? The figure below shows many repeated samples of size 40 from the population of pitchers.\n\n\n\n\n\n\n\n\n\nOur original result was really random noise: we happened by chance to draw a sample with some highly-paid left-handed pitchers!\nThis exercise illustrates what is known as the replication crisis: findings that are surprising in one sample may not hold in other repeated samples from the same population, or in the population as a whole. The replication crisis has many sources. One principal source is the one we illustrated above: sample-based estimates involve some randomness, and well-meaning researchers are (unfortunately) very good at telling interesting stories.\nOne solution to the replication crisis is to pay close attention to the statistical uncertainty in our estimates, such as that from random sampling. Another solution is to re-evaluate findings that are of interest on new samples. In any case, both the roots of the problem and the solutions are closely tied to sources of randomness in estimates, such as those generated using samples from a population.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#the-future-of-sample-surveys",
    "href": "topics/sampling.html#the-future-of-sample-surveys",
    "title": "Population Sampling",
    "section": "The future of sample surveys",
    "text": "The future of sample surveys\nSample surveys served as a cornerstone of social science research from the 1950s to the present. But there are concerns about their future:\n\nsome sampling frames, such as landline telephones, have become obsolete\nresponse rates have been falling for decades\nsample surveys are slower and more expensive than digital data\n\nWhat is the future for sample surveys? How can they be combined with other data?\nWe will close with a discussion of these questions, which you can also engage with in the Groves 2011 reading that follows this module.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#footnotes",
    "href": "topics/sampling.html#footnotes",
    "title": "Population Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, a simple random sample draws units independently with equal probabilities, and with replacement. Our sample is actually drawn without replacement. In an infinite population, the two are equivalent.↩︎",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/weights.html",
    "href": "topics/weights.html",
    "title": "Weights",
    "section": "",
    "text": "When studying population-level inequality, our goal is to draw inference about all units in the population. We want to know about the people in the U.S., not just the people who answer the Current Population Survey. Drawing inference from a sample to a population is most straightforward for a simple random sample: when people are chosen at random with equal probabilities. For simple random samples, the sample average of any variable is an unbiased and consistent estimator of the population average.\nBut the Current Population Survey is not a simple random sample. Neither are most labor force samples! These samples still begin with a sampling frame, but people are chosen with unequal probabilities. We need sample weights to address this fact.\nIn the CPS, a key goal is to estimate unemployment in each state. Every state needs to have enough sample size—even tiny states like Wyoming. In order to make those estimates, the CPS oversamples people who live in small states.\nTo draw good population inference, our analysis must incorporate what we know about how the data were collected. If we ignore the weights, our sample will have too many people from Wyoming and too few people from California. Weights correct for this.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#how-survey-designers-create-weights",
    "href": "topics/weights.html#how-survey-designers-create-weights",
    "title": "Weights",
    "section": "How survey designers create weights",
    "text": "How survey designers create weights\nTo calculate sampling weight on person \\(i\\), those who design survey samples take the ratio \\[\\text{weight on unit }i = \\frac{1}{\\text{probability of including person }i\\text{ in the sample}}\\] You can think of the sampling weight as the number of population members a given sample member represents. If there are 100 people with a 1% chance of inclusion, then on average 1 of them will be in the sample. That person represents \\(\\frac{1}{.01}=100\\) people.\n\n\n\n\n\n\nExample redux: California and Wyoming\n\n\n\nSuppose Californians are sampled with probability 0.0004. Then each Californian represents 1 / 0.0004 = 2,500 people. Each Californian should receive a weight of 2,500. Working out the same math for Wyoming, each Wyoming resident should receive a weight of 250. The total weight on these two samples will then be proportional to the sizes of these two populations.\n\n\nIn practice, weighting is more complicated: survey administrators adjust weights for differential nonresponse across population subgroups (a method called post-stratification). How to construct weights is beyond the scope of this course, and could be a whole course in itself!",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#point-estimates",
    "href": "topics/weights.html#point-estimates",
    "title": "Weights",
    "section": "Point estimates",
    "text": "Point estimates\nWhen we download data, we typically download a column of weights. For simplicity, suppose we are given a sample of four people. The weight column tells us how many people in the population each person represents. The employed column tells us whether each person employed.\n\n\n     name weight employed\n1    Luis      4        1\n2 William      1        0\n3   Susan      1        0\n4  Ayesha      4        1\n\n\nIf we take an unweighted mean, we would conclude that only 50% of the population is employed. But with a weighted mean, we would conclude that 80% of the population is employed! This might be the case if the sample was designed to oversample people at a high risk of unemployment.\n\n\n\n\n\n\n\n\n\nEstimator\nMath\nExample\nResult\n\n\n\n\nUnweighted mean\n\\(=\\frac{\\sum_{i=1}^n Y_i}{n}\\)\n\\(=\\frac{1 + 0 + 0 + 1}{4}\\)\n= 50% employed\n\n\nWeighted mean\n\\(=\\frac{\\sum_{i=1}^n w_iY_i}{\\sum_{i=1}^n w_i}\\)\n\\(=\\frac{4*1 + 1*0 + 1*0 + 4*1}{4 + 1 + 1 + 4}\\)\n= 80% employed\n\n\n\nIn R, the weighted.mean(x, w) function will calculate weighted means where x is an argument for the outcome variable and w is an argument for the weight variable.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#standard-errors",
    "href": "topics/weights.html#standard-errors",
    "title": "Weights",
    "section": "Standard errors",
    "text": "Standard errors\nAs you know from statistics, our sample mean is unlikely to equal the population mean. There is random variation in which people were chosen for inclusion in our sample, and this means that across hypothetical repeated samples we would get different sample means! You likely learned formulas to create a standard errors, which quantifies how much a sample estimator would move around across repeated samples.\nUnfortunately, the formula you learned doesn’t work for complex survey samples! Simple random samples (for which those formulas hold) are actually quite rare. When you face a complex survey sample, those who administer the survey might provide\n\na vector of \\(n\\) weights for making a point estimate\na matrix of \\(n\\times k\\) replicate weights for making standard errors\n\nBy providing \\(k\\) different ways to up- and down-weight various observations, the replicate weights enable you to generate \\(k\\) estimates that vary in a way that mimics how the estimator might vary if applied to different samples from the population. For instance, our employment sample might come with 3 replicate weights.\n\n\n     name weight employed repwt1 repwt2 repwt3\n1    Luis      4        1      3      5      3\n2 William      1        0      1      2      2\n3   Susan      1        0      3      1      1\n4  Ayesha      4        1      5      3      4\n\n\nThe procedure to use replicate weights depends on how they are constructed. Often, it is relatively straightforward:\n\nuse weight to create a point estimate \\(\\hat\\tau\\)\nuse repwt* to generate \\(k\\) replicate estimates \\(\\hat\\tau^*_1,\\dots,\\hat\\tau^*_k\\)\ncalculate the standard error of \\(\\hat\\tau\\) using the replicate estimates \\(\\hat\\tau^*\\). The formula will depend on how the replicate weights were constructed, but it will likely involve the standard deviation of the \\(\\hat\\tau^*\\) multiplied by some factor\nconstruct a confidence interval1 by a normal approximation \\[(\\text{point estimate}) \\pm 1.96 * (\\text{standard error estimate})\\]\n\nIn our concrete example, the point estimate is 80% employed. The replicate estimates are 0.67, 0.73, 0.70. Variation across the replicate estimates tells us something about how the estimate would vary across hypothetical repeated samples from the population.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#computational-strategy-for-replicate-weights",
    "href": "topics/weights.html#computational-strategy-for-replicate-weights",
    "title": "Weights",
    "section": "Computational strategy for replicate weights",
    "text": "Computational strategy for replicate weights\nUsing replicate weights can be computationally tricky! It becomes much easier if you write an estimator() function. Your function accepts two arguments\n\ndata is the tibble containing the data\nweight_name is the name of a column containing the weight to be used (e.g., “repwt1”)\n\nExample. If our estimator is the weighted mean of employment,\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    summarize(\n      estimate = weighted.mean(\n        x = employed,\n        # extract the weight column\n        w = sim_rep |&gt; pull(weight_name)\n      )\n    ) |&gt; \n    # extract the scalar estimate\n    pull(estimate)\n}\n\nIn the code above, sim_rep |&gt; pull(weight_name) takes the data frame sim_rep and extracts the weight variable that is named weight_name. There are other ways to do this also.\nWe can now apply our estimator to get a point estimate with the main sampling weight,\n\nestimate &lt;- estimator(data = sim_rep, weight_name = \"weight\")\n\nwhich yields the point estimate 0.80. We can use the same function to produce the replicate estimates,\n\nreplicate_estimates &lt;- c(\n  estimator(data = sim_rep, weight_name = \"repwt1\"),\n  estimator(data = sim_rep, weight_name = \"repwt2\"),\n  estimator(data = sim_rep, weight_name = \"repwt3\")\n)\n\nyielding the three estimates: 0.67, 0.73, 0.70. In real data, you will want to apply this in a loop because there may be dozens of replicate weights.\nThe standard error of the estimator will be some function of the replicate estimates, likely involving the standard deviation of the replicate estimates. Check with the data distributor for a formula for your case. Once you estimate the standard error, a 95% confidence interval can be constructed with a Normal approximation, as discussed above.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#application-in-the-cps",
    "href": "topics/weights.html#application-in-the-cps",
    "title": "Weights",
    "section": "Application in the CPS",
    "text": "Application in the CPS\nStarting in 2005, the CPS-ASEC samples include 160 replicate weights. If you download replicate weights for many years, the file size will be enormous. We illustrate the use of replicate weights with a question that can be explored with only one year of data: among 25-year olds in 2023, how did the proportion holding four-year college degrees differ across those identifying as male and female?\nWe first load some packages, including the foreach package which will be helpful when looping through replicate weights.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(foreach)\n\nTo answer our research question, we download 2023 CPS-ASEC data including the variables sex, educ, age, the weight variable asecwt, and the replicate weights repwtp*.\n\ncps_data &lt;- read_dta(\"../data_raw/cps_00079.dta\")\n\nWe then define an estimator to use with these data. It accepts a tibble data and a character weight_name identifying the name of the weight variable, and it returns a tibble with two columns: sex and estimate for the estimated proportion with a four-year degree.\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    # Define focal_weight to hold the selected weight\n    mutate(focal_weight = data |&gt; pull(weight_name)) |&gt;\n    # Restrict to those age 25+\n    filter(age &gt;= 25) |&gt;\n    # Restrict to valid reports of education\n    filter(educ &gt; 1 & educ &lt; 999) |&gt;\n    # Define a binary outcome: a four-year degree\n    mutate(college = educ &gt;= 110) |&gt;\n    # Estimate weighted means by sex\n    group_by(sex) |&gt;\n    summarize(estimate = weighted.mean(\n      x = college,\n      w = focal_weight\n    ))\n}\n\nWe produce a point estimate by applying that estimator with the asecwt.\n\nestimate &lt;- estimator(data = cps_data, weight_name = \"asecwt\")\n\n\n\n# A tibble: 2 × 2\n  sex        estimate\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;\n1 1 [male]      0.369\n2 2 [female]    0.397\n\n\nUsing the foreach package, we apply the estimator 160 times—once with each replicate weight—and use the argument .combine = \"rbind\" to stitch results together by rows.\n\nlibrary(foreach)\nreplicate_estimates &lt;- foreach(r = 1:160, .combine = \"rbind\") %do% {\n  estimator(data = cps_data, weight_name = paste0(\"repwtp\",r))\n}\n\n\n\n# A tibble: 320 × 2\n   sex        estimate\n   &lt;dbl+lbl&gt;     &lt;dbl&gt;\n 1 1 [male]      0.368\n 2 2 [female]    0.396\n 3 1 [male]      0.371\n 4 2 [female]    0.400\n 5 1 [male]      0.371\n 6 2 [female]    0.397\n 7 1 [male]      0.369\n 8 2 [female]    0.397\n 9 1 [male]      0.370\n10 2 [female]    0.398\n# ℹ 310 more rows\n\n\nWe estimate the standard error of our estimator by a formula \\[\\text{StandardError}(\\hat\\tau) = \\sqrt{\\frac{4}{160}\\sum_{r=1}^{160}\\left(\\hat\\tau^*_r - \\hat\\tau\\right)^2}\\] where the formula comes from the survey documentation. We carry out this procedure within groups defined by sex, since we are producing estimate for each sex.\n\nstandard_error &lt;- replicate_estimates |&gt;\n  # Denote replicate estimates as estimate_star\n  rename(estimate_star = estimate) |&gt;\n  # Merge in the point estimate\n  left_join(estimate,\n            by = join_by(sex)) |&gt;\n  # Carry out within groups defined by sex\n  group_by(sex) |&gt;\n  # Apply the formula from survey documentation\n  summarize(standard_error = sqrt(4 / 160 * sum((estimate_star - estimate) ^ 2)))\n\n\n\n# A tibble: 2 × 2\n  sex        standard_error\n  &lt;dbl+lbl&gt;           &lt;dbl&gt;\n1 1 [male]          0.00280\n2 2 [female]        0.00291\n\n\nFinally, we combine everything and construct a 95% confidence interval by a Normal approximation.\n\nresult &lt;- estimate |&gt;\n  left_join(standard_error, by = \"sex\") |&gt;\n  mutate(ci_min = estimate - 1.96 * standard_error,\n         ci_max = estimate + 1.96 * standard_error)\n\n\n\n# A tibble: 2 × 5\n  sex        estimate standard_error ci_min ci_max\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1 [male]      0.369        0.00280  0.364  0.375\n2 2 [female]    0.397        0.00291  0.391  0.403\n\n\nWe use ggplot() to visualize the result.\n\nresult |&gt;\n  mutate(sex = as_factor(sex)) |&gt;\n  ggplot(aes(\n    x = sex, \n    y = estimate,\n    ymin = ci_min, \n    ymax = ci_max,\n    label = scales::percent(estimate)\n  )) +\n  geom_errorbar(width = .2) +\n  geom_label() +\n  scale_x_discrete(\n    name = \"Sex\", \n    labels = str_to_title\n  ) +\n  scale_y_continuous(name = \"Proportion with 4-Year College Degree\") +\n  ggtitle(\n    \"Sex Disparities in College Completion\",\n    subtitle = \"Estimates from the 2023 CPS-ASEC among those age 25+\"\n  )\n\n\n\n\n\n\n\n\nWe conclude that those identifying as female are more likely to hold a college degree. Because we can see the confidence intervals generated using the replicate weights, we are reasonably confident in the statistical precision of our point estimates.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#footnotes",
    "href": "topics/weights.html#footnotes",
    "title": "Weights",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf we hypothetically drew many complex survey samples from the population in this way, an interval generated this way would contain the true population mean 95% of the time.↩︎",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Probability samples",
      "Weights"
    ]
  },
  {
    "objectID": "topics/sample_splitting.html",
    "href": "topics/sample_splitting.html",
    "title": "Sample Splitting",
    "section": "",
    "text": "A predictive model is an input-output function:\nThe performance of a model can be measured by how well the output \\(\\hat{y}\\) corresponds to the true outcome \\(y\\). This page considers three ways to assess the performance of a model.\nOften, the purpose a predictive model is to accomplish out-of-sample prediction (2). But when learning the model, often only one sample is available. Therefore, evaluation by split-sample prediction (3) is often desirable because it most closely mimics this task.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Sample Splitting"
    ]
  },
  {
    "objectID": "topics/sample_splitting.html#simulated-setting",
    "href": "topics/sample_splitting.html#simulated-setting",
    "title": "Sample Splitting",
    "section": "Simulated setting",
    "text": "Simulated setting\nWhen a model is evaluated by in-sample prediction, there is a danger: even if the features have no predictive value in the population, a model might discover patterns that exist in the training sample due to random variation.\nTo illustrate this, we generate a simulation with 100 features x1,…,x100 and one outcome y, all of which are independent normal variables. We know from the beginning that x* should be useless predictors: they contain no information about y.\nWe first load tidyverse\nand write a function to generate the data\n\ngenerate_data &lt;- function(sample_size = 1000, num_features = 100) {\n  # Predictors are independent Normal\n  X &lt;- replicate(num_features, rnorm(sample_size))\n  colnames(X) &lt;- paste0(\"x\",1:num_features)\n  \n  as_tibble(X) |&gt;\n    # Outcome is an independent Normal\n    mutate(y = rnorm(n()))\n}\n\nbefore applying that function to generate one sample.\n\ndata &lt;- generate_data(sample_size = 1000, num_features = 100)",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Sample Splitting"
    ]
  },
  {
    "objectID": "topics/sample_splitting.html#in-sample-prediction",
    "href": "topics/sample_splitting.html#in-sample-prediction",
    "title": "Sample Splitting",
    "section": "In-sample prediction",
    "text": "In-sample prediction\nWe then estimate a linear regression model, where . includes all the x1,…,x100 features other than y as predictors.\n\nmodel &lt;- lm(y ~ ., data = data)\n\nand a benchmark of no model, which only includes an intercept.\n\nno_model &lt;- lm(y ~ 1, data = data)\n\nWe know from the simulation that the model is useless: the x-variables contain no information about y. But if we make predictions in-sample, we will see that the mean squared error of the model is surprisingly lower (better) than no model.\n\ndata |&gt;\n  mutate(\n    predicted_model = predict(model),\n    predicted_no_model = predict(no_model),\n    squared_error_model = (y - predicted_model) ^ 2,\n    squared_error_no_model = (y - predicted_no_model) ^ 2,\n  ) |&gt;\n  select(starts_with(\"squared\")) |&gt;\n  summarize_all(.funs = mean)\n\n# A tibble: 1 × 2\n  squared_error_model squared_error_no_model\n                &lt;dbl&gt;                  &lt;dbl&gt;\n1               0.902                   1.00",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Sample Splitting"
    ]
  },
  {
    "objectID": "topics/sample_splitting.html#out-of-sample-prediction",
    "href": "topics/sample_splitting.html#out-of-sample-prediction",
    "title": "Sample Splitting",
    "section": "Out-of-sample prediction",
    "text": "Out-of-sample prediction\nThe problem is that the model fit to the noise in the data. We can see this with the out-of-sample performance assessment. First, we generate a new dataset of out-of-sample data.\n\nout_of_sample &lt;- generate_data()\n\nThen, we use the model learned in data and predict in out_of_sample. By this evaluation, predictions are now worse (higher mean squared error) than no model.\n\nout_of_sample |&gt;\n  mutate(\n    predicted_model = predict(model, newdata = out_of_sample),\n    predicted_no_model = predict(no_model, newdata = out_of_sample),\n    squared_error_model = (y - predicted_model) ^ 2,\n    squared_error_no_model = (y - predicted_no_model) ^ 2,\n  ) |&gt;\n  select(starts_with(\"squared\")) |&gt;\n  summarize_all(.funs = mean)\n\n# A tibble: 1 × 2\n  squared_error_model squared_error_no_model\n                &lt;dbl&gt;                  &lt;dbl&gt;\n1                1.05                  0.981",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Sample Splitting"
    ]
  },
  {
    "objectID": "topics/sample_splitting.html#split-sample-prediction",
    "href": "topics/sample_splitting.html#split-sample-prediction",
    "title": "Sample Splitting",
    "section": "Split-sample prediction",
    "text": "Split-sample prediction\nIn practice, we often do not have a second sample. We can therefore mimic the out-of-sample task by a sample split, which we can create using the initial_split function in the rsample package,\n\nlibrary(rsample)\nsplit &lt;- initial_split(data, prop = .5)\n\nwhich randomly assigns the data into training and testing sets of equal size. We can extract those data by typing training(split) and testing(split).\nThe strategy is to learn on training(split)\n\nmodel &lt;- lm(y ~ ., data = training(split))\nno_model &lt;- lm(y ~ 1, data = training(split))\n\nand evaluate performance on testing(split).\n\ntesting(split) |&gt;\n  mutate(\n    predicted_model = predict(model, newdata = testing(split)),\n    predicted_no_model = predict(no_model, newdata = testing(split)),\n    squared_error_model = (y - predicted_model) ^ 2,\n    squared_error_no_model = (y - predicted_no_model) ^ 2,\n  ) |&gt;\n  select(starts_with(\"squared\")) |&gt;\n  summarize_all(.funs = mean)\n\n# A tibble: 1 × 2\n  squared_error_model squared_error_no_model\n                &lt;dbl&gt;                  &lt;dbl&gt;\n1                1.29                   1.01\n\n\nJust like the out-of-sample prediction, this shows that the model is worse than no model at all. By sample splitting, we can learn this even when we have only one sample.\nAn important caveat is that sample splitting has a cost: the number of cases available for training is smaller once we split the sample. This can mean that the sample-split predictions will have worse performance than predictions trained on the full sample and evaluated out-of-sample.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Sample Splitting"
    ]
  },
  {
    "objectID": "topics/sample_splitting.html#repeating-this-many-times",
    "href": "topics/sample_splitting.html#repeating-this-many-times",
    "title": "Sample Splitting",
    "section": "Repeating this many times",
    "text": "Repeating this many times\nIf we repeat the above many times, we can see the distribution of these performance evaluation strategies across repeated samples.\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\n\n\n\n\n\n\n\nBy the gold standard of out-of-sample prediction, no model is better than a model. In-sample prediction yields the misleading appearance that a model is better than no model. Split-sample prediction successfully mimics the out-of-sample behavior when only one sample is available.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Sample Splitting"
    ]
  },
  {
    "objectID": "topics/sample_splitting.html#closing-thoughts",
    "href": "topics/sample_splitting.html#closing-thoughts",
    "title": "Sample Splitting",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nSample splitting is an art as much as a science. In particular applications, the gain from sample splitting is not always clear and must be balanced against the reduction in cases available for training. It is important to remember that out-of-sample prediction remains the gold standard, and sample splitting is one way to approximate that when only one sample is available.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Sample Splitting"
    ]
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "Discussion Meetings",
    "section": "",
    "text": "This page contains a general plan for the topics of the 10 discussion section meetings that will occur this quarter.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#jan-79",
    "href": "discussion.html#jan-79",
    "title": "Discussion Meetings",
    "section": "Jan 7–9",
    "text": "Jan 7–9\nIn this discussion, we will introduce ourselves and ensure our computers are prepared for the class. This will involve checking that we have R, RStudio, tinytex, and tidyverse installed. See R basics.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#jan-1416",
    "href": "discussion.html#jan-1416",
    "title": "Discussion Meetings",
    "section": "Jan 14–16",
    "text": "Jan 14–16\nOn Thursday, lecture will use data simulated to resemble the Current Population Survey. In this discussion, you will register for an account to access the actual data. Click here for detailed instructions.\nYour TA may walk through how to look at the survey documentation for one or more of the variables. If time allows, we will break into small groups to look for other variables that might be interesting in a project, and then come back together to tell the class about them.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#jan-2123",
    "href": "discussion.html#jan-2123",
    "title": "Discussion Meetings",
    "section": "Jan 21–23",
    "text": "Jan 21–23\nLast week’s lecture introduced visualization with the ggplot() function in the tidyverse package. This discussion explores more of what ggplot() can do.\n\nForm groups of about 4 students\nIntroduce yourself to your group members\nChoose a graph that your group likes in Ch 1 of R For Data Science.\n\nLook at the code for that graph\nDiscuss what each element of the code is doing\nChoose someone to present to the class\n\nAt the end of discussion, we will regroup. Each group will present the code for their chosen graph to the class",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#jan-2830",
    "href": "discussion.html#jan-2830",
    "title": "Discussion Meetings",
    "section": "Jan 28–30",
    "text": "Jan 28–30\nYou should read the following paper before this discussion:\n\nEngland, Paula, Andrew Levine, and Emma Mishel. 2020. Progress toward gender equality in the United States has slowed or stalled, PNAS 117(13):6990–6997.\n\nThe problem set reproduces findings from this paper. The discussion will focus on conceptual questions from the reading. The questions below are only guidelines, and your TA might have other topics to discuss.\n1. The authors write that “change in the gender system has been deeply asymmetric.” Explain this in a sentence or two to someone who hasn’t read the article.\n2. The authors discuss cultural changes that could lead to greater equality. Propose a question that could (hypothetically) be included in the CPS-ASEC questionnaire to help answer questions about cultural changes.\n\n\n\n\n\n\nTip\n\n\n\nIf you are not sure how to word a survey question, here are some examples from the American Time Use Survey, Current Population Survey, and General Social Survey.\n\n\n3. The authors discuss institutional changes that could lead to greater equality. Propose a question that could (hypothetically) be included in the CPS-ASEC questionnaire to help answer questions about institutional changes.\n4. What was one fact presented in this paper that most surprised you?\n5. What about the graphs in this paper is compelling? What would you improve or change if you were producing these graphs?",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#feb-46",
    "href": "discussion.html#feb-46",
    "title": "Discussion Meetings",
    "section": "Feb 4–6",
    "text": "Feb 4–6\nYour TA will introduce the final project, which will be carried out in groups of about 5 students within your discussion section. Your TA will help you to form groups with common interest areas.\nPossible topics include (but are not limited to) inequality by race or gender, trends in poverty or inequality over time, and the causal effect of education or other life experiences on subsequent outcomes. Another possibility is to form groups centered on a particular dataset, with the topic to be decided later.\nAll subsequent discussions are devoted to group work on the final project.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#feb-1113",
    "href": "discussion.html#feb-1113",
    "title": "Discussion Meetings",
    "section": "Feb 11–13",
    "text": "Feb 11–13\nBy the end of this discussion, your group should select a dataset for the final project. If you aren’t sure where to start, we suggest the options at ipums.org.\nIf you finish early, you are always welcome to move on to the tasks below.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#feb-1820",
    "href": "discussion.html#feb-1820",
    "title": "Discussion Meetings",
    "section": "Feb 18–20",
    "text": "Feb 18–20\nBy the end of this discussion, your group should\n\nselect an outcome variable\ndefine the unit of analysis (e.g., a person, a school, a state) for which that outcome variable is defined\ndetermine how you plan to aggregate the outcome variable across units (e.g., by a mean, median, proportion)\n\nIf you finish early, you are always welcome to move on to the tasks below.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#feb-2527",
    "href": "discussion.html#feb-2527",
    "title": "Discussion Meetings",
    "section": "Feb 25–27",
    "text": "Feb 25–27\nBy the end of this discussion, your group should discuss the target population and whether you will need to use sampling weights in your analysis.\nIf you finish early, you are always welcome to move on to the tasks below.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#mar-46",
    "href": "discussion.html#mar-46",
    "title": "Discussion Meetings",
    "section": "Mar 4–6",
    "text": "Mar 4–6\nThis discussion is open for your group to work on any remaining tasks to finish up your final project. Recall that PDF slides and writeup are due by 5pm on Mar 7, with one submission per group.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#mar-1113",
    "href": "discussion.html#mar-1113",
    "title": "Discussion Meetings",
    "section": "Mar 11–13",
    "text": "Mar 11–13\nYour group will present your final project in this discussion. During each presentation by a group that is not your own, you will be asked to note something you appreciated about their presentation.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#other-discussion-possibilities",
    "href": "discussion.html#other-discussion-possibilities",
    "title": "Discussion Meetings",
    "section": "Other discussion possibilities",
    "text": "Other discussion possibilities\nThis part of the page has other possibilities for discussion topics. We may use these in one of the discussions depending on how the course material is progressing over the quarter.\n\nExplain a function\nThis discussion explores more of what the tidyverse can do.\n\nForm groups of about 4 students\nIntroduce yourself to your group members\nChoose a function in Ch 3 of R For Data Science that we did not cover explicitly in class.\n\nPossibilities include: arrange(), distinct(), rename(), relocate(), the slice_ functions, ungroup()\nFind an example of the function in the chapter.\nDiscuss how the code works.\nPrepare to explain to the class.\n\nAt the end of discussion, we will regroup. Each group will present the code for their chosen graph to the class\n\n\n\nWrite a function\nNow that we have worked with functions in R, it is time to understand them on a deeper level. In this exercise, we will write our own functions.\n\nA basic function\nYou can store a function as an object in your environment, just like any other object. The function below accepts a numeric variable x and returns twice the value of x.\n\ndouble_x &lt;- function(x) {\n  doubled &lt;- 2 * x\n  return(doubled)\n}\n\nThere are a few pieces to the code above\n\nWe created a function that has a name: double_x\nOur function that takes one argument named x\nThe body of the function is the lines within the {}. These lines take the argument, do some things, and then return() a result. The object within return() is what R sends back after running the function.\n\nOnce you create that function, you can run it just like any other function.\n\ndouble_x(x = 2)\n\n[1] 4\n\n\n\n\nA function with two arguments\nA function can also take multiple arguments, such as one that adds x and y.\n\nadd_x_y &lt;- function(x, y) {\n  added &lt;- x + y\n  return(added)\n}\n\nwhich works as follows.\n\nadd_x_y(x = 2, y = 3)\n\n[1] 5\n\n\n\n\nChallenge: Write your own function\nA function does not have to just take numbers as an argument. It can also take a dataset as an argument. Sometimes, we might want an estimator(data) function that takes data as an argument and applies an estimator() to that data, to return an estimate of something. You will create one such function here.\nAs an example, suppose three surveys ask people if they prefer chocolate ice cream (prefers_chocolate = TRUE) or vanilla ice cream (prefers_chocolate = FALSE). The survey also records whether the respondent is a child age = \"child\" or an adult age = \"adult\".\n\nlibrary(tidyverse)\nsample_a &lt;- tibble(\n  age = c(\"child\",\"child\",\"child\",\"adult\",\"adult\"),\n  prefers_chocolate = c(T,T,F,F,F)\n)\nsample_b &lt;- tibble(\n  age = c(\"child\",\"child\",\"adult\",\"adult\"),\n  prefers_chocolate = c(T,F,T,F)\n)\nsample_c &lt;- tibble(\n  age = c(\"child\",\"child\",\"child\",\"adult\",\"adult\",\"adult\"),\n  prefers_chocolate = c(T,T,F,F,F,T)\n)\n\nWe want to know the proportion preferring chocolate among children and adults in each sample. To estimate in sample_a, we would write\n\nestimate &lt;- sample_a |&gt;\n  group_by(age) |&gt;\n  summarize(prefers_chocolate = mean(prefers_chocolate))\n\nNow write that within a function.\n\nestimator &lt;- function(data) {\n  # do some things to data\n  # return() your estimate\n}\n\nand apply the estimator to sample_a, sample_b, and sample_c.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "topics/questions_data_can_answer.html",
    "href": "topics/questions_data_can_answer.html",
    "title": "Questions data can answer",
    "section": "",
    "text": "Data science should begin with clarity about what questions data can (and cannot) answer. As a motivating example, we will discuss questions about inequality as introduced in p. 49–53 of this paper:\nWe will emphasize two key ideas in this paper:"
  },
  {
    "objectID": "topics/questions_data_can_answer.html#choosing-a-summary-of-a-distribution",
    "href": "topics/questions_data_can_answer.html#choosing-a-summary-of-a-distribution",
    "title": "Questions data can answer",
    "section": "Choosing a summary of a distribution",
    "text": "Choosing a summary of a distribution\nThe histogram below visualizes the distribution of annual household income across U.S. households as estimated from the 2022 Current Population Survey. To produce this graph, we categorized households into discrete income groups that are each $25,000 wide. The height of each bar corresponds to the number of households falling in that income group. We can see that the most common household income values are below $100,000, but a small number of households have very high incomes that create a long upper tail at the right.\n\n\n\n\n\n\n\n\n\nWe often want to take a distribution and convert it to a single-number summary. One way to summarize the distribution is by its median: the value at which 50% of households have higher incomes and 50% of households have lower incomes.\n\n\n\n\n\n\n\n\n\nThe median is a useful measure of central tendency: it gives a sense of the income value in the middle of the distribution. But it may not give us a good sense of inequality, which requires some sense of the spread of the distribution. The graph below presents an alternative summary. The 90th percentile is the household income value such that 90% of households have lower incomes. The 10th percentile is the value such that 10% of households have lower incomes. The 90/10 income ratio (\\(\\frac{\\text{90th percentile}}{\\text{10th percentile}}\\)) is one measure of inequality: how many dollars does a household at the 90th percentile get for every dollar that goes to a household at the 10th percentile? We might say that inequality is high to the degree that the 90/10 income ratio is large.\n\n\n\n\n\n\n\n\n\nThe median and the 90/10 income ratio are only two of many ways to summarize a distribution. At the end of this course, you will produce your own visualization that can have any summary you want. Before analyzing data, it is often useful to ask ourselves: what summary do we want to produce? Ideally, you will produce a summary that is informative and perhaps surprising to your readers."
  },
  {
    "objectID": "topics/questions_data_can_answer.html#normative-and-objective-claims",
    "href": "topics/questions_data_can_answer.html#normative-and-objective-claims",
    "title": "Questions data can answer",
    "section": "Normative and objective claims",
    "text": "Normative and objective claims\nInequality is a topic that brings out normative commitments: beliefs about how the world should be. Many of us study inequality because of our own moral beliefs in fairness, equality, and opportunity. Inequality is also a topic full of objective facts, such as quantitative evidence describing the U.S. household income distribution. When studying inequality, it is important to distinguish between normative claims (what should be) and objective claims (what is).\n\nNormative claims: An illustration with manna\nNormative claims are the focus of moral philosophy. Because this is a course on data science, we will mostly avoid normative claims. To illustrate the difference between the kinds of approaches useful in the two settings, we consider a story from the Hebrew scriptures as retold by Jencks (2002).\nIn the story, the Israelites are wandering in the desert without food. God provides a form of bread (manna) that falls down from heaven every day. People do not work for their manna; it is provided freely by God. Taking this example, Jencks considers a normative question about inequality: how should the manna be distributed among the people?\n\n\n\nThe Israelites Collecting Manna from Heaven. Austria. About 1400–1410. Unknown artist. Getty Open Content Program. getty.edu/art/collection/object/105T6R\n\n\nJencks presents a utilitarian moral argument. Suppose that (1) we want to maximize overall societal welfare. Suppose also that (2) each person gets diminishing welfare returns to additional amounts of manna: the first pound of manna relieves you from starvation, the second pound is a bit less helpful, and the third pound even less than that. Under premises (1) and (2), it would make no sense to give lots of manna to a few people while others go hungry. Instead, the welfare-maximizing distribution of manna would be one in which each person receives an equal share.\nThe story of the manna illustrates the elements of a moral argument. One must argue for premises, such as the notion that we should maximize welfare and the notion that each person receives diminishing returns from manna. A debate could happen by challenging some of these premises, such as by arguing that a child might need less manna than a full-grown adult to achieve the same level of welfare. Applying these arguments to present-day inequality is further complicated by the fact that money is earned through work instead of falling freely from the sky. Moral arguments about inequality are important and should be the subject of more thorough treatments in classes on moral and political philosophy.\n\n\nObjective claims\nWhile we will refrain from explicit normative arguments, we will produce objective claims about inequality that we or our readers might find normatively troubling. For example, it might be concerning to us that a household at the 90th percentile receives $14 for every $1 received by a household at the 10th percentile. People’s normative commitments are often bound up in their objective beliefs about the world. Consider a person who believes that high inequality is desirable in order to promote economic growth. This person might be surprised by an objective fact: the countries with high levels of economic inequality are not necessarily the same countries that have high levels of economic output per capita. We will visualize this objective fact in the pages to come.\nAs a data scientist, your task is to produce statistical summaries that help people better understand the world. As a scholar of inequality, you might hope that your objective evidence will be troubling to people with normative commitments such as beliefs in equality and fairness. But we will focus on making precise objective claims."
  },
  {
    "objectID": "assignments/pset1.html",
    "href": "assignments/pset1.html",
    "title": "Problem Set 1: Visualization",
    "section": "",
    "text": "Due: 5pm on Friday, January 17.\nStudent identifer: [type your anonymous identifier here]\nThis problem set involves both data analysis and reading.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "assignments/pset1.html#visualize-40-points",
    "href": "assignments/pset1.html#visualize-40-points",
    "title": "Problem Set 1: Visualization",
    "section": "1. Visualize (40 points)",
    "text": "1. Visualize (40 points)\n\n\n\n\n\n\nTip\n\n\n\nFunctions named in this problem are links to helper pages that provide documentation.\n\n\nUse ggplot to visualize these data. To denote the different trajectories,\n\nmake your plot using geom_point() or geom_line()\nuse the x-axis for age\nuse the y-axis for income\nuse color for quantity\nuse facet_grid to make a panel of facets where each row is an education value and each column is a cohort value\n\nYou should prepare the graph as though you were going to publish it. Modify the axis titles so that a reader would know what is on the axis. Use appropriate capitalization in all labels. Optionally, try using the label_currency() function from the scales package so that the y-axis uses dollar values.\nYour code should be well-formatted as defined by R4DS. In your produced PDF, no lines of code should run off the page.\nMany different graphs can be equally correct. You will be evaluated by\n\nhaving publication-ready graph aesthetics\ncode that follows style conventions\n\n\n# your code goes here",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "assignments/pset1.html#understand-components-of-research-question",
    "href": "assignments/pset1.html#understand-components-of-research-question",
    "title": "Problem Set 1: Visualization",
    "section": "2. Understand components of research question",
    "text": "2. Understand components of research question\nThe graph above answered a descriptive research question. Here are a few components of that question:\n\nAnnual income in 2022 dollars\nAmerican workers in each subgroup defined by birth cohort, age, and education\n10th, 50th, and 90th percentile\nA person\n\nFor 2.1–2.4, write the letter of the corresponding component of the research question.\n2.1 (2.5 points) What is the unit of analysis?\n2.2 (2.5 points) What is the outcome variable?\n2.3 (2.5 points) What are the target populations?\n2.4 (2.5 points) What are the summary statistics?",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "assignments/pset1.html#recap-and-connections-to-your-project",
    "href": "assignments/pset1.html#recap-and-connections-to-your-project",
    "title": "Problem Set 1: Visualization",
    "section": "Recap and connections to your project",
    "text": "Recap and connections to your project\nThe visualization you produced in this problem set is an example of the kind of visualization you will produce in the final project. As in this problem set, your visualization in the project should include well-written labels to make the graph readable. A difference between the two is that in this problem set we provided a dataset in which each row was already a summary statistic. In the final project, you will manipulate data in which each row corresponds to a unit of analysis that you aggregate to produce a summary statistic. This is a skill you will practice on the next problem set.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "assignments/pset2.html",
    "href": "assignments/pset2.html",
    "title": "Problem Set 2: Data Transformation",
    "section": "",
    "text": "Due: 5pm on Friday, Jan 31.\nStudent identifer: [type your anonymous identifier here]\nThis problem set draws on the following paper.\nA note about sex and gender. As we have discussed in class, sex typically refers to categories assigned at birth (e.g., female, male). Gender is a performed construct with many possible values: man, woman, nonbinary, etc. The measure in the CPS-ASEC is “sex,” coded male or female. We will use these data to study sex disparities between those identifying as male and female. The paper at times uses “gender” to refer to this construct.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "assignments/pset2.html#data-analysis-existing-question",
    "href": "assignments/pset2.html#data-analysis-existing-question",
    "title": "Problem Set 2: Data Transformation",
    "section": "1. Data analysis: Existing question",
    "text": "1. Data analysis: Existing question\n25 points. Reproduce Figure 1 from the paper.\nVisit cps.ipums.org to download data from the 1962–2023 March Annual Social and Economic Supplement. When choosing samples, you want all samples in the “ASEC” tab and none of the samples in the “Basic Monthly” tab. Include these variables in your cart: sex, age, asecwt, empstat.\nTo reduce extract size, select cases to those ages 25–54. Before submitting your extract, we recommend changing the data format to “Stata (.dta)” so that you get value labels.\n\n\n\n\n\n\nTip\n\n\n\nLook ahead: you will later study a new outcome of your own choosing. You could add it to your cart now if you want.\n\n\nOn your computer, analyze these data.\n\nfilter to asecwt &gt; 0 (see paper footnote on p. 6995 about negative weights)\nmutate to create an employed variable indicating that empstat == 10 | empstat == 12\nmutate to convert sex to a factor variable using as_factor\ngroup by sex and year\nsummarize the proportion employed: use weighted.mean to take the mean of employed using the weight asecwt\n\nYour figure will be close but not identical to the original. Yours will include some years that the original did not. Feel free to change aesthetics of the plot, such as the words used in labels. For example, it would be more accurate to the data to label the legend “Sex” with values “Male” and “Female.”\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(haven)",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "assignments/pset2.html#a-new-outcome",
    "href": "assignments/pset2.html#a-new-outcome",
    "title": "Problem Set 2: Data Transformation",
    "section": "2. A new outcome",
    "text": "2. A new outcome\n25 points. The CPS-ASEC has numerous variables. Pick another variable of your choosing. Add it to your cart in IPUMS, and visualize how that variable has changed over time for those identifying as male and female.\nAs in the previous plot, year should be on the x-axis and color should represent sex. The y-axis is up to you. You can examine something like median income, proportion holding college degrees, or the 90th percentile of usual weekly work hours. You can restrict to some subset if you want, such as those who are employed.\nYour answer should include\n\na written statement of what you estimated: the variable you chose, any sample restrictions you made, and how you summarized that variable\na written interpretation of what you found\ncode following style conventions\nyour publication-quality visualization",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "assignments/pset2.html#recap-and-connections-to-your-project",
    "href": "assignments/pset2.html#recap-and-connections-to-your-project",
    "title": "Problem Set 2: Data Transformation",
    "section": "Recap and connections to your project",
    "text": "Recap and connections to your project\nA few skills relevant to the project were practiced in this problem set. In your project, you should clearly define your unit of analysis and target population. You should use sample weights if appropriate. And you should write readable, well-organized code that carries out your data transformation and visualization.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Science",
    "section": "",
    "text": "Together, we will use tools from data science to answer social science questions. As an area of application, we will focus on questions about inequality and social stratification.",
    "crumbs": [
      " ",
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Social Data Science",
    "section": "Learning goals",
    "text": "Learning goals\nAs a result of participating in this course, students will be able to\n\nvisualize economic inequality with graphs that summarize survey data\nconnect theories about inequality to quantitative empirical evidence\nevaluate the effects of hypothetical interventions to reduce inequality\nconduct data analysis using the R programming language",
    "crumbs": [
      " ",
      "Home"
    ]
  },
  {
    "objectID": "topics/working_with_data.html",
    "href": "topics/working_with_data.html",
    "title": "Working with data",
    "section": "",
    "text": "This topic is covered on Jan 7.\n\nData science is a world of abundant opportunities. Over recent decades, the field has been transformed by the rapid growth of both computational power and available data. With the proliferation of new technological advances and data opportunities, a social scientist might reasonably ask: where do I begin?\nA good place to begin is to first\n\nChoose a question data can answer\nWork with data to answer the question\n\nWe will first talk about how to ask a good question that data can answer. Then, we will begin preparing our computers with software and tools to answer those questions. Over the quarter, we will develop the skills to ask our own questions and find data to answer them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Working with Data"
    ]
  },
  {
    "objectID": "topics/asking_questions.html",
    "href": "topics/asking_questions.html",
    "title": "Asking questions with data",
    "section": "",
    "text": "slides\nAsking your own research question is fun, and it is also daunting. There is no formula for asking a good question, and much research involves trial and error. In this lecture, we will discuss together what makes a good research question.",
    "crumbs": [
      " ",
      "Working with Data",
      "Asking questions with data"
    ]
  },
  {
    "objectID": "topics/asking_questions.html#some-characteristics-of-a-good-question",
    "href": "topics/asking_questions.html#some-characteristics-of-a-good-question",
    "title": "Asking questions with data",
    "section": "Some characteristics of a good question",
    "text": "Some characteristics of a good question\nWhile good questions come in many forms, good research questions often has a few characteristics.\n\nA unit of analysis\nThe unit of analysis is the bedrock of a good research question. What will a row of your dataset represent? While the answer may seem straightforward, many units of analysis are possible: a person, a school, a neighborhood, a country, etc. In settings with repeated observations, the unit of analysis might be a person observed at a particular age: Jose at age 29 may be one row of your dataset and Jose at age 30 may be another row. The unit of analysis is the unit at which the outcome is defined, so getting it right is important.\n\n\nA clear population\nA good research question addresses a population that is relevant to the author’s argument. A well-defined population could be a set of units, such as American residents in a certain age range. It could be the set of all students at Cornell. It could also be a set of aggregate entities, such as the population of all four-year colleges and universities in the U.S., where each unit in the population is a college or university.\nWhen the population is well-defined, a reader should be able to imagine making a list of all the units in that population. When presented with a new unit and asked whether that unit is part of the population, you should be able to confidently answer “yes” or “no” and not “maybe.”\nIt is rarely satisfying to answer the question “who is being studied?” with the answer “the people in my sample.” While full-count enumerations of interesting populations exist, usually your sample is a subset of a broader population of interest. Good research tells us who that population is.\n\n\nA precise outcome\nFor each unit in the population, there is an outcome. This might be the income of an individual person, or the graduation rate of an individual college. A good research question tells you what the outcome is, and why it matters.\n\n\nThe potential for surprising results\nThis may be the hardest to pin down. Before you work with data, try to tell a story about why the results might go one way or another way. Perhaps you study a particular population of employed people where there is one set of reasons to expect men to earn more and another set of reasons to expect women to earn more. Try to convince yourself that the results could go either way! This builds excitement about the empirical answer, because it shows that the answer is really unknown before data analysis. Write down your arguments because they will also help readers be excited about your questions.\nQuestions where only one answer is plausible are often tedious. Questions that are exciting are often the ones where results could surprise you by going several possible directions.",
    "crumbs": [
      " ",
      "Working with Data",
      "Asking questions with data"
    ]
  },
  {
    "objectID": "slides/m_discussion_poststratification_dag.html",
    "href": "slides/m_discussion_poststratification_dag.html",
    "title": "m_discussion_poststratification_dag",
    "section": "",
    "text": "DAGs are not just useful for causal inference: they can be useful whenever we need to know whether one variable is statistically independent of another. This is true, for example, when drawing inference about a population from a sample.\nA researcher uses an opt-in online web survey to draw inference about support for President Biden. They ask respondents: ``Do you approve of President Biden’s performance in office?’’ with the answer choices Yes/No. The researcher also gathers data on two demographic characteristics: whether the respondent completed college and current employment. They write:\n\nMy sample is not representative. Suppose for every person in the population, \\(S\\) denotes whether they are included in my sample. Then \\(S\\) is related to their approval of President Biden (\\(Y\\)).\nHowever, I believe my sample is representative when I look at a set of people who all take the same value along college completion and employment, such as those who finished college and are currently employed. If these variables are \\(X_1,X_2\\), I believe this independence statement: \\(S\\) is independent of \\(Y\\) given \\(X_1,X_2\\). I will therefore get population estimates by a procedure with several steps: use my sample to estimate the mean outcome \\(E(Y\\mid \\vec{X} = \\vec{x})\\) in each stratum, then use Census data to estimate the size of each stratum \\(P(\\vec{X} = \\vec{x})\\) in the population, then estimate \\(E(Y) = \\sum_{\\vec{x}}E(Y\\mid \\vec{X} = \\vec{x})P(\\vec{X} = \\vec{x})\\).\n\nThis researcher’s reasoning is a common strategy known as post-stratification. This question is about formalizing a set of conditions under which the researcher is right and wrong.\nBefore you begin, we want to emphasize one aspect of the researcher’s assumption that is different from the exchangeability assumption for causal inference.\n\nfor causal claims, we assume conditional exchangeability: \\(A\\) independent of \\(Y^a\\) given \\(\\vec{X}\\)\n\ninvolves the potential outcome \\(Y^a\\)\nholds if the only unblocked paths between \\(A\\) and \\(Y\\) are causal paths\n\nfor sample-to-population inference, we assume conditionally independent sampling \\(S\\) independent of \\(Y\\) given \\(\\vec{X}\\)\n\ninvolves the factual outcome \\(Y\\); there is no intervention here\nholds if there are no unblocked paths between \\(S\\) and \\(Y\\)\n\n\nAlthough the assumption is different, the principles of DAGs are still relevant.\n\n\nDraw a DAG under which the researcher’s claim is valid. Use \\(S,Y,X_1,X_2\\).\n\n\n\nIn a sentence or two, explain your DAG from 3.1 to the researcher. Tell us in words what is meant by each edge in your DAG.\n\n\n\nDraw a DAG showing a counterexample under which the researcher’s claim is invalid.\n\n\n\nIn a sentence or two, explain your DAG from 3.3 to the researcher. Tell us particularly about the path that creates a statistical dependence between \\(S\\) and \\(Y\\)."
  },
  {
    "objectID": "slides/m_discussion_poststratification_dag.html#using-dags-in-a-new-context",
    "href": "slides/m_discussion_poststratification_dag.html#using-dags-in-a-new-context",
    "title": "m_discussion_poststratification_dag",
    "section": "",
    "text": "DAGs are not just useful for causal inference: they can be useful whenever we need to know whether one variable is statistically independent of another. This is true, for example, when drawing inference about a population from a sample.\nA researcher uses an opt-in online web survey to draw inference about support for President Biden. They ask respondents: ``Do you approve of President Biden’s performance in office?’’ with the answer choices Yes/No. The researcher also gathers data on two demographic characteristics: whether the respondent completed college and current employment. They write:\n\nMy sample is not representative. Suppose for every person in the population, \\(S\\) denotes whether they are included in my sample. Then \\(S\\) is related to their approval of President Biden (\\(Y\\)).\nHowever, I believe my sample is representative when I look at a set of people who all take the same value along college completion and employment, such as those who finished college and are currently employed. If these variables are \\(X_1,X_2\\), I believe this independence statement: \\(S\\) is independent of \\(Y\\) given \\(X_1,X_2\\). I will therefore get population estimates by a procedure with several steps: use my sample to estimate the mean outcome \\(E(Y\\mid \\vec{X} = \\vec{x})\\) in each stratum, then use Census data to estimate the size of each stratum \\(P(\\vec{X} = \\vec{x})\\) in the population, then estimate \\(E(Y) = \\sum_{\\vec{x}}E(Y\\mid \\vec{X} = \\vec{x})P(\\vec{X} = \\vec{x})\\).\n\nThis researcher’s reasoning is a common strategy known as post-stratification. This question is about formalizing a set of conditions under which the researcher is right and wrong.\nBefore you begin, we want to emphasize one aspect of the researcher’s assumption that is different from the exchangeability assumption for causal inference.\n\nfor causal claims, we assume conditional exchangeability: \\(A\\) independent of \\(Y^a\\) given \\(\\vec{X}\\)\n\ninvolves the potential outcome \\(Y^a\\)\nholds if the only unblocked paths between \\(A\\) and \\(Y\\) are causal paths\n\nfor sample-to-population inference, we assume conditionally independent sampling \\(S\\) independent of \\(Y\\) given \\(\\vec{X}\\)\n\ninvolves the factual outcome \\(Y\\); there is no intervention here\nholds if there are no unblocked paths between \\(S\\) and \\(Y\\)\n\n\nAlthough the assumption is different, the principles of DAGs are still relevant.\n\n\nDraw a DAG under which the researcher’s claim is valid. Use \\(S,Y,X_1,X_2\\).\n\n\n\nIn a sentence or two, explain your DAG from 3.1 to the researcher. Tell us in words what is meant by each edge in your DAG.\n\n\n\nDraw a DAG showing a counterexample under which the researcher’s claim is invalid.\n\n\n\nIn a sentence or two, explain your DAG from 3.3 to the researcher. Tell us particularly about the path that creates a statistical dependence between \\(S\\) and \\(Y\\)."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The course is divided into three broad sections:\n\nJan 7–16: Working with data\nJan 21–Feb 6: Inference without models\nFeb 11 onward: Inference with models\n\nEach date will be a link to the slides for that lecture.\n\n\n\nDate\nCourse Section\nWebsite Page\n\n\n\n\nJan 7\nWelcome\n\n\n\n\nAsking questions\n\n\n\n\nSetting up your computer\n\n\n\nJan 9\nVisualization\n\n\n\nJan 14 & Jan 16\nData transformation\n\n\n\nThu, Jan 16\nData transformation\n\n\n\nTue, Jan 21\nPopulation sampling\n\n\n\nThu, Jan 23\nPopulation sampling\n\n\n\nTue, Jan 28\nDefining counterfactuals\n\n\n\nThu, Jan 30\nExchangeability and experiments\n\n\n\nTue, Feb 4\nExchangeability and observational studies\n\n\n\nThu, Feb 6\nExchangeability and observational studies\n\n\n\nTue, Feb 11\nWhy model?\n\n\n\nThu, Feb 13\nProperties of estimators\n\n\n\nTue, Feb 18\nEstimating the properties of estimators\n\n\n\nThu, Feb 20\nModeling for causal inference: Outcome model\n\n\n\nTue, Feb 25\nModeling for causal inference: IPTW\n\n\n\nThu, Feb 27\nMachine learning as a plug-in: Descriptive and causal\n\n\n\nTue, Mar 4\nMachine learning: Penalized regression\n\n\n\nThu, Mar 6\nMachine learning: Trees and forests\n\n\n\nTue, Mar 11\nMachine learning vs regression: A case study (FF Challenge)\n\n\n\nThu, Mar 13\nCourse recap"
  },
  {
    "objectID": "schedule.html#lecture-topics",
    "href": "schedule.html#lecture-topics",
    "title": "Schedule",
    "section": "",
    "text": "The course is divided into three broad sections:\n\nJan 7–16: Working with data\nJan 21–Feb 6: Inference without models\nFeb 11 onward: Inference with models\n\nEach date will be a link to the slides for that lecture.\n\n\n\nDate\nCourse Section\nWebsite Page\n\n\n\n\nJan 7\nWelcome\n\n\n\n\nAsking questions\n\n\n\n\nSetting up your computer\n\n\n\nJan 9\nVisualization\n\n\n\nJan 14 & Jan 16\nData transformation\n\n\n\nThu, Jan 16\nData transformation\n\n\n\nTue, Jan 21\nPopulation sampling\n\n\n\nThu, Jan 23\nPopulation sampling\n\n\n\nTue, Jan 28\nDefining counterfactuals\n\n\n\nThu, Jan 30\nExchangeability and experiments\n\n\n\nTue, Feb 4\nExchangeability and observational studies\n\n\n\nThu, Feb 6\nExchangeability and observational studies\n\n\n\nTue, Feb 11\nWhy model?\n\n\n\nThu, Feb 13\nProperties of estimators\n\n\n\nTue, Feb 18\nEstimating the properties of estimators\n\n\n\nThu, Feb 20\nModeling for causal inference: Outcome model\n\n\n\nTue, Feb 25\nModeling for causal inference: IPTW\n\n\n\nThu, Feb 27\nMachine learning as a plug-in: Descriptive and causal\n\n\n\nTue, Mar 4\nMachine learning: Penalized regression\n\n\n\nThu, Mar 6\nMachine learning: Trees and forests\n\n\n\nTue, Mar 11\nMachine learning vs regression: A case study (FF Challenge)\n\n\n\nThu, Mar 13\nCourse recap"
  },
  {
    "objectID": "schedule.html#assignments",
    "href": "schedule.html#assignments",
    "title": "Schedule",
    "section": "Assignments",
    "text": "Assignments\n\n\n\nDue Date\nAssignment\n\n\n\n\nFri, Jan 10 at 5pm\nProblem Set 0\n\n\nFri, Jan 17 at 5pm\nProblem Set 1\n\n\nFri, Jan 24 at 5pm\nPeer Review 1\n\n\nFri, Jan 31 at 5pm\nProblem Set 2\n\n\nFri, Feb 07 at 5pm\nPeer Review 2\n\n\nFri, Feb 14 at 5pm\nProblem Set 3\n\n\nFri, Feb 21 at 5pm\nPeer Review 3\n\n\nFri, Feb 28 at 5pm\nProblem Set 4\n\n\nFri, Mar 07 at 5pm\nProject\n\n\nFri, Mar 14 at 5pm\nProject Peer Review"
  },
  {
    "objectID": "forms.html",
    "href": "forms.html",
    "title": "Forms",
    "section": "",
    "text": "Assignment extension form. If you are experiencing an exceptional circumstance that may warrant an extension, use this to request one.\nRegrade request form. If you think we have graded something wrong, you can report it on this form. Responses are only accepted for within 72 hours of grade post.\nExcused absence form. To request an excused absence from lecture or discussion.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Final Project",
    "section": "",
    "text": "The culmination of the course is a group research project. You will\nYou can answer any question about inequality (broadly defined) using the ideas we learned in this course.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#what-you-will-submit",
    "href": "assignments/project.html#what-you-will-submit",
    "title": "Final Project",
    "section": "What you will submit",
    "text": "What you will submit\nThere are two submitted items, both due 5pm on Friday, Mar 7.\nWriteup. A .qmd document compiled into a PDF. It should include all code that produces your results. This must be no more than 1,000 words and will contain 1 or more visualizations.\nSlides. A PDF that you will present in discussion. Keep text to a minimum in the slides. You should plan to have all group members speak during the presentation. Your presentation should be 10 minutes or less.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#group-structure",
    "href": "assignments/project.html#group-structure",
    "title": "Final Project",
    "section": "Group structure",
    "text": "Group structure\nWe will form groups of about 5 students each within discussion sections. Groups will be formed during discussion in the middle of the quarter.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#key-components-of-the-project",
    "href": "assignments/project.html#key-components-of-the-project",
    "title": "Final Project",
    "section": "Key components of the project",
    "text": "Key components of the project\nYour goal should be to tell us a story using the data. What do we learn by studying this outcome, aggregated this way, in these subgroups from this population?\nA few points should be part of your writeup:\n\nDefine your unit of analysis\nDefine your target population\n\nMotivate: why is this population interesting to study?\n\nDescribe how your sample was chosen from that population\n\nthis may be a probability sample, such as those available via IPUMS. If so, tell us a little bit about the sampling design\nthis may be a convenience sample. If so, why does it speak to the population and what are the limitations?\nthis may be data on the entire population, as in our baseball example\n\nChoose an outcome variable, which is defined for each unit in the population. Your outcome variable can be a factual variable (\\(Y\\)) for descriptive claims, or a potential outcome (\\(Y^a\\)) or difference in potential outcomes (\\(Y^1-Y^0\\)) for causal claims.\n\nexample: annual wage and salary income\n\nChoose one or more variables on which to create population subgroups\n\nexample: subgroups defined by sex (male and female)\n\nChoose a summary statistic, which aggregates the outcome distribution to one summary per subgroup\n\nexamples: proportion, mean, median, 90th percentile\n\nWrite with clarity on causal assumptions\n\nif your goal is causal with potential outcomes, draw a DAG in which those potential outcomes can be identified by an assumption of conditional exchangeability. If there are unmeasured confounders, discuss how their omission from your DAG may threaten the credibility of your estimates. It is ok if the assumptions are doubtful as long as you write them down clearly.\nif your goal is descriptive with only factual outcomes, then write your results carefully using only descriptive language. As a heuristic, if your goal is descriptive then you should not be using the sentence structure “X [verb] Y”, such as “going to college increases earnings.” This claim suggests a college graduate would have earned less if they had not gone to college—a counterfactual outcome we did not see. For truly descriptive claims, you should phrase more like: “There is a difference in earnings among those who did and did not go to college.” A heuristic to recognize a non-causal claim is that it can be phrased it in an “among” statement: “Among subgroup A, we find ___. Among subgroup B, we find ___.” Or “There is a disparity in Y across subgroups defined by X.”\n\nVisualize your findings in a ggplot",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#considerations-to-bear-in-mind",
    "href": "assignments/project.html#considerations-to-bear-in-mind",
    "title": "Final Project",
    "section": "Considerations to bear in mind",
    "text": "Considerations to bear in mind\n\nWeights. If your sample is drawn from the population with unequal probabilities, you should use sampling weights\nModels. If your question involves many subgroups (e.g., ages) with few observations in each subgroup, you can (but are not required to) use a statistical model to estimate your summary statistic in the subgroup by a predicted value. For example, you could use OLS to predict the proportion mean income at each age. If you do this, you should report the predicted value of the summary statistic, not the coefficients of the model.\nAggregation. Your data must begin with units (e.g., people) who you aggregate into subgroups (e.g., age groups). Your data might come pre-aggregated, such as data where each row contains data for all students in a particular college or university. Then you would need to aggregate further, such as to produce summaries for private versus public universities.\nDropped cases. As you move from raw data to the data that produce your graph, you might drop cases on the way. For example, some cases may have missing values on key predictors. Report how many are dropped, and why. Our goal here is transparent, open science.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#have-fun",
    "href": "assignments/project.html#have-fun",
    "title": "Final Project",
    "section": "Have fun",
    "text": "Have fun\nAs a teaching team, the project is our favorite part of the course. Preparing you to succeed in the project has been (in some sense) the entire goal of all that precedes the project in the course. We hope you will find joy in answering questions with data, as we do.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/pset4.html",
    "href": "assignments/pset4.html",
    "title": "Problem Set 4: Statistical Learning",
    "section": "",
    "text": "Due: 5pm on Friday, Feb 28.\nThis problem set has not yet been posted.\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office Hours",
    "section": "",
    "text": "Person\nTime\nLocation\n\n\n\n\nNathan\nT 1:30–2:30pm\nHaines A55\n\n\nIan\nTh 1–2pm\nHaines 241C\n\n\nChristina\nTh 2–3pm\nHaines A58-C\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "slides/m_discussion_sailing.html",
    "href": "slides/m_discussion_sailing.html",
    "title": "Untitled",
    "section": "",
    "text": "You are looking into a sailing class through UCLA Recreation! For each claim below, tell us whether the claim is causal or descriptive.\n2.1 (5 points) \nLast year, there was a survey of students who did and did not take the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher among those who took the class.\nAnswer. Your answer here\n2.2 (5 points) \nLast year, there was a survey of students before and after the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher in the survey taken after the class.\nAnswer. Your answer here\n2.3 (5 points) \nOn average, the students in this class emerged more prepared to sail than they would have been without the class.\nAnswer. Your answer here"
  },
  {
    "objectID": "slides/m_discussion_sailing.html#a-sailing-class",
    "href": "slides/m_discussion_sailing.html#a-sailing-class",
    "title": "Untitled",
    "section": "",
    "text": "You are looking into a sailing class through UCLA Recreation! For each claim below, tell us whether the claim is causal or descriptive.\n2.1 (5 points) \nLast year, there was a survey of students who did and did not take the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher among those who took the class.\nAnswer. Your answer here\n2.2 (5 points) \nLast year, there was a survey of students before and after the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher in the survey taken after the class.\nAnswer. Your answer here\n2.3 (5 points) \nOn average, the students in this class emerged more prepared to sail than they would have been without the class.\nAnswer. Your answer here"
  },
  {
    "objectID": "topics/interventions.html",
    "href": "topics/interventions.html",
    "title": "Causal Interventions",
    "section": "",
    "text": "If we want to reduce inequality, we want to know the effect of an intervention: what would happen if some treatment went differently?\nAs a running illustration, this section considers the rapid expansion of higher education over the 20th century. We will discuss how completion of a college degree causally increases economic outcomes, especially for students unlikely to finish college.\nThis material draws on the following book:\n\nBrand, Jennie E. 2023. Overcoming the Odds: The Benefits of Completing College for Unlikely Graduates. Russell Sage Foundation.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Randomized experiments",
      "Causal Interventions"
    ]
  },
  {
    "objectID": "who_we_are.html",
    "href": "who_we_are.html",
    "title": "Who We Are",
    "section": "",
    "text": "Get to know a little bit about our teaching team! For office hours, see the Syllabus.\n\n\n\n\n\n\n\n\nIan Lundberg\nianlundberg@ucla.edu\n(he / him)\nWorking with data to understand inequality brings me joy and meaning, as I first discovered as a college student years ago. I hope to share that joy with you! Other joys of mine include hiking, surfing, and oatmeal with blueberries.\n\n\n\n\n\n\n\n\nChristina Wilmot\ncwilmot@ucla.edu (she / they)\nI am interested in studying the many ways that technology and society interact. My background is in computer science and software engineering, so I really enjoy bringing those tools to sociologists. Outside of work I enjoy crafting, gaming, and obsessing over my two adorable cats.\n\n\n\n\n\nNathan Hoffmann\nnathanihoff@g.ucla.edu (he / him)\nI’m a PhD candidate in sociology who uses statistics and machine learning to study international migration, education, and sexuality. I also like cooking, biking, and concerts."
  },
  {
    "objectID": "assignments/pset0.html",
    "href": "assignments/pset0.html",
    "title": "Problem Set 0: Computing environment",
    "section": "",
    "text": "Due: 5pm on Friday, January 10.\nTo complete the problem set, download pset0.qmd. Add these four lines of code to the top if they do not appear in your download,\n---\ntitle: \"Problem Set 0: Computing environment\"\nformat: pdf\n---\nand render to a PDF. You will then submit two things:",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#help-with-getting-started",
    "href": "assignments/pset0.html#help-with-getting-started",
    "title": "Problem Set 0: Computing environment",
    "section": "Help with getting started",
    "text": "Help with getting started\nIf you are stuck, make sure that you first follow the installation instructions in setting up your computer.\nOpen the downloaded problem set in RStudio. You should then click the “Render” button as shown in the image below.\n This will run the R code and combine the output with the text in the document into a PDF. By default, this PDF will be output in the same directory that you saved pset0.qmd in. You will then submit the PDF on Canvas.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#issues",
    "href": "assignments/pset0.html#issues",
    "title": "Problem Set 0: Computing environment",
    "section": "Issues",
    "text": "Issues\nIf you run into issues while attempting to render the Problem Set, be sure to open a question on Piazza!",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "topics/r_basics.html",
    "href": "topics/r_basics.html",
    "title": "Setting up your computer",
    "section": "",
    "text": "This topic is covered on Jan 7.\nIn order to begin producing objective evidence about inequality, you will first need to prepare your computer to analyze data.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#install-statistical-software-r",
    "href": "topics/r_basics.html#install-statistical-software-r",
    "title": "Setting up your computer",
    "section": "Install statistical software: R",
    "text": "Install statistical software: R\nWe will write code in the R programming language. R is available as open-source software at https://cran.r-project.org/. The first step to set up your computer is to install R.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#install-the-interface-rstudio",
    "href": "topics/r_basics.html#install-the-interface-rstudio",
    "title": "Setting up your computer",
    "section": "Install the interface RStudio",
    "text": "Install the interface RStudio\nWe will work with R using an interface called RStudio, which makes it easy to write code and see results all in one place. You should install RStudio Desktop, which is available to download here: https://posit.co/download/rstudio-desktop/",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#install-the-tidyverse-package",
    "href": "topics/r_basics.html#install-the-tidyverse-package",
    "title": "Setting up your computer",
    "section": "Install the tidyverse package",
    "text": "Install the tidyverse package\nMany R functions are made freely available in open-source packages that contain sets of functions designed to carry out common tasks. One package we will use often is the tidyverse, which contains functions to manipulate and visualize data. To install tidyverse, first open RStudio. Find the Console, which is a place where you can type code to immediately execute.\n\nIn the console type,\n\ninstall.packages(\"tidyverse\")\n\nand press enter or return on your keyboard. This runs a line of code to install a set of software packages.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#install-tinytex-to-produce-pdf-reports",
    "href": "topics/r_basics.html#install-tinytex-to-produce-pdf-reports",
    "title": "Setting up your computer",
    "section": "Install tinytex to produce PDF reports",
    "text": "Install tinytex to produce PDF reports\nHomework assignments will be submitted in PDF form. We will learn how to use RStudio to produce a PDF documents that embed your code, results, and written responses. In order to do so, your computer needs to have some version of LaTeX, which is software that typesets documents. Some versions of LaTeX are large and difficult to install. If you have never used LaTeX on your computer, we recommend that you install as follows: paste the code below into your R console and press enter or return to install a minimal version of the software.\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nStudents often find this step confusing, and computers present various errors. If you have an error, look on Piazza to see if anyone else has encountered your error. If not, then post a screenshot of your error on Piazza so we can help you to resolve the problem.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#support-and-guidance",
    "href": "topics/r_basics.html#support-and-guidance",
    "title": "Setting up your computer",
    "section": "Support and guidance",
    "text": "Support and guidance\nCongratulations on preparing your computing environment!\nThroughout the first part of the course, we will often use the online textbook R for Data Science by Hadley Wickham as a reference. The book will introduce how to work with data using R and RStudio. If you want additional guidance for setting up the software, see the Prerequisites section of R4DS. To learn more about RStudio, visit the RStudio User Guide.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "assignments/pset3.html",
    "href": "assignments/pset3.html",
    "title": "Problem Set 3: Causal Inference",
    "section": "",
    "text": "Due: 5pm on Friday, February 14.\nStudent identifer: [type your anonymous identifier here]",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "assignments/pset3.html#potential-outcomes-in-a-hypothetical-setting",
    "href": "assignments/pset3.html#potential-outcomes-in-a-hypothetical-setting",
    "title": "Problem Set 3: Causal Inference",
    "section": "1. Potential outcomes in a hypothetical setting",
    "text": "1. Potential outcomes in a hypothetical setting\nJose says that coming to UCLA caused him to discover sociology, and it became his major! He says that if he had gone to UCSD, he would have stuck with biology.\n1.1 (5 points)\nIn Jose’s claim, what is the treatment?\nAnswer. Your answer here\n1.2 (5 points)\nUsing the mathematical notation we discussed in class, define the two potential outcomes to which Jose is referring.\nAnswer. Your answer here\n1.3 (5 points) \nIn a sentence or two, say how the Fundamental Problem of Causal Inference applies to Jose’s claim.\nAnswer. Your answer here\n1.4 (5 points) \nUsing conditional expectations or probabilities, write the following in math: the probability of majoring in sociology is higher among students who attend UCLA than among students who attend UCSD.\nAnswer. Your answer here\n1.5 (5 points) \nUsing conditional expectations or probabilities, write the following in math: the probability of majoring in sociology would be higher if we intervened to send students to UCLA than if we intervened to send them to UCSD.\nAnswer. Your answer here",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "assignments/pset3.html#potential-outcomes-in-a-real-experiment",
    "href": "assignments/pset3.html#potential-outcomes-in-a-real-experiment",
    "title": "Problem Set 3: Causal Inference",
    "section": "2. Potential outcomes in a real experiment",
    "text": "2. Potential outcomes in a real experiment\nThis problem is based on:\nBertrand, M & Mullainathan, S. 2004. “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” American Economic Review 94(4):991–1013.\nRead the first 10 pages of the paper (through the end of section 2). In this paper,\n\nthe unit of analysis is a resume submitted to a job opening\nthe treatment is the name at the top of the resume\nthe outcome is whether the employer called or emailed back for an interview\n\n2.1. (5 points) Fundamental Problem \nOne submitted resume had the name “Emily Baker.” It yielded a callback. The same resume could have had the name “Lakisha Washington.” Explain how the Fundamental Problem of Causal Inference applies to this case (1–2 sentences).\n2.2. (5 points) Exchangeability \nIn a sentence, what is the exchangeability assumption in this study? For concreteness, for this question you may suppose that the only names in the study were “Emily Baker” and “Lakisha Washington.” Be sure to explicitly state the treatment and the potential outcomes.\n2.3 (15 points) Analyzing the experimental data \nLoad packages that our code will use.\n\nlibrary(tidyverse)\nlibrary(haven)\n\nDownload the study’s data from OpenICPSR: https://www.openicpsr.org/openicpsr/project/116023/version/V1/view. This will require creating an account and agreeing to terms for using the data ethically. Put the data in the folder on your computer where this .Rmd is located. Read the data into R using read_dta.\n\nd &lt;- read_dta(\"lakisha_aer.dta\")\n\n\nIf you have an error, you might need to set your working directory first. This tells R where to look for data files. At the top of RStudio, click Session -&gt; Set Working Directory -&gt; To Source File Location.\n\nYou will now see d in your Global Environment at the top right of RStudio.\nWe will use two variables:\n\n\n\nName\nRole\nValues\n\n\n\n\ncall\noutcome\n1 if resume submission yielded a callback\n\n\n\n\n0 if not\n\n\nrace\ncategory of treatments\nb if first name signals Black\n\n\n\n\nw if first name signals white\n\n\n\nThe top of Table 1 reports callback rates: 9.65% for white names and 6.45% for Black names. Reproduce those numbers. Write code that reproduces these numbers.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "topics/learning_exercise.html",
    "href": "topics/learning_exercise.html",
    "title": "Learning Exercise",
    "section": "",
    "text": "Gender inequality in employment is much greater among new parents than among non-parents. This exercise seeks to estimate the proportion employed among married men and women1 with a 1-year-old child at home. Our data include those with at least one child age 0–18.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Learning Exercise"
    ]
  },
  {
    "objectID": "topics/learning_exercise.html#synthetic-data",
    "href": "topics/learning_exercise.html#synthetic-data",
    "title": "Learning Exercise",
    "section": "Synthetic data",
    "text": "Synthetic data\nTo speed data access, we downloaded data from the basic monthly Current Population Survey for all months from 2010–2019. We processed these data, grouped by sex and age of the youngest child, and estimated the proportion employed. We then generated synthetic data: we created a new dataset for you to use with simulated people using these known probabilities.\nSynthetic data is good in our setting for two reasons\n\nwe know the answer\nyou can download the synthetic data right from this website\n\nFor transparency, here is the code with which we created the synthetic data. The line below will load the synthetic data.\n\nparents &lt;- read_csv(\"https://info3370.github.io/data/parents.csv\")\n\nYour synthetic data intentionally omits any parents with child age 1! Here is a graph showing the averages in your data, grouped by child age and sex.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Learning Exercise"
    ]
  },
  {
    "objectID": "topics/learning_exercise.html#your-task-predict-at-child-age-1",
    "href": "topics/learning_exercise.html#your-task-predict-at-child-age-1",
    "title": "Learning Exercise",
    "section": "Your task: Predict at child age 1",
    "text": "Your task: Predict at child age 1\nYour task is to answer the question: what proportion are employed among female respondents whose youngest child is 1 year old?\n\nyou can use a model with the other ages\nyou can use strategies from the statistical learning page\nyou can use the male respondents if you think they are helpful\n\nNear the end of discussion, we will ask every table to make one estimate. Then we will reveal the truth and see who is closest!",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Learning Exercise"
    ]
  },
  {
    "objectID": "topics/learning_exercise.html#one-approach-to-the-task",
    "href": "topics/learning_exercise.html#one-approach-to-the-task",
    "title": "Learning Exercise",
    "section": "One approach to the task",
    "text": "One approach to the task\nOne approach is to estimate a linear regression model with child_age interacted with sex. We would first create a fitted model object,\n\nmodel &lt;- lm(at_work ~ sex * child_age, data = parents)\n\nthen define the target population to predict\n\ntarget_population &lt;- tibble(sex = \"female\", child_age = 0)\n\nand report a predicted value for the employment rate of female respondents with a 1-year-old youngest child.\n\npredict(model, newdata = target_population)\n\n        1 \n0.5468421",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Learning Exercise"
    ]
  },
  {
    "objectID": "topics/learning_exercise.html#footnotes",
    "href": "topics/learning_exercise.html#footnotes",
    "title": "Learning Exercise",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEach married pair need not be of different sex. The data include same-sex couples.↩︎",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Learning Exercise"
    ]
  },
  {
    "objectID": "topics/visualization.html",
    "href": "topics/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "This topic is covered on Jan 14.\nVisualizing data is an essential skill for data science. We will write our first code to visualize how countries’ level of economic output is related to their level of inequality. We will use data reported in tabular form in Jencks 2002 Table 1, made available in digital form in jencks_table1.csv.",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#a-motivating-question-economic-output-and-inequality",
    "href": "topics/visualization.html#a-motivating-question-economic-output-and-inequality",
    "title": "Visualization",
    "section": "A motivating question: Economic output and inequality",
    "text": "A motivating question: Economic output and inequality\n\nIs economic output higher in countries with higher levels of income inequality? Some argue that this would be the case, for example if high levels of income inequality provide an incentive for hard work and innovation. But is it true?\n\n\n\n\n\n\n\n\n\nTo begin answering this question, we first have to define measures of economic output (the \\(y\\)-axis) and inequality (the \\(x\\)-axis).\n\nMeasuring economic output\nWe use Gross Domestic Product (GDP) Per Capita as a measure of economic output. This measure captures the total economic production of a country divided by the population of the country. For ease of comparison, our data normalizes GDP per capita by dividing by the U.S. GDP per capita. In our data, Sweden’s GDP per capita is recorded as 0.68, indicating that Sweden’s GDP per capita was 68% as high as the U.S. GDP per capita at the time the data were collected.\n\n\nMeasuring inequality\nTo measure inequality, we consider a measure that asks how many times higher an income at the top of the distribution is compared with an income at the bottom of the distribution. Our measure is called the 90/10 income ratio. We will illustrate this concept using the 2022 U.S. household income distribution, visualized below.\n\n\n\n\n\n\n\n\n\nThis graph is a histogram. The width of each bar in the histogram is $25k. The height of each bar shows the number of households with incomes falling within the range of household incomes (x-axis) that correspond to the width of the bar.\nThere are two summary statistics: the 90th percentile in blue and the 10th percentile in green. The 90th percentile is the income value such that 90% of households have incomes that are lower. 90% of the gray mass is to the left of the blue line. The 10th percentile is the income value such that 10% of households have incomes that are lower. We can think of the 90th percentile as a measure of a high income in the distribution and the 10th percentile as a measure of a low income in the distribution.\nThe 90/10 income ratio is the 90th percentile divided by the 10th percentile. For U.S. household incomes in 2022, this works out as\n\\[\n\\text{90/10 ratio} = \\frac{\\text{90th percentile}}{\\text{10th percentile}} = \\frac{$212,110}{$15,640} = 13.6\n\\]\nA higher value of the 90/10 ratio corresponds to higher inequality. In our hypothetical case, a household at the 90th percentile has an income that is 13.6 times as high as the income of a household at the 10th percentile.",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#prepare-the-environment",
    "href": "topics/visualization.html#prepare-the-environment",
    "title": "Visualization",
    "section": "Prepare the environment",
    "text": "Prepare the environment\n\nOpen a new R Script by clicking the button at the top left of RStudio. Save your R Script in a folder you will use for this exercise by clicking File -&gt; Save from the menu at the very top of your screen.\n Paste the code below into your R Script. Place your cursor within the line and hit CMD + Return or CTRL + Enter to run the code and load the tidyverse package.\n\nlibrary(tidyverse)\n\nYou will see action in the console. You have added some functionality to R for this session!\nThe data can be loaded from the course website with the line below.\n\ntable1 &lt;- read_csv(file = \"https://soc114.github.io/data/jencks_table1.csv\")\n\nWhen you run this code, the object table1 will appear in your environment pane.",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#explore-the-data",
    "href": "topics/visualization.html#explore-the-data",
    "title": "Visualization",
    "section": "Explore the data",
    "text": "Explore the data\nType table1 in your console. You can see the data!\n\nThe data contain four variables (columns):\n\ncountry country name\nratio ratio is the 90/10 income ratio in the country\ngdp is GDP per capita in the country, expressed as a proportion of U.S. GDP\nlife_expectancy life expectancy at birth\n\nThere is one row for each country. For details on the data, see Jencks (2002) Table 1.",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#produce-a-visualization",
    "href": "topics/visualization.html#produce-a-visualization",
    "title": "Visualization",
    "section": "Produce a visualization",
    "text": "Produce a visualization\nTo visualize data, we will use the ggplot() function which you have already loaded into your R session as part of the tidyverse package.\n\nBegin with an empty graph\n\nA function in R takes in arguments and returns an object. The arguments are the inputs that we give to the function. The function then returns something back to us.\nThe ggplot() function takes two arguments:\n\ndata = table1 says that data will come from the object table1\nmapping = aes(x = ratio, y = gdp) maps the data to the aesthetics of the graph. This line says that the ratio variable will be placed on the \\(x\\)-axis and the gdp variable will be on the \\(y\\)-axis.\n\nWhen you run this code, the function returns an object which is the resulting plot. The plot will appear in the Plots pane in RStudio.\n\nggplot(\n  data = table1,\n  mapping = aes(x = ratio, y = gdp)\n)\n\n\n\n\n\n\n\n\n\n\nAdd a layer to the graph\nOnce we have an empty graph, we can add elements to the graph in layers. ggplot() is set up to add layers connected by a + symbol between lines. For example, we can add points to the graph by adding the layer geom_point().\n\nggplot(\n  data = table1,\n  mapping = aes(x = ratio, y = gdp)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nImprove labels\n\nggplot(\n  data = table1,\n  mapping = aes(x = ratio, y = gdp)\n) +\n  geom_point() +\n  labs(\n    x = \"Inequality: 90th percentile / 10th percentile of household income\",\n    y = \"GDP as a Fraction of U.S. GDP\"\n  )\n\n\n\n\n\n\n\n\n\n\nCustomize in many ways\n\nThere are many ways to customize the graph. For example, the code below\n\nloads the ggrepel package in order to add country labels to the points\nuses geom_smooth to add a trend line\nuses scale_y_continuous to convert the \\(y\\)-axis labels from decimals to percentages\n\n\nlibrary(ggrepel)\ntable1 |&gt;\n  ggplot(aes(x = ratio, y = gdp)) +\n  geom_point() +\n  geom_smooth(\n    formula = 'y ~ x',\n    method = \"lm\", \n    se = F, \n    color = \"black\"\n  ) +\n  geom_text_repel(\n    aes(label = country),\n    size = 3\n  ) +\n  scale_y_continuous(\n    labels = scales::label_percent(),\n    name = \"GDP as a Percent of U.S.\"\n  ) +\n  scale_x_continuous(name = \"Inequality: 90th percentile / 10th percentile of household income\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "assignments/pset4_in_progress.html",
    "href": "assignments/pset4_in_progress.html",
    "title": "Problem Set 4: Statistical Learning",
    "section": "",
    "text": "Due: 5pm on Friday, Feb 28.\nIdea: - Income prediction challenge. OLS with two predictor sets. Sample split. Which one is better? - OLS with all variables. G-formula for causal effect of college on earnings, in subgroups of parent college\nThis problem set has not yet been posted.\nclass website page is a possibility for data here. Effect of college on subjective social class.\nknitr::opts_chunk$set(eval = FALSE)\nNOTE: This problem set will be updated to explicitly involve statistical learning with more guidance, as well as some causal inference.\nStudent identifer: [type your anonymous identifier here]\nThis problem set is connected to the PSID Income Prediction Challenge from discussion."
  },
  {
    "objectID": "assignments/pset4_in_progress.html#notes",
    "href": "assignments/pset4_in_progress.html#notes",
    "title": "Problem Set 4: Statistical Learning",
    "section": "NOTES",
    "text": "NOTES\nGoals are\n\nSample split\nEstimate MSE\nParametric g-formula outcome\nthere is no ML algorithm on this problem set nor any IPTW\n\nWould be nice to have a nonlinear confounder and a binary treatment.\n\nlibrary(tidyverse)\n# taken from principal stratification \ndata &lt;- readRDS(\"../data_raw/motherhood.RDS\") |&gt;\n  select(\n    treated, employed, age = age_2, sex, race, employed_baseline, \n    educ, marital, fulltime, tenure, experience,\n    weight = w\n  ) |&gt;\n  na.omit()"
  },
  {
    "objectID": "assignments/pset4_in_progress.html#prediction-challenge",
    "href": "assignments/pset4_in_progress.html#prediction-challenge",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Prediction challenge",
    "text": "Prediction challenge\n\ndata &lt;- read_csv(\"data_raw/income_challenge/for_students/learning.csv\") |&gt;\n  mutate(across(contains(\"educ\"), \\(x) factor(x,levels = c(\"Less than high school\",\"High school\",\"Some college\",\"College\"))))"
  },
  {
    "objectID": "assignments/pset4_in_progress.html#ols-prediction",
    "href": "assignments/pset4_in_progress.html#ols-prediction",
    "title": "Problem Set 4: Statistical Learning",
    "section": "1. OLS prediction",
    "text": "1. OLS prediction\nPredict g3_log_income given all other variables by OLS.\n\nfit &lt;- lm(\n  g3_log_income ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data\n)\n\nEstimate the average causal effect of college vs high school.\n\ndata |&gt;\n  mutate(yhat1 = predict(fit, newdata = data |&gt; mutate(g3_educ = \"College\")),\n         yhat0 = predict(fit, newdata = data |&gt; mutate(g3_educ = \"High school\"))) |&gt;\n  summarize(ate = mean(yhat1 - yhat0))\n\nEstimate within subgroups of parents’ education\n\ndata |&gt;\n  mutate(yhat1 = predict(fit, newdata = data |&gt; mutate(g3_educ = \"College\")),\n         yhat0 = predict(fit, newdata = data |&gt; mutate(g3_educ = \"High school\"))) |&gt;\n  group_by(g2_educ) |&gt;\n  summarize(ate = mean(yhat1 - yhat0))"
  },
  {
    "objectID": "assignments/pset4_in_progress.html#causal-forest-prediction",
    "href": "assignments/pset4_in_progress.html#causal-forest-prediction",
    "title": "Problem Set 4: Statistical Learning",
    "section": "2. Causal forest prediction",
    "text": "2. Causal forest prediction\n\nlibrary(grf)\n\n\nfor_forest &lt;- data |&gt;\n      filter(g3_educ %in% c(\"High school\",\"College\"))\n\nfit &lt;- causal_forest(\n  X = model.matrix(\n    ~ race + sex + g2_log_income + g2_educ,\n    data = for_forest\n  ),\n  Y = for_forest$g3_log_income,\n  W = for_forest$g3_educ == \"College\"\n)\n\nsummary(fit)\naverage_treatment_effect(fit, target.sample = \"control\")\n\n\n\nconditi\n\n# Prepare the predictor matrix\nmodel.matrix(fit)\n\nX &lt;- X0 &lt;- X1 &lt;- model.matrix(\n  ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data\n)\nX1[\"\"]\n\nX_factual &lt;- model.matrix(\n  object = ~g3_log_income ~ race + sex,\n  data = data.frame(data)\n)\nX_treated &lt;- model.matrix(\n  ~g3_log_income ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data |&gt; mutate(g3_educ = \"College\")\n)\nX_untreated &lt;- model.matrix(\n  ~g3_log_income ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data |&gt; mutate(g3_educ = \"High school\")\n)\n\nfit &lt;- regression_forest(\n  X = \n)\n\n\ndata |&gt;\n  group_by(g3_educ) |&gt;\n  summarize(y = mean(g3_log_income)) |&gt;\n  mutate(\n    g3_educ = factor(g3_educ),\n    g3_educ = fct_relevel(g3_educ, \"Less than high school\",\"High school\",\"Some college\",\"College\")\n  ) |&gt;\n  ggplot(aes(x = g3_educ, y = y)) +\n  geom_point() +\n  scale_y_continuous(labels = function(x) scales::label_dollar()(exp(x)))\n\n\ndata |&gt;\n  mutate(\n    yhat_factual = predict(fit_ols),\n    yhat_counterfactual = predict(fit_ols, newdata = data |&gt; mutate(g3_educ = \"College\"))\n  ) |&gt;\n  select(g2_log_income, starts_with(\"yhat\")) |&gt;\n  pivot_longer(cols = -g2_log_income) |&gt;\n  ggplot(aes(x = g2_log_income, y = value, color = name)) +\n  geom_line()"
  },
  {
    "objectID": "assignments/pset4_in_progress.html#income-prediction-challenge",
    "href": "assignments/pset4_in_progress.html#income-prediction-challenge",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Income Prediction Challenge",
    "text": "Income Prediction Challenge\nCollaboration note. This question is an individual write-up connected to your group work from discussion. We expect that the approach you tell us might be the same as that of your other group members, but your answers to these questions should be in your own words.\n1.1 (5 points) How did you choose the predictor variables you used? Correct answers might be entirely conceptual, entirely data-driven, or a mixture of both.\n1.2 (5 points) What learning algorithms or models did you consider, and how did you choose one? Correct answers might be entirely conceptual, entirely data-driven, or a mixture of both.\n1.3 (20 points) Split the learning data randomly into train and test. Your split can be 50-50 or another ratio. Learn in the train set and make predictions in the test set. What do you estimate for your out-of-sample mean squared error? There is no written answer here; the answer is the code and result."
  },
  {
    "objectID": "assignments/pset4_in_progress.html#grad.-machine-learning-versus-statistics",
    "href": "assignments/pset4_in_progress.html#grad.-machine-learning-versus-statistics",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Grad. Machine learning versus statistics",
    "text": "Grad. Machine learning versus statistics\n\nThis question is required for grad students. It is optional for undergrads, and worth no extra credit.\n\n20 points. This question is about the relative gain in this problem as we move from no model to a statistical model to a machine learning model.\nFirst, use your train set to estimate 3 learners and predict in your test set.\n\nNo model. For every test observation, predict the mean of the train outcomes\nOrdinary Least Squares. Choose a set of predictors \\(\\vec{X}\\). For every test observation, predict using a linear model lm() fit to the train set with the predictors \\(\\vec{X}\\).\nMachine learning. Use the same set of predictors \\(\\vec{X}\\). For every test observation, predict using a machine learning model fit to the train set with the predictors \\(\\vec{X}\\). Your machine learning model could be a Generalized Additive Model (gam()), a decision tree (rpart()), or some other machine learning approach.\n\nReport your out-of-sample mean squared error estimates for each approach. How did mean squared error change from (a) to (b)? From (b) to (c)?\nInterpret what you found. To what degree does machine learning improve predictability, beyond what can be achieved by Ordinary Least Squares?"
  },
  {
    "objectID": "topics/statistical_learning.html",
    "href": "topics/statistical_learning.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "The reading with this class is Berk 2020 Ch 1 p. 1–5, stopping at paragraph ending “…is nonlinear.” Then p. 14–17 “Model misspecification…” through “…will always be in play.”\nStatistical learning is a term that captures a broad set of ideas in statistics and machine learning. This page focuses on one sense of statistical learning: using data on a sample to learn a subgroup mean in the population.\nAs an example, we continue to use the data on baseball salaries, with a small twist. The file baseball_with_record.csv contains the following variables\npopulation &lt;- read_csv(\"https://info3370.github.io/data/baseball_with_record.csv\")\nOur goal: using a sample, estimate the mean salary of all Dodger players in 2023. Because we have the population, we know the true mean is $6.23m.\nA sparse sample will hinder our ability to accomplish the goal. We will work with samples containing many MLB players, but only a few Dodgers. We will use statistical learning strategies to pool information from those other teams’ players to help us make a better estimate of the Dodger mean salary.\nOur predictor will be the record from the previous year. We assume that teams with similar win percentages in 2022 might have similar salaries in 2023.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Statistical Learning"
    ]
  },
  {
    "objectID": "topics/statistical_learning.html#prepare-our-data-environment",
    "href": "topics/statistical_learning.html#prepare-our-data-environment",
    "title": "Statistical Learning",
    "section": "Prepare our data environment",
    "text": "Prepare our data environment\nFor illustration, draw a sample of 5 players per team\n\nsample &lt;- population |&gt;\n  group_by(team) |&gt;\n  sample_n(5) |&gt;\n  ungroup()\n\nConstruct a tibble with the observation to be predicted: the Dodgers.\n\nto_predict &lt;- population |&gt;\n  filter(target_subgroup) |&gt;\n  distinct(team, record)",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Statistical Learning"
    ]
  },
  {
    "objectID": "topics/statistical_learning.html#ordinary-least-squares",
    "href": "topics/statistical_learning.html#ordinary-least-squares",
    "title": "Statistical Learning",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\nWe could model salary next year as a linear function of team record by Ordinary Least Squares. In math, OLS produces a prediction \\[\\hat{Y}_i = \\hat\\alpha + \\hat\\beta X_i\\] with \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) chosen to minimize the sum of squared errors, \\(\\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\). Visually, it minimizes all the line segments below.\n\n\n\n\n\n\n\n\n\nHere is how to estimate an OLS model using R.\n\nmodel &lt;- lm(salary ~ record, data = sample)\n\nThen we could predict the mean salary for the Dodgers.\n\nto_predict |&gt;\n  mutate(predicted = predict(model, newdata = to_predict))\n\n# A tibble: 1 × 3\n  team         record predicted\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 L.A. Dodgers  0.685  6668053.\n\n\nOur model-based estimate compares to the true population mean of $6.23m.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Statistical Learning"
    ]
  },
  {
    "objectID": "topics/statistical_learning.html#penalized-regression",
    "href": "topics/statistical_learning.html#penalized-regression",
    "title": "Statistical Learning",
    "section": "Penalized regression",
    "text": "Penalized regression\nPenalized regression is just like OLS, except that it prefers coefficient estimates that are closer to 0. This can reduce sampling variability. One penalized regression is ridge regression, which penalizes the sum of squared coefficients. In our example, it estimates the parameters to minimize\n\\[\\underbrace{\\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Squared Error} + \\underbrace{\\lambda\\beta^2}_\\text{Penalty}\\]\nwhere the positive scalar penalty \\(\\lambda\\) encodes our preference for coefficients to be near zero. Otherwise, penalized regression is just like OLS!\nThe gam() function in the mgcv package will allow you to fit a ridge regression as follows.\n\nlibrary(mgcv)\n\n\nmodel &lt;- gam(\n  salary ~ s(record, bs = \"re\"),\n  data = sample\n)\n\nPredict the Dodger mean salary just as before,\n\nto_predict |&gt;\n  mutate(predicted = predict(model, newdata = to_predict))\n\n# A tibble: 1 × 3\n  team         record predicted\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl[1d]&gt;\n1 L.A. Dodgers  0.685  6252449.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Statistical Learning"
    ]
  },
  {
    "objectID": "topics/statistical_learning.html#splines",
    "href": "topics/statistical_learning.html#splines",
    "title": "Statistical Learning",
    "section": "Splines",
    "text": "Splines\nWe may want to allow a nonlinear relationship between the predictor and the outcome. One way to do that is with splines, which estimate part of the model locally within regions of the predictor space separated by knots. The code below uses a linear spline with knots at 0.4 and 0.6.\n\nlibrary(splines)\nmodel &lt;- lm(\n  salary ~ bs(record, degree = 1, knots = c(.4,.6)),\n  data = sample\n)\n\n\n\n\n\n\n\n\n\n\nWe can predict the Dodger mean salary just as before!\n\nto_predict |&gt;\n  mutate(predicted = predict(model, newdata = to_predict))\n\n# A tibble: 1 × 3\n  team         record predicted\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 L.A. Dodgers  0.685  3269869.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Statistical Learning"
    ]
  },
  {
    "objectID": "topics/statistical_learning.html#trees",
    "href": "topics/statistical_learning.html#trees",
    "title": "Statistical Learning",
    "section": "Trees",
    "text": "Trees\nPerhaps our response surface is bumpy, and poorly approximated by a smooth function. Decision trees search the predictor space for discrete places where the outcome changes, and assume that the response is flat within those regions.\n\nlibrary(rpart)\nmodel &lt;- rpart(salary ~ record, data = sample)\n\n\n\n\n\n\n\n\n\n\nPredict as in the other strategies.\n\nto_predict |&gt;\n  mutate(predicted = predict(model, newdata = to_predict))\n\n# A tibble: 1 × 3\n  team         record predicted\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 L.A. Dodgers  0.685  4253845.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Statistical Learning"
    ]
  },
  {
    "objectID": "topics/statistical_learning.html#conclusion",
    "href": "topics/statistical_learning.html#conclusion",
    "title": "Statistical Learning",
    "section": "Conclusion",
    "text": "Conclusion\nStatistical learning in this framing is all about\n\nwe have a subgroup with few sampled units (the Dodgers)\nwe want to use other units to help us learn\nour goal is to predict the population mean in the subgroup",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for description",
      "Statistical Learning"
    ]
  }
]