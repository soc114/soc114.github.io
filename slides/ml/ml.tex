
\documentclass{beamer}
\usecolortheme{dove}
\setbeamertemplate{navigation symbols}{}
\usepackage{amsmath,amssymb,amsfonts,amsthm, multicol, subfigure, color}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{xcolor}
\definecolor{seagreen}{RGB}{46, 139, 87}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\log{\text{log}}
\newcommand\logit{\text{logit}}
\newcommand\iid{\stackrel{\text{iid}}{\sim}}
\newcommand\E{\text{E}}
\newcommand\V{\text{V}}
\renewcommand\P{\text{P}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Cor}{\text{Cor}}
\newcommand\doop{\texttt{do}}
\usepackage{stackrel}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes.arrows,positioning,shapes,patterns,calc}
\newcommand\slideref[1]{\vskip .1cm \tiny \textcolor{gray}{{#1}}}
\newcommand\red[1]{\color{red}#1}
\newcommand\blue[1]{\color{blue}#1}
\newcommand\gray[1]{\color{gray}#1}
\newcommand\seagreen[1]{\color{seagreen}#1}
\newcommand\purple[1]{\color{purple}#1}
\newcommand\orange[1]{\color{orange}#1}
\newcommand\black[1]{\color{black}#1}
\newcommand\white[1]{\color{white}#1}
\newcommand\teal[1]{\color{teal}#1}
\newcommand\magenta[1]{\color{magenta}#1}
\newcommand\Fuchsia[1]{\color{Fuchsia}#1}
\newcommand\BlueGreen[1]{\color{BlueGreen}#1}
\newcommand\bblue[1]{\textcolor{blue}{\textbf{#1}}}
\newcommand\bred[1]{\textcolor{red}{\textbf{#1}}}
\newcommand\bgray[1]{\textcolor{gray}{\textbf{#1}}}
\newcommand\bgreen[1]{\textcolor{seagreen}{\textbf{#1}}}
\newcommand\bref[2]{\href{#1}{\color{blue}{#2}}}
\colorlet{lightgray}{gray!40}
\pgfdeclarelayer{bg}    % declare background layer for tikz
\pgfsetlayers{bg,main} % order layers for tikz
\newcommand\mycite[1]{\begin{scriptsize}\textcolor{darkgray}{(#1)}\end{scriptsize}}
\newcommand{\tcframe}{\frame{
%\small{
\only<1|handout:0>{\tableofcontents}
\only<2|handout:1>{\tableofcontents[currentsubsection]}}
%}
}

\newcommand{\goalsframe}{\begin{frame}{Learning goals for today}
By the end of class, you will be able to
\begin{itemize}
    \item use statistical learning to estimate when data are sparse
    \item work with models that are ``wrong''
 \end{itemize} 
  \vskip .2in
\end{frame}}

\newcommand{\credible}{\begin{frame}{Credible science}
\begin{enumerate}
\item replicability
\item reproducibility
\end{enumerate}
\end{frame}}

\usepackage[round]{natbib}
\bibliographystyle{humannat-mod}
\setbeamertemplate{enumerate items}[default]
\usepackage{mathtools}

\title{Studying Social Inequality with Data Science}
\author{Ian Lundberg}
\date{\today}

\begin{document}

\begin{frame}
\begin{tikzpicture}[x = \textwidth, y = \textheight]
\node at (0,0) {};
\node at (1,1) {};
\node[anchor = north west, align = left, font = \huge] at (0,.9) {Social\\Data\\Science};
\node[anchor = north east, align = right] (number) at (1,.9) {Soc 114\\Winter 2025};
\node[anchor = north, font = \Large, align = center] at (.5,.5) {Supervised Machine Learning};
\end{tikzpicture}
\end{frame}

\goalsframe

\begin{frame}{Prediction function}

A \textbf{prediction function} is an input-output function:
\begin{itemize}
\item input a vector of predictors $\vec{x}$
\item output a predicted outcome $\hat{y} = \hat{f}(\vec{x})$
\end{itemize}

\begin{center}
\begin{tikzpicture}[x = 1in, y = .2in]
\node (x) at (1,0) {$\vec{x}$};
\node (y) at (2,0) {$\hat{y}$};
\draw[->, thick] (x) -- (y);
\node[anchor = north, font = {\footnotesize\bf}] at (1,1.5) {Input};
\node[anchor = north, font = {\footnotesize\bf}] at (2,1.5) {Output};
\node[anchor = north, font = {\footnotesize\bf}, gray] at (0,-1) {Example:};
\node[anchor = north, font = \footnotesize, gray] at (1,-1) {\{Sex,Age\}};
\node[anchor = north, font = \footnotesize, gray, align = left] at (2,-1) {Probability of\\Employment\\Given Sex\\and Age};
\end{tikzpicture}
\end{center}

\end{frame}

\begin{frame}
\begin{tikzpicture}
\node[anchor = north] at (.5,2.8) {Age};
\node[anchor = north] at (1.5,2.8) {Sex};
\node[anchor = north] at (4.5,2.8) {Employed};
% Learning grids
\draw[xstep = 1, ystep = .5] (0,0) grid (2,2);
\draw[xstep = 1, ystep = .5] (4,0) grid (5,2);
% Prediction grid
\draw[xstep = 1, ystep = .5] (0,-2) grid (2,-1.5);
\draw[xstep = 1, ystep = .5] (4,-2) grid (5,-1.5);
% Ages
\node[font = \small] at (.5,1.75) {26};
\node[font = \small] at (.5,1.25) {40};
\node[font = \small] at (.5,.75) {61};
\node[font = \small] at (.5,.25) {32};
% Sexes
\node[font = \small] at (1.5,1.75) {F};
\node[font = \small] at (1.5,1.25) {M};
\node[font = \small] at (1.5,.75) {M};
\node[font = \small] at (1.5,.25) {F};
% Employment
\node[font = \small] at (4.5,1.75) {1};
\node[font = \small] at (4.5,1.25) {1};
\node[font = \small] at (4.5,.75) {0};
\node[font = \small] at (4.5,.25) {1};
% To predict
\node[font = \small] at (.5,-1.75) {63};
\node[font = \small] at (1.5,-1.75) {F};
\node[font = \small] at (4.5,-1.75) {?};
% Label cases
\node[anchor = east, align = right] at (-.5,1) {cases for learning};
\node[anchor = east, align = right] at (-.5,-1.75) {case to predict};
\end{tikzpicture}
\end{frame}

\begin{frame}{OLS is a prediction function}{$\text{Input }\vec{x}\rightarrow \text{Output }\hat{y}$}

$$
\hat{y} = \hat{f}(\vec{x}) = \hat\beta_0 + \hat\beta_1(\text{Sex = Male}) + \hat\beta_2(\text{Age})
$$
\begin{itemize}
\item Learn $\hat{f}$ in a \textbf{learning sample} with $\{\vec{x}_i,y_i\}_{i=1}^n$
\begin{itemize}
\item Computer finds $\hat\beta_0$, $\hat\beta_1$, $\hat\beta_2$ that predict well in the learning sample
\end{itemize}
\item At a new $\vec{x}$ value, predict $\hat{f}(\vec{x})$
\end{itemize}

\end{frame}

\begin{frame}{Logistic regression is a prediction function}{$\text{Input }\vec{x}\rightarrow \text{Output }\hat{y}$}

$$
\hat{y} = \hat{f}(\vec{x}) = \text{logit}^{-1}\left(\hat\beta_0 + \hat\beta_1(\text{Sex = Male}) + \hat\beta_2(\text{Age})\right)
$$
\begin{itemize}
\item Learn $\hat{f}$ in a \textbf{learning sample} with $\{\vec{x}_i,y_i\}_{i=1}^n$
\begin{itemize}
\item Computer finds $\hat\beta_0$, $\hat\beta_1$, $\hat\beta_2$ that predict well in the learning sample
\end{itemize}
\item At a new $\vec{x}$ value, predict $\hat{f}(\vec{x})$
\end{itemize}

\end{frame}

\begin{frame}{Matching is a prediction function}{$\text{Input }\vec{x}\rightarrow \text{Output }\hat{y}$}

$$
\hat{y} = \hat{f}(\vec{x}) = y_j
$$
where unit $j$ is the best match among the learning sample, which minimizes a distance from the case to predict: $d(\vec{x},\vec{x}_j)$ is small
\begin{itemize}
\item Learn $\hat{f}$ in a \textbf{learning sample} with $\{\vec{x}_i,y_i\}_{i=1}^n$
\begin{itemize}
\item Computer finds $j$ with $\vec{x}_j$ most similar to $\vec{x}$
\end{itemize}
\item At a new $\vec{x}$ value, predict $\hat{f}(\vec{x})$
\end{itemize}

\end{frame}

\begin{frame}{There are many prediction functions}

\begin{itemize}
\item input a vector of predictors $\vec{x}$
\item output a predicted outcome $\hat{y} = \hat{f}(\vec{x})$
\end{itemize}

\end{frame}

\begin{frame}{Trees as a prediction function}

\includegraphics<1>[width = \textwidth]{figures/tree_sim_1}
\includegraphics<2>[width = \textwidth]{figures/tree_sim_2}
\includegraphics<3>[width = \textwidth]{figures/tree_sim_3}
\includegraphics<4>[width = \textwidth]{figures/tree_sim_4}
\includegraphics<5>[width = \textwidth]{figures/tree_sim_viz}

\end{frame}

\begin{frame}{Trees as a prediction function: How that worked}

\includegraphics<1>[width = \textwidth]{figures/tree_sim_1}
\includegraphics<2>[width = \textwidth]{figures/tree_choice_1}
\includegraphics<3>[width = \textwidth]{figures/tree_sim_2}
\includegraphics<4>[width = \textwidth]{figures/tree_choice_2}
\includegraphics<5>[width = \textwidth]{figures/tree_sim_3}
\includegraphics<6>[width = \textwidth]{figures/tree_sim_4}
\includegraphics<7>[width = \textwidth]{figures/tree_sim_viz}

\end{frame}

\begin{frame}{A tree can be interpretable}

TODO: real data example. Have them run it if in class.

\end{frame}


\goalsframe

\end{document}
