{
  "hash": "d4ba458ae5c2a0164198afebe69320f1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression\"\nsubtitle: \"UCLA Soc 114\"\nformat:\n  #revealjs: default # For the web slides\n  beamer: default      # To generate the PDF version\nexecute:\n  echo: true\n# resources: \n#   - lec04_bootstrap.pdf\n---\n\n\n::: {.cell}\n\n:::\n\n\n## Linear regression: Learning goals\n\nSome things you may know\n\n- How to fit a linear model\n- How to make predictions\n\nData science ideas\n\n- Why model at all?\n- Penalized linear regression\n\n## Data for illustration\n\nU.S. adult income by\n\n- sex (male, female)\n- age (30--50)\n- year (2010--2019)\n\namong those working 35+ hours per week for 50+ weeks per year. Data are simulated based on the 2010â€“2019 American Community Survey (ACS).\n\n## Data for illustration\n\nThe function below will simulate data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate <- function(n = 100) {\n  read_csv(\"https://ilundberg.github.io/description/assets/truth.csv\") |>\n    slice_sample(n = n, weight_by = weight, replace = T) |>\n    mutate(income = exp(rnorm(n(), meanlog, sdlog))) |>\n    select(year, age, sex, income)\n}\n```\n:::\n\n\n## Data for illustration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated <- simulate(n = 3e4)\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30,000 x 4\n   year   age sex    income\n  <dbl> <dbl> <chr>   <dbl>\n1  2011    48 female 93676.\n2  2012    38 female 98805.\n3  2013    38 female 52330.\n# i 29,997 more rows\n```\n\n\n:::\n:::\n\n\n## Conditional expectation\n\n. . .\n\nMean of an outcome within a population subgroup.\n\n. . .\n\n- **expectation** refers to taking a mean\n\n. . .\n\n- **conditional** refers to within a subgroup\n\n. . .\n\nExample: Mean income among females age 47 in 2019 \n\n. . .\n\n**Task.** Estimate this in our data.\n\n## Code: Find the subgroup\n\n`filter()` restricts our data to cases meeting requirements:\n\n- the `sex` variable equals the value `female`\n- the `age` variable equals the value `47`\n- the `year` variable equals the value `2019`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubgroup <- simulated |>\n  filter(sex == \"female\") |>\n  filter(age == 47) |>\n  filter(year == 2019)\n```\n:::\n\n\n## Code: Estimate the mean\n\n`summarize()` aggregates to the mean\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubgroup |>\n  summarize(conditional_expectation = mean(income))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 1\n  conditional_expectation\n                    <dbl>\n1                  71530.\n```\n\n\n:::\n:::\n\n\n## Code: Mean in many subgroups\n\n. . .\n\nWith `group_by`, you can `summarize` many subgroups\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated |>\n  group_by(sex, age, year) |>\n  summarize(conditional_expectation = mean(income))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 420 x 4\n# Groups:   sex, age [42]\n  sex      age  year conditional_expectation\n  <chr>  <dbl> <dbl>                   <dbl>\n1 female    30  2010                  45928.\n2 female    30  2011                  43688.\n3 female    30  2012                  42714.\n# i 417 more rows\n```\n\n\n:::\n:::\n\n\n## Conditional expectation: Math\n\n. . .\n\nThe **conditional expectation function** is the subgroup mean of $Y$ within a subgroup with the predictor values $\\vec{X} = \\vec{x}$.\n\n$$\nf(\\vec{x}) = \\text{E}(Y\\mid\\vec{X} = \\vec{x})\n$$\n\nTo learn $f(\\vec{x})$ from data is a central task in **statistical learning**.\n\n# Statistical Learning by Pooling Information\n\n## A subgroup is small\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated |>\n  filter(sex == \"female\") |>\n  filter(year == 2019) |>\n  filter(age == 47)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 67 x 4\n   year   age sex    income\n  <dbl> <dbl> <chr>   <dbl>\n1  2019    47 female 15761.\n2  2019    47 female 32995.\n3  2019    47 female 83967.\n# i 64 more rows\n```\n\n\n:::\n:::\n\n\n. . .\n\nVery few cases $\\rightarrow$ statistically uncertain \n\n. . .\n\n**How to better estimate for 47-year-old females in 2019?**\n\n## Pooling information across subgroups\n\n. . .\n\nWe have many female respondents in 2019. Few are age 47.\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated |>\n  filter(sex == \"female\") |>\n  filter(year == 2019)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,427 x 4\n   year   age sex    income\n  <dbl> <dbl> <chr>   <dbl>\n1  2019    32 female 52130.\n2  2019    46 female 17465.\n3  2019    41 female 66012.\n# i 1,424 more rows\n```\n\n\n:::\n:::\n\n\n. . .\n\nCould we use them to learn about the 47-year-olds?\n\n## Pooling information across subgroups\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-10-1.pdf)\n:::\n:::\n\n\n## Pooling information across subgroups\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-11-1.pdf)\n:::\n:::\n\n\n## Pooling information across subgroups\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-12-1.pdf)\n:::\n:::\n\n\n## Practice question\n\n$$\n\\text{E}(Y\\mid X) = \\beta_0 + \\beta_1 X\n$$\n\nSuppose $\\beta_0 = 5$ and $\\beta_1 = 3$\n\n1. What is the conditional mean when $X = 0$?\n2. What is the conditional mean when $X = 1$?\n3. What is the conditional mean when $X = 2$?\n4. How much does the conditional mean change for each unit increase in $X$?\n\n## Code\n\nThe next slides explain how to code a model in R.\n\n## Code: Simulate data\n\n. . .\n\nGenerate some data\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated <- simulate(n = 3e4)\n```\n:::\n\n\n. . .\n\nRestrict to female respondents in 2019\n\n::: {.cell}\n\n```{.r .cell-code}\nfemale_2019 <- simulated |>\n  filter(sex == \"female\") |>\n  filter(year == 2019)\n```\n:::\n\n\n. . .\n\n(Below is `simulate` if you did not copy it before)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate <- function(n = 100) {\n  read_csv(\"https://ilundberg.github.io/description/assets/truth.csv\") |>\n    slice_sample(n = n, weight_by = weight, replace = T) |>\n    mutate(income = exp(rnorm(n(), meanlog, sdlog))) |>\n    select(year, age, sex, income)\n}\n```\n:::\n\n\n## Code: Learn a model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(\n  formula = income ~ age, \n  data = female_2019\n)\n```\n:::\n\n\n- `model` is an object of class `lm` for **l**inear **m**odel\n- `lm()` function creates this object\n- `formula` argument is a model formula\n     - `outcome ~ predictor` is the syntax\n- `data` is a dataset containing `outcome` and `predictor`\n\n## Code: Examine the learned model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = income ~ age, data = female_2019)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-52689 -29518 -12682  16013 400507 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  47242.6     7518.3   6.284 4.37e-10 ***\nage            233.7      185.7   1.259    0.208    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 43360 on 1437 degrees of freedom\nMultiple R-squared:  0.001102,\tAdjusted R-squared:  0.0004064 \nF-statistic: 1.585 on 1 and 1437 DF,  p-value: 0.2083\n```\n\n\n:::\n:::\n\n\n## Code: Predict for a new X value\n\n. . .\n\nDefine X value at which to predict\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- tibble(age = 47)\n```\n:::\n\n\n. . .\n\nPredict for that subgroup\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(model, newdata = to_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n58228.57 \n```\n\n\n:::\n:::\n\n\n. . .\n\nRecap: Our model **pooled information**:\n\n- People of all ages contributed to `model`\n- Then we predicted at a single age\n\n## Code: Three steps\n\n- Estimate a model\n- Define $x$ to predict\n- Predict $\\hat{Y} = \\hat{\\text{E}}(Y\\mid X = x)$\n\n**What if you were going to do this many times on different data?**\n\n## Code: Three steps in a function\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator <- function(data) {\n  # Learn the model from the data\n  model <- lm(formula = income ~ age, data = data)\n  # Define our target subgroup\n  to_predict <- tibble(age = 47)\n  # Predict\n  estimate <- predict(model, newdata = to_predict)\n  # Return the estimate\n  return(estimate)\n}\n```\n:::\n\n\n## Code: All together\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator(data = female_2019)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n58228.57 \n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfemale_2019 |>\n  estimator()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n58228.57 \n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated |> \n  filter(sex == \"female\") |>\n  filter(year == 2010) |>\n  estimator()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      1 \n54637.9 \n```\n\n\n:::\n:::\n\n\n## Practice question\n\nBelow is the line fit to the population data. Suppose we want to learn $\\text{E}(\\log(Y)\\mid X = 30)$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-24-1.pdf)\n:::\n:::\n\n\n1. Why might this model make a misleading estimate?\n2. Why might the model still be useful?\n\n# Additive vs Interactive\n\n## Two models\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-25-1.pdf)\n:::\n:::\n\n\n## Two models\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-26-1.pdf)\n:::\n:::\n\n\n## Two models\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-27-1.pdf)\n:::\n:::\n\n\n## Two models: Interaction\n\n$$\n\\begin{aligned}\n\\text{E}(Y\\mid X, \\text{Female}) &= \\beta_0^\\text{Female} + \\beta_1^\\text{Female}\\times \\text{Age} \\\\\n\\text{E}(Y\\mid X, \\text{Male}) &= \\beta_0^\\text{Male} + \\beta_1^\\text{Male}\\times \\text{Age} \\\\\n\\end{aligned}\n$$\n\n. . .\n\nEquivalently,\n$$\\text{E}(Y \\mid X, \\text{Sex}) = \\gamma_0 + \\gamma_1(\\text{Female}) + \\gamma_2(\\text{Age}) + \\gamma_3 (\\text{Age} \\times \\text{Female})$$\n. . .\n\nwhere\n$$\\begin{aligned}\n\\gamma_0 &= \\beta_0^\\text{Male} \n&\\gamma_1 &= \\beta_0^\\text{Female} - \\beta_0^\\text{Male} \\\\\n\\gamma_2 &= \\beta_1^\\text{Male} \n&\\gamma_3 &= \\beta_1^\\text{Female} - \\beta_1^\\text{Male}\n\\end{aligned}$$\n\n## Two models: Interaction in code\n\nGenerate data in 2019 that vary in both `sex` and `age`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_2019 <- simulated |>\n  filter(year == 2019)\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3,204 x 4\n   year   age sex   income\n  <dbl> <dbl> <chr>  <dbl>\n1  2019    41 male  50285.\n2  2019    45 male  31057.\n3  2019    34 male  66166.\n# i 3,201 more rows\n```\n\n\n:::\n:::\n\n\n## Two models: Interaction in code\n\n. . .\n\nThe `*` operator allows slopes to differ across groups\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(\n  formula = income ~ sex * age,\n  data = all_2019\n)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-31-1.pdf)\n:::\n:::\n\n\n## Two models: Additive model in R\n\nThe `+` operator assumes slopes are the same across groups\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(\n  formula = income ~ sex + age,\n  data = all_2019\n)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-33-1.pdf)\n:::\n:::\n\n\n## Interactions make lots of terms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(\n  formula = income ~ sex * age * year,\n  data = simulated\n)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = income ~ sex * age * year, data = simulated)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-81158 -33849 -14946  15839 972817 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n(Intercept)       1.387e+06  2.343e+06   0.592    0.554\nsexmale          -1.943e+06  3.117e+06  -0.623    0.533\nage              -6.273e+04  5.760e+04  -1.089    0.276\nyear             -6.680e+02  1.163e+03  -0.574    0.566\nsexmale:age       6.646e+04  7.675e+04   0.866    0.386\nsexmale:year      9.519e+02  1.547e+03   0.615    0.538\nage:year          3.130e+01  2.859e+01   1.095    0.274\nsexmale:age:year -3.247e+01  3.809e+01  -0.852    0.394\n\nResidual standard error: 57790 on 29992 degrees of freedom\nMultiple R-squared:  0.03332,\tAdjusted R-squared:  0.03309 \nF-statistic: 147.7 on 7 and 29992 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## Interactions make lots of terms\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-36-1.pdf)\n:::\n:::\n\n\n# Penalized Regression\n\n## Penalized regression\n\nOLS is a linear model\n\n$$\\text{E}(Y\\mid\\vec{X}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots$$\n\n. . .\n\nThere are many linear models beyond OLS.\n\n- (other ways of estimating the $\\beta$ coefficients)\n\n## Penalized regression\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-37-1.pdf)\n:::\n:::\n\n\n## Penalized regression\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec05_linear_regression_files/figure-beamer/unnamed-chunk-38-1.pdf)\n:::\n:::\n\n\n## Unpenalized regression: In math\n\nOLS chose $\\alpha, \\vec\\beta$ to minimize this function:\n$$\n\\begin{aligned}\n\\underbrace{\\sum_i\\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Sum of Squared Error}\n\\end{aligned}\n$$\nwhere $\\hat{Y}_i = \\hat\\alpha + \\sum_j X_j \\hat\\beta_j$\n\n## Penalized regression: In math\n\nPenalized (ridge) regression chose $\\alpha, \\vec\\beta$ to minimize this function:\n$$\n\\begin{aligned}\n\\underbrace{\\sum_i\\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda \\sum_{j} \\beta_j^2}_\\text{Penalty Term}\n\\end{aligned}\n$$\nwhere $\\hat{Y}_i = \\hat\\alpha + \\sum_j X_j \\hat\\beta_j$\n\n## Penalized regression: Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated <- simulate(n = 1e5)\n```\n:::\n\n\n## Penalized regression: Code\n\nThe `glmnet` package supports penalized regression\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n:::\n\n\n## Penalized regression: Code\n\nCreate a model matrix of predictors\n\n- Each column will correspond to a coefficient\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(~ age * sex * year, data = simulated)\n```\n:::\n\n\n. . .\n\nCreate a vector of the outcomes\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- simulated |> pull(income)\n```\n:::\n\n\n## Penalized regression: Code\n\nUse the `cv.glmnet` function\n\n::: {.cell}\n\n```{.r .cell-code}\npenalized <- cv.glmnet(\n  x = X,    # model matrix we created\n  y = y,    # outcome vector we created\n  alpha = 0 # penalize sum of beta ^ 2\n)\n```\n:::\n\n\n## Penalized regression: Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat <- predict(\n  penalized,\n  newx = X\n)\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(yhat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   lambda.1se   \n Min.   :60582  \n 1st Qu.:62568  \n Median :65476  \n Mean   :65063  \n 3rd Qu.:67425  \n Max.   :69405  \n```\n\n\n:::\n:::\n\n\n## When to use penalized regression?\n\n. . .\n\n- Many predictors and few observations\n     - High-variance estimates\n     \n. . .\n     \n- When you are willing to accept bias\n     - Model will be a bit wrong on average\n\n## Linear regression: Learning goals\n\nSome things you may know\n\n- How to fit a linear model\n- How to make predictions\n\nData science ideas\n\n- Why model at all?\n- Penalized linear regression",
    "supporting": [
      "lec05_linear_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}