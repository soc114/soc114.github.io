{
  "hash": "1b8a02e03a268e794c45ba2a015f50cb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Population sampling\"\n---\n\n> Topic for 1/12.\n\n<!-- Note from W25. In lecture 2, stratified and clustered sampling without them doing any coding followed by future of sample surveys would be enough for a whole class. Replication crisis could be moved to its own class. As it was, future of sample surveys was squeezed into about 10 minutes. -->\n\n\n::: {.cell}\n\n:::\n\n\nClaims about inequality are often claims about a population. Our data are typically only a sample! This module addresses the link between samples and populations.\n\n<!-- This page covers two lectures. -->\n\n<!-- - The first lecture covers [full count enumeration](#full-count-enumeration) through the [Current Population Survey](#sec-cps). After lecture, you should read about [probability sampling](https://www150.statcan.gc.ca/n1/edu/power-pouvoir/ch13/prob/5214899-eng.htm). -->\n<!-- - The second lecture uses salaries of [Major League Baseball players](#sec-baseball) to carry out three sampling strategies and explore their performance. After class, you should read [Groves 2011](https://academic.oup.com/poq/article/75/5/861/1831518) on the past and future of sampling. -->\n    \n## Full count enumeration\n\nWhat proportion of our class prefers to sit in the front of the room?\n\nWe answered this question in class using **full count enumeration**: list the entire target population and ask them the question. Full count enumeration is ideal because it removes all statistical sources of error. But in settings with a larger target population, the high cost of full count enumeration may be prohibitive.\n\n## Simple random sample\n\nWe carried out a **simple random sample**^[Technically, a simple random sample draws units independently with equal probabilities, and with replacement. Our sample is actually drawn without replacement. In an infinite population, the two are equivalent.] in class.\n\n- everyone generated a random number between 0 and 1\n- those with values less than 0.1 were sampled\n- our sample estimate was the proportion of those sampled to prefer the front of the room\n\nIn a simple random sample, each person in the population is sampled with equal probabilities. Because the probabilities are known, a simple random sample is a **probability sample**.\n\n{{< video https://www.youtube.com/embed/IDGKgpiM218 >}}\n\n## Unequal probability sample\n\nSuppose we want to make subgroup estimates:\n\n- what proportion prefer the front, among those sitting in the first 3 rows?\n- what proportion prefer the front, among those sitting in the back 17 rows?\n\nIn a simple random sample, we might only get a few or even zero people in the first 3 rows! To reduce the chance of this bad sample, we could draw an unequal probability sample:\n\n- those in rows 1--3 are selected with probability 0.5\n- those in rows 4--20 are selected with probability 0.1\n\nOur unequal probability sample will over-represent the first three rows, thus creating a large enough sample in this subgroup to yield precise estimates.\n\n{{< video https://www.youtube.com/embed/4r_85uHrNY0 >}}\n\nHaving drawn an unequal probability sample, suppose we now want to estimate the class-wide proportion who prefer sitting in the front. We will have a problem: those who prefer the front may be more likely to sit there, and they are also sampled with a higher probability! Sample inclusion is related to the value of our outcome.\n\nBecause the sampling probabilities are known, we can correct for this by applying **sampling weights**, which for each person equals the inverse of the known probability of inclusion for that person.\n\nFor those in rows 1--3,\n\n- we sampled with probability 50\\%\n- on average 1 in every 2 people is sampled\n- each person in the sample represents 2 people in the population\n- $w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.5} = 2$\n\nFor those in rows 4--20,\n\n- we sampled with probability 10\\%\n- on average 1 in every 10 people is sampled\n- each person in the sample represents 10 people in the population\n- $w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.1} = 10$\n\nTo estimate the population mean, we can use the weighted sample mean,\n\n$$\\frac{\\sum_i y_iw_i}{\\sum_i w_i}$$\n\n{{< video https://www.youtube.com/embed/nczwdCiTmMk >}}\n\n## Stratified random sample\n\nWe could also draw a **stratified random sample** by first partitioning the population into subgroups (called **strata**) and then drawing samples within each subgroup. For instance,\n\n- sample 10 of the 20 people in rows 1--3\n- sample 10 of the 130 people in rows 4--17\n\nIn simple random or unequal probability sampling, it is always possible that by random chance we sample no one in the front of the room. Stratified random sampling rules this out: we know in advance how our sample will be balanced across the two strata.\n\n::: {.callout-note}\nIn our real-data example at the end of this page, the Current Population Survey is stratified by state so that the Bureau of Labor Statistics knows in advance that they will gather a sufficient sample to estimate unemployment in each state.\n:::\n\n## A real case: The Current Population Survey {#sec-cps}\n\nEvery month, the Bureau of Labor Statistics in collaboration with the U.S. Census Bureau collects data on unemployment in the Current Population Survey (CPS). The CPS is a probability sample designed to estimate the unemployment rate in the U.S. and in each state.\n\nWe will be using the CPS in discussion. This video introduces the CPS and points you toward where you can access the data via [IPUMS-CPS](https://cps.ipums.org/cps/).\n\n{{< video https://www.youtube.com/embed/1VxcmPOWwG0 >}}\n\n## Example: Baseball players {#sec-baseball}\n\nAs one example where full-count enumeration is possible, we will examine the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by [USA Today](https://databases.usatoday.com/major-league-baseball-salaries-2023/) and are available in [baseball.csv](../data/baseball.csv).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbaseball <- read_csv(\"https://soc114.github.io/data/baseball.csv\")\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 944 × 4\n  player            team         position   salary\n  <chr>             <chr>        <chr>       <dbl>\n1 Scherzer, Max     N.Y. Mets    RHP      43333333\n2 Verlander, Justin N.Y. Mets    RHP      43333333\n3 Judge, Aaron      N.Y. Yankees OF       40000000\n4 Rendon, Anthony   L.A. Angels  3B       38571429\n5 Trout, Mike       L.A. Angels  OF       37116667\n# ℹ 939 more rows\n```\n\n\n:::\n:::\n\n\nSalaries are high, and income inequality is also high among baseball players\n\n- 4% were paid the league minimum of \\$720,000\n- 53% were paid less than \\$2,000,000\n- the highest-paid players---Max Scherzer and Justin Verlander---each earned $43,333,333\n- the highest-paid half of players take home 92% of the total pay\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3_sampling_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nPay also varies widely across teams!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3_sampling_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Conceptualize the sampling strategy\n\nSuppose you did not have the whole population. You still want to learn the population mean salary! How could you learn that in a sample of 60 out of the 944 players?\n\nBefore reading on, think through three questions:\n\n1) What would it mean to use each of these strategies?\n\n- a simple random sample of 60 players\n- a sample stratified by the 30 MLB teams\n- a sample clustered by the 30 MLB teams\n\n2) Which strategies have advantages in terms of\n\n- being least expensive?\n- having the best statistical properties?\n\n3) Given that you already have the population, how would you write some R code to carry out the sampling strategies? You might use `sample_n()` and possibly `group_by()`.\n\n### Sampling strategies in code\n\nIn a **simple random sample**, we draw 60 players from the entire league. Each player's probability of sample inclusion is $\\frac{60}{n}$ where $n$ is the number of players in the league (944).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_sample <- function(population) {\n  population |>\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 60 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |>\n    # Sample 60 players\n    sample_n(size = 60)\n}\n```\n:::\n\n\nTo use this function, we give it the `baseball` data as the population and it returns a tibble containing a sample of 60 players.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_sample(population = baseball)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 60 × 6\n   player             team         position   salary p_sampled sampling_weight\n   <chr>              <chr>        <chr>       <dbl>     <dbl>           <dbl>\n 1 Walker, Christian  Arizona      1B        6500000    0.0636            15.7\n 2 Fujinami, Shintaro Oakland      RHP       3250000    0.0636            15.7\n 3 Tatis, Fernando**  San Diego    OF        7000000    0.0636            15.7\n 4 McCann, James*     Baltimore    C        12000000    0.0636            15.7\n 5 Merrifield, Whit   Toronto      2B        6750000    0.0636            15.7\n 6 Pena, Jeremy       Houston      SS         754900    0.0636            15.7\n 7 Harper, Bryce*     Philadelphia OF       27538462    0.0636            15.7\n 8 Stubbs, Garrett    Philadelphia C          741000    0.0636            15.7\n 9 Mancini, Trey      Chicago Cubs 1B        7000000    0.0636            15.7\n10 Walsh, Jared*      L.A. Angels  1B        2650000    0.0636            15.7\n# ℹ 50 more rows\n```\n\n\n:::\n:::\n\n\nIn a **stratified random sample** by team, we sample 2 players on each of 30 teams. A stratified random sample is often a higher-quality sample, because it eliminates the possibility of an unlucky draw that completely omits a few teams. All teams are equally represented no matter what happens in the randomization. The downside of a stratified random sample is that it is costly.\n\nEach player's probability of sample inclusion is $\\frac{2}{n}$ where $n$ is the number on that player's team (which ranges from 28 to 35).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstratified_sample <- function(population) {\n  population |>\n    # Draw sample within each team\n    group_by(team) |>\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 2 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |>\n    # Within each team, sample 2 players\n    sample_n(size = 2) |>\n    ungroup()\n}\n```\n:::\n\n\nIn a sample **clustered by team**, we might first sample 3 teams and then sample 20 players on each sampled team. A clustered sample is often less costly, for example because you would only need to call up the front office of 3 teams instead of 30 teams. But this type of sample is lower quality, because there is some chance that one will randomly select a few teams that all have particularly high or low average salaries. A clustered random sample is less expensive but is more susceptible to random error based on the clusters chosen.\n\nEach player's probability of sample inclusion is P(Team Chosen) $\\times$ P(Chosen Within Team) = $\\frac{3}{30}\\times\\frac{20}{n}$ where $n$ is the number on that player's team (which ranges from 28 to 35).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclustered_sample <- function(population) {\n  \n  # First, sample 3 teams\n  sampled_teams <- population |>\n    # Make one row per team\n    distinct(team) |>\n    # Sample 3 teams\n    sample_n(3) |>\n    # Store those 3 team names in a vector\n    pull()\n  \n  # Then load data on those teams and sample 20 per team\n  population |>\n    filter(team %in% sampled_teams) |>\n    # Define sampling probability and weight\n    group_by(team) |>\n    mutate(\n      p_sampled = (3 / 30) * (20 / n()),\n      sampling_weight = 1 / p_sampled\n    ) |>\n    # Sample 20 players\n    sample_n(20) |>\n    ungroup()\n}\n```\n:::\n\n\n### Weighted mean estimator\n\nGiven a sample, how do we estimate the population mean? The weighted mean estimator can also be placed in a function\n\n- we hand our sample to the function\n- we get a numeric estimate back\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator <- function(sample) {\n  sample |>\n    summarize(estimate = weighted.mean(\n      x = salary, \n      w = sampling_weight\n    )) |>\n    pull(estimate)\n}\n```\n:::\n\n\nHere is what it looks like to use the estimator.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_example <- simple_sample(population = baseball)\nestimator(sample = sample_example)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4708913\n```\n\n\n:::\n:::\n\n\nTry it for yourself! The true mean salary in the league is $4,965,481. How close do you come when you apply the estimator to a sample drawn by each strategy?\n\n### Evaluating performance: Many samples\n\nWe might like to know something about performance across many repeated samples. The `replicate` function will carry out a set of code many times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_estimates <- replicate(\n  n = 1000,\n  expr = {\n    a_sample <- simple_sample(population = baseball)\n    estimator(sample = a_sample)\n  }\n)\n```\n:::\n\n\nSimulate many samples. Which one is the best? Strategy A, B, or C?\n\n{{< video https://www.youtube.com/embed/K38fqGB60no >}}\n\n### The danger of one sample\n\nIn actual science, we typically have only one sample. Any estimate we produce from that sample involves some signal about the population quantities, and also some noise. Herein is the danger: researchers are very good at telling stories about why their sample evidence tells something about the population, even when it may be random noise. We illustrate this with an example.\n\nDoes salary differ between left- and right-handed pitchers? To address this question, I create a tibble with only the pitchers(those for whom the `position` variable takes the value `LHP` or `RHP`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npitchers <- baseball |>\n  filter(position == \"LHP\" | position == \"RHP\")\n```\n:::\n\n\nTo illustrate what can happen with a sample, we now draw a sample. Let's first set our computer's random number seed so we get the same sample each time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1599)\n```\n:::\n\n\nThen draw a sample of 40 pitchers \n\n\n::: {.cell}\n\n```{.r .cell-code}\npitchers_sample <- pitchers |>\n  sample_n(size = 40)\n```\n:::\n\n\nand examine the mean difference in salary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npitchers_sample |>\n  group_by(position) |>\n  summarize(salary_mean = mean(salary))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  position salary_mean\n  <chr>          <dbl>\n1 LHP         9677309.\n2 RHP         3182428.\n```\n\n\n:::\n:::\n\n\nThe left-handed pitchers make millions of dollars more per year! You can probably tell many stories why this might be the case. Maybe left-handed pitchers are needed by all teams, and there just aren't many available because so few people are left-handed!\n\nWhat happens if we repeat this process many times? The figure below shows many repeated samples of size 40 from the population of pitchers.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3_sampling_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nOur original result was really random noise: we happened by chance to draw a sample with some highly-paid left-handed pitchers!\n\nThis exercise illustrates what is known as the **replication crisis**: findings that are surprising in one sample may not hold in other repeated samples from the same population, or in the population as a whole. The replication crisis has many sources. One principal source is the one we illustrated above: sample-based estimates involve some randomness, and well-meaning researchers are (unfortunately) very good at telling interesting stories.\n\nOne solution to the replication crisis is to pay close attention to the statistical uncertainty in our estimates, such as that from random sampling. Another solution is to re-evaluate findings that are of interest on new samples. In any case, both the roots of the problem and the solutions are closely tied to sources of randomness in estimates, such as those generated using samples from a population.\n\n{{< video https://www.youtube.com/embed/VN8-54WcJfM >}}\n\n## The future of sample surveys\n\nSample surveys served as a cornerstone of social science research from the 1950s to the present. But there are concerns about their future:\n\n- some sampling frames, such as landline telephones, have become obsolete\n- response rates have been falling for decades\n- sample surveys are slower and more expensive than digital data\n\nWhat is the future for sample surveys? How can they be combined with other data?\n\nWe will close with a discussion of these questions, which you can also engage with in the [Groves 2011](https://academic.oup.com/poq/article/75/5/861/1831518) reading that follows this module.\n\n{{< video https://www.youtube.com/embed/k4o0RA78kxA >}}\n\n## Using weights\n\nWhen studying population-level inequality, our goal is to draw inference about all units in the population. We want to know about the people in the U.S., not just the people who answer the Current Population Survey. Drawing inference from a sample to a population is most straightforward for a simple random sample: when people are chosen at random with equal probabilities. For simple random samples, the sample average of any variable is an unbiased and consistent estimator of the population average.\n\nBut the Current Population Survey is not a simple random sample. Neither are most labor force samples! These samples still begin with a sampling frame, but people are chosen with **unequal** probabilities. We need sample weights to address this fact.\n\nIn the CPS, a key goal is to estimate unemployment in each state. Every state needs to have enough sample size---even tiny states like Wyoming. In order to make those estimates, the CPS **oversamples** people who live in small states.\n\n::: {.callout-note}\n## An example: California and Wyoming\nIn 2022, California had 14,822 CPS-ASEC respondents out of a population of 39,029,342. Wyoming had 2,199 CPS-ASEC respondents out of 581,381 residents. The average probability that a CA resident was sampled was about 0.04 percent, whereas the same probability in WY was 0.4 percent. You are 10 times more likely to be sampled for the ASEC if you live in Wyoming.\n:::\n\nTo draw good population inference, our analysis must incorporate what we know about how the data were collected. If we ignore the weights, our sample will have too many people from Wyoming and too few people from California. Weights correct for this.\n\n## How survey designers create weights\n\nTo calculate sampling weight on person $i$, those who design survey samples take the ratio \n$$\\text{weight on unit }i = \\frac{1}{\\text{probability of including person }i\\text{ in the sample}}$$\nYou can think of the sampling weight as the number of population members a given sample member represents. If there are 100 people with a 1\\% chance of inclusion, then on average 1 of them will be in the sample. That person represents $\\frac{1}{.01}=100$ people.\n\n::: {.callout-note}\n## Example redux: California and Wyoming\nSuppose Californians are sampled with probability 0.0004. Then each Californian represents 1 / 0.0004 = 2,500 people. Each Californian should receive a weight of 2,500. Working out the same math for Wyoming, each Wyoming resident should receive a weight of 250. The total weight on these two samples will then be proportional to the sizes of these two populations.\n:::\n\nIn practice, weighting is more complicated: survey administrators adjust weights for differential nonresponse across population subgroups (a method called post-stratification). How to construct weights is beyond the scope of this course, and could be a whole course in itself!\n\n## Point estimates\n\nWhen we download data, we typically download a column of weights. For simplicity, suppose we are given a sample of four people. The `weight` column tells us how many people in the population each person represents. The `employed` column tells us whether each person employed.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n     name weight employed\n1    Luis      4        1\n2 William      1        0\n3   Susan      1        0\n4  Ayesha      4        1\n```\n\n\n:::\n:::\n\n\nIf we take an unweighted mean, we would conclude that only 50\\% of the population is employed. But with a weighted mean, we would conclude that 80\\% of the population is employed! This might be the case if the sample was designed to oversample people at a high risk of unemployment.\n\n| Estimator | Math | Example | Result |\n|:--|:--|:--|:--|\n| Unweighted mean | $=\\frac{\\sum_{i=1}^n Y_i}{n}$ | $=\\frac{1 + 0 + 0 + 1}{4}$ | = 50\\% employed |\n| Weighted mean | $=\\frac{\\sum_{i=1}^n w_iY_i}{\\sum_{i=1}^n w_i}$ | $=\\frac{4*1 + 1*0 + 1*0 + 4*1}{4 + 1 + 1 + 4}$ | = 80\\% employed |\n\nIn R, the [`weighted.mean(x, w)`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/weighted.mean.html) function will calculate weighted means where `x` is an argument for the outcome variable and `w` is an argument for the weight variable.\n\n## Standard errors\n\nAs you know from statistics, our sample mean is unlikely to equal the population mean. There is random variation in which people were chosen for inclusion in our sample, and this means that across hypothetical repeated samples we would get different sample means! You likely learned formulas to create a standard errors, which quantifies how much a sample estimator would move around across repeated samples.\n\nUnfortunately, the formula you learned doesn't work for complex survey samples! Simple random samples (for which those formulas hold) are actually quite rare. When you face a complex survey sample, those who administer the survey might provide\n\n- a vector of $n$ weights for making a point estimate\n- a matrix of $n\\times k$ replicate weights for making standard errors\n\nBy providing $k$ different ways to up- and down-weight various observations, the replicate weights enable you to generate $k$ estimates that vary in a way that mimics how the estimator might vary if applied to different samples from the population. For instance, our employment sample might come with 3 replicate weights.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n     name weight employed repwt1 repwt2 repwt3\n1    Luis      4        1      3      5      3\n2 William      1        0      1      2      2\n3   Susan      1        0      3      1      1\n4  Ayesha      4        1      5      3      4\n```\n\n\n:::\n:::\n\n\nThe procedure to use replicate weights depends on how they are constructed. Often, it is relatively straightforward:\n\n- use `weight` to create a point estimate $\\hat\\tau$\n- use `repwt*` to generate $k$ replicate estimates $\\hat\\tau^*_1,\\dots,\\hat\\tau^*_k$\n- calculate the standard error of $\\hat\\tau$ using the replicate estimates $\\hat\\tau^*$. The formula will depend on how the replicate weights were constructed, but it will likely involve the standard deviation of the $\\hat\\tau^*$ multiplied by some factor\n- construct a confidence interval^[If we hypothetically drew many complex survey samples from the population in this way, an interval generated this way would contain the true population mean 95\\% of the time.] by a normal approximation\n$$(\\text{point estimate}) \\pm 1.96 * (\\text{standard error estimate})$$\n\nIn our concrete example, the point estimate is 80\\% employed. The replicate estimates are 0.67, 0.73, 0.70. Variation across the replicate estimates tells us something about how the estimate would vary across hypothetical repeated samples from the population.\n\n## Computational strategy for replicate weights\n\nUsing replicate weights can be computationally tricky! It becomes much easier if you write an `estimator()` function. Your function accepts two arguments\n\n- `data` is the `tibble` containing the data\n- `weight_name` is the name of a column containing the weight to be used (e.g., \"repwt1\")\n\n**Example.** If our estimator is the weighted mean of employment,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator <- function(data, weight_name) {\n  data |>\n    summarize(\n      estimate = weighted.mean(\n        x = employed,\n        # extract the weight column\n        w = sim_rep |> pull(weight_name)\n      )\n    ) |> \n    # extract the scalar estimate\n    pull(estimate)\n}\n```\n:::\n\n\nIn the code above, `sim_rep |> pull(weight_name)` takes the data frame `sim_rep` and extracts the weight variable that is named `weight_name`. There are other ways to do this also.\n\nWe can now apply our estimator to get a point estimate with the main sampling weight,\n\n::: {.cell}\n\n```{.r .cell-code}\nestimate <- estimator(data = sim_rep, weight_name = \"weight\")\n```\n:::\n\nwhich yields the point estimate 0.80. We can use the same function to produce the replicate estimates,\n\n::: {.cell}\n\n```{.r .cell-code}\nreplicate_estimates <- c(\n  estimator(data = sim_rep, weight_name = \"repwt1\"),\n  estimator(data = sim_rep, weight_name = \"repwt2\"),\n  estimator(data = sim_rep, weight_name = \"repwt3\")\n)\n```\n:::\n\nyielding the three estimates: 0.67, 0.73, 0.70. In real data, you will want to apply this in a loop because there may be dozens of replicate weights.\n\nThe standard error of the estimator will be some function of the replicate estimates, likely involving the standard deviation of the replicate estimates. Check with the data distributor for a formula for your case. Once you estimate the standard error, a 95\\% confidence interval can be constructed with a Normal approximation, as discussed above.\n\n## Application in the CPS\n\nStarting in 2005, the CPS-ASEC samples include 160 replicate weights. If you download replicate weights for many years, the file size will be enormous. We illustrate the use of replicate weights with a question that can be explored with only one year of data: among 25-year olds in 2023, how did the proportion holding four-year college degrees differ across those identifying as male and female?\n\nWe first load some packages, including the `foreach` package which will be helpful when looping through replicate weights.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(foreach)\n```\n:::\n\n\nTo answer our research question, we download 2023 CPS-ASEC data including the variables [`sex`](https://cps.ipums.org/cps-action/variables/SEX), [`educ`](https://cps.ipums.org/cps-action/variables/EDUC), [`age`](https://cps.ipums.org/cps-action/variables/AGE), the weight variable [`asecwt`](https://cps.ipums.org/cps-action/variables/ASECWT), and the replicate weights [`repwtp*`](https://cps.ipums.org/cps-action/variables/REPWTP).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncps_data <- read_dta(\"../data_raw/cps_00079.dta\")\n```\n:::\n\n\nWe then define an estimator to use with these data. It accepts a tibble `data` and a character `weight_name` identifying the name of the weight variable, and it returns a tibble with two columns: `sex` and `estimate` for the estimated proportion with a four-year degree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimator <- function(data, weight_name) {\n  data |>\n    # Define focal_weight to hold the selected weight\n    mutate(focal_weight = data |> pull(weight_name)) |>\n    # Restrict to those age 25+\n    filter(age >= 25) |>\n    # Restrict to valid reports of education\n    filter(educ > 1 & educ < 999) |>\n    # Define a binary outcome: a four-year degree\n    mutate(college = educ >= 110) |>\n    # Estimate weighted means by sex\n    group_by(sex) |>\n    summarize(estimate = weighted.mean(\n      x = college,\n      w = focal_weight\n    ))\n}\n```\n:::\n\n\nWe produce a point estimate by applying that estimator with the `asecwt`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimate <- estimator(data = cps_data, weight_name = \"asecwt\")\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  sex        estimate\n  <dbl+lbl>     <dbl>\n1 1 [male]      0.369\n2 2 [female]    0.397\n```\n\n\n:::\n:::\n\n\nUsing the `foreach` package, we apply the estimator 160 times---once with each replicate weight---and use the argument `.combine = \"rbind\"` to stitch results together by rows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(foreach)\nreplicate_estimates <- foreach(r = 1:160, .combine = \"rbind\") %do% {\n  estimator(data = cps_data, weight_name = paste0(\"repwtp\",r))\n}\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 320 × 2\n   sex        estimate\n   <dbl+lbl>     <dbl>\n 1 1 [male]      0.368\n 2 2 [female]    0.396\n 3 1 [male]      0.371\n 4 2 [female]    0.400\n 5 1 [male]      0.371\n 6 2 [female]    0.397\n 7 1 [male]      0.369\n 8 2 [female]    0.397\n 9 1 [male]      0.370\n10 2 [female]    0.398\n# ℹ 310 more rows\n```\n\n\n:::\n:::\n\n\nWe estimate the standard error of our estimator by a formula\n$$\\text{StandardError}(\\hat\\tau) = \\sqrt{\\frac{4}{160}\\sum_{r=1}^{160}\\left(\\hat\\tau^*_r - \\hat\\tau\\right)^2}$$\nwhere the formula comes from the [survey documentation](https://cps.ipums.org/cps/repwt.shtml#q40). We carry out this procedure within groups defined by sex, since we are producing estimate for each sex.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandard_error <- replicate_estimates |>\n  # Denote replicate estimates as estimate_star\n  rename(estimate_star = estimate) |>\n  # Merge in the point estimate\n  left_join(estimate,\n            by = join_by(sex)) |>\n  # Carry out within groups defined by sex\n  group_by(sex) |>\n  # Apply the formula from survey documentation\n  summarize(standard_error = sqrt(4 / 160 * sum((estimate_star - estimate) ^ 2)))\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  sex        standard_error\n  <dbl+lbl>           <dbl>\n1 1 [male]          0.00280\n2 2 [female]        0.00291\n```\n\n\n:::\n:::\n\n\nFinally, we combine everything and construct a 95\\% confidence interval by a Normal approximation.\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- estimate |>\n  left_join(standard_error, by = \"sex\") |>\n  mutate(ci_min = estimate - 1.96 * standard_error,\n         ci_max = estimate + 1.96 * standard_error)\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  sex        estimate standard_error ci_min ci_max\n  <dbl+lbl>     <dbl>          <dbl>  <dbl>  <dbl>\n1 1 [male]      0.369        0.00280  0.364  0.375\n2 2 [female]    0.397        0.00291  0.391  0.403\n```\n\n\n:::\n:::\n\n\nWe use `ggplot()` to visualize the result.\n\n::: {.cell}\n\n```{.r .cell-code}\nresult |>\n  mutate(sex = as_factor(sex)) |>\n  ggplot(aes(\n    x = sex, \n    y = estimate,\n    ymin = ci_min, \n    ymax = ci_max,\n    label = scales::percent(estimate)\n  )) +\n  geom_errorbar(width = .2) +\n  geom_label() +\n  scale_x_discrete(\n    name = \"Sex\", \n    labels = str_to_title\n  ) +\n  scale_y_continuous(name = \"Proportion with 4-Year College Degree\") +\n  ggtitle(\n    \"Sex Disparities in College Completion\",\n    subtitle = \"Estimates from the 2023 CPS-ASEC among those age 25+\"\n  )\n```\n\n::: {.cell-output-display}\n![](3_sampling_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\nWe conclude that those identifying as female are more likely to hold a college degree. Because we can see the confidence intervals generated using the replicate weights, we are reasonably confident in the statistical precision of our point estimates.\n\n# Takeaways\n\n- we use samples to learn about the population\n- this often requires sample weights because of unequal inclusion probabilities\n- point estimates are easy with functions like `weighted.mean()`\n- standard errors are harder, but possible via replicate weights\n- it can help to write an explicit `estimator()` function that carries out all the steps to estimate the unknown population parameters\n\nAt the highest level, it is important to remember that our goal is to study the population---not the sample. When we understand how the sample was generated from the population, this makes it possible to draw the correct inferences about the population from the sample.",
    "supporting": [
      "3_sampling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}