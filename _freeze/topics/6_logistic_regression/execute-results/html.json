{
  "hash": "0f08a41c245d2fed4d12e49d1ceee099",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression\"\nformat: \n  html:\n    fig-height: 3\n---\n\n> Here are slides in [website](../slides/lec06_logistic_regression/lec06_logistic_regression.qmd) and [pdf](../slides/lec06_logistic_regression/lec06_logistic_regression.pdf) format.\n\nLogistic regression is an approach to predict binary outcomes ($Y$ taking the values `{0,1}` or `{FALSE,TRUE}`) as a function of one or more predictor variables (a vector $\\vec{X}$). This page introduces logistic regression using an example from the baseball data.\n\n## Baseball data example\n\nAs an example, we continue to use the data on baseball salaries, with a small twist. The file [`baseball_population.csv`](../data/baseball_population.csv) contains the following variables\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopulation <- read_csv(\"https://soc114.github.io/data/baseball_population.csv\")\n```\n:::\n\n\n::: {.cell}\n\n:::\n\n\n- `player` is the player name\n- `salary` is the 2023 salary\n- `position` is the position played (e.g., `LHP` for left-handed pitcher)\n- `team` is the team name\n- `team_past_record` was the team's win percentage in the previous season\n- `team_past_salary` was the team's average salary in the previous season\n\n## A binary outcome\n\nSuppose we model the probability that a player is a catcher (`position == \"C\"`) as a linear function of player salary. For illustration, we do this on the full population.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_binary_outcome <- lm(\n  position == \"C\" ~ salary,\n  data = population\n)\n```\n:::\n\n\nCatchers tend to have low salaries, so the probability of being a catcher declines as player salary rises. But the linear model carries this trend perhaps further than it ought to: the estimated probability of being a catcher for a player making \\$40 million is -2%! This prediction doesn't make a lot of sense.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npopulation |>\n  mutate(yhat = predict(ols_binary_outcome)) |>\n  ggplot(aes(x = salary, y = yhat)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_x_continuous(\n    name = \"Player Salary\",\n    labels = label_currency(scale = 1e-6, suffix = \"m\")\n  ) +\n  scale_y_continuous(\n    name = \"Predicted Probability\\nof Being a Catcher\"\n  ) +\n  theme_minimal() +\n  ggtitle(\"Modeling a binary outcome with a line\")\n```\n\n::: {.cell-output-display}\n![](6_logistic_regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nLogistic regression is similar to OLS, except that it uses a nonlinear function (the logistic function) to convert between coefficients that can take any negative or positive values and predictions that always fall in the [0,1] interval.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](6_logistic_regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nMathematically, logistic regression replaces $\\text{E}(Y\\mid\\vec{X})$ on the left side of the equation with the logistic function.\n\n$$\n\\underbrace{\\log\\left(\\frac{\\text{P}(Y\\mid\\vec{X})}{1 - \\text{P}(Y\\mid\\vec{X})}\\right)}_\\text{Logistic Function} = \\alpha + \\vec{X}'\\vec\\beta\n$$\n\nIn our example with the catchers, we can use logistic regression to model the probability of being a catcher using the `glm()` function. The `family = \"binomial\"` line tells the function that we want to estimate logistic regression (since \"binomial\" is a distribution for outcomes drawn at random with a given probability).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_regression <- glm(\n  position == \"C\" ~ salary,\n  data = population,\n  family = \"binomial\"\n)\n```\n:::\n\n\nWe can predict exactly as with OLS, except that we need to add the `type = \"response\"` argument to ensure that R transforms the predicted values into the space of predicted probabilities [0,1] instead of the space in which the coefficients are defined ($-\\inf,\\inf$).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npopulation |>\n  mutate(yhat = predict(logistic_regression, type = \"response\")) |>\n  distinct(salary, yhat) |>\n  ggplot(aes(x = salary, y = yhat)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_x_continuous(\n    name = \"Player Salary\",\n    labels = label_currency(scale = 1e-6, suffix = \"m\")\n  ) +\n  scale_y_continuous(\n    name = \"Predicted Probability\\nof Being a Catcher\"\n  ) +\n  theme_minimal() +\n  ggtitle(\"Modeling a binary outcome with logistic regression\")\n```\n\n::: {.cell-output-display}\n![](6_logistic_regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Predicting at new values\n\nSuppose we had a new player whose salary was \\$5 million. What is the probability that this player is a catcher? We can define data at which to make a prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- tibble(salary = 5e6)\n```\n:::\n\n\nThen we can predict, just as with OLS. Importantly, we use the `type = \"response\"` argument to specify that we want to predict the probability of being a catcher, not the log odds of being a catcher.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(\n  logistic_regression,\n  newdata = to_predict,\n  type = \"response\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1 \n0.07255671 \n```\n\n\n:::\n:::\n\n\nWhat about a player with a salary of \\$40 million? With a salary so high, this player has a much lower probability of being a catcher.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict_40 <- tibble(salary = 40e6)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(\n  logistic_regression,\n  newdata = to_predict_40,\n  type = \"response\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1 \n0.01090266 \n```\n\n\n:::\n:::\n\n\nA player with a salary of \\$40 million has only a 1-in-100 chance of being a catcher, according to our model.\n\n## Comparison: Linear and logistic regression\n\nTo summarize, linear regression and logistic regression both use an assumed model to share information across units with different values of $\\vec{X}$. This model involves a linear predictor $\\hat\\mu = \\vec{X}'\\hat{\\vec\\beta} = \\hat\\beta_0 + X_1\\hat\\beta_1 + X_2\\hat\\beta_2 + \\cdots$.\n\nThe difference is that\n\n- in OLS, the predicted value of $Y$ is $\\hat\\mu$\n- in logistic regression, the predicted value of $Y$ is $\\text{logit}^{-1}(\\hat\\mu)$, which is a nonlinear function that ensures predicted values always fall between 0 and 1\n\nIn many practical settings, either OLS or logistic regression are good options with binary outcomes. The difference arises mainly when probabilities approach 0 and 1, in applications where it would be problematic to make predictions outside the (0,1) interval.\n\n\n",
    "supporting": [
      "6_logistic_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}