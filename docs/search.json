[
  {
    "objectID": "topics/s0_working_with_data.html",
    "href": "topics/s0_working_with_data.html",
    "title": "Working with data",
    "section": "",
    "text": "This topic is covered on Jan 7.\n\nData science is a world of abundant opportunities. Over recent decades, the field has been transformed by the rapid growth of both computational power and available data. With the proliferation of new technological advances and data opportunities, a social scientist might reasonably ask: where do I begin?\nA good place to begin is to first\n\nChoose a question data can answer\nWork with data to answer the question\n\nWe will first talk about how to ask a good question that data can answer. Then, we will begin preparing our computers with software and tools to answer those questions. Over the quarter, we will develop the skills to ask our own questions and find data to answer them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/8_data_driven_selection.html",
    "href": "topics/8_data_driven_selection.html",
    "title": "Data-Driven Estimator Selection",
    "section": "",
    "text": "Topic for 2/2.\nQuantitative social scientists have long faced the question of how to choose a model. Even within the scope of linear regression, one might wonder whether a model that interacts two predictors is better than one that includes them only additively. As computational advances have yielded new algorithms for prediction, the number of choices has exploded. Many models are possible. How should we choose?\nAn algorithm for prediction takes as its input a feature vector \\(\\vec{x}\\) and returns as its output a predicted value, \\(\\hat{y}\\). One way to choose among several algorithms is to find the one that produces predictions \\(\\hat{y}\\) that are as close as possible to the true outcomes \\(y\\). While this predictive metric might seem grounded in data science, this page will show how metrics of predictive performance can also help with a classical social science task: estimating subgroup means. By the end of the page, we will have motivated why one should care about metrics of predictive performance and learned tools to estimate predictive performance by sample splitting.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/8_data_driven_selection.html#predicting-for-individuals",
    "href": "topics/8_data_driven_selection.html#predicting-for-individuals",
    "title": "Data-Driven Estimator Selection",
    "section": "Predicting for individuals",
    "text": "Predicting for individuals\nContinuing with the example of baseball player salaries, we consider a model to predict salary this year as a linear function of team average salary from last year. We first prepare the environment and load data.\n\nlibrary(tidyverse)\n\n\nbaseball &lt;- read_csv(\"https://soc114.github.io/data/baseball_population.csv\")\n\nThen we draw a sample of 5 players per team.\n\nset.seed(90095)\nlearning &lt;- baseball |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 5) |&gt;\n  ungroup()\n\nWe estimate a linear regression model on this sample.\n\nlinear_regression &lt;- lm(\n  salary ~ team_past_salary, \n  data = learning\n)\n\nThe figure below visualizes the predictions from this linear regression, calculated for all players who were not part of the random learning sample.\n\n\n\n\n\n\n\n\n\nWhile one might have hoped to tell a story about high-quality prediction, the dominant story in the individual-level prediction plot is one of poor prediction: players’ salaries vary widely around the estimated regression line. To put that fact to a number, one might consider \\(R^2\\) which involves a ratio of two expected squared prediction errors, one from the prediction function \\(\\hat{f}\\) and one from a comparison model that predicts the mean for all cases.1\n\\[\nR^2\n= 1 - \\frac{\n    \\overbrace{\n        \\text{E}\\left[\n            \\left(\n                Y - \\hat{f}(\\vec{X})\n            \\right)^2\n        \\right]\n    }^{\\substack{\\text{Expected Squared}\\\\\\text{Prediction Error}}}\n}\n{\n    \\underbrace{\n        \\text{E}\\left[\\left(Y - \\text{E}(Y)\\right)^2\\right]\n    }_\\text{Variance of $Y$}\n}\n\\] In other words, subtracting the predicted values from the individual players’ salaries only reduces the expected squared error by 5.8%. If the goal is to predict for individuals, the model does not seem very good!",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/8_data_driven_selection.html#estimating-subgroup-means",
    "href": "topics/8_data_driven_selection.html#estimating-subgroup-means",
    "title": "Data-Driven Estimator Selection",
    "section": "Estimating subgroup means",
    "text": "Estimating subgroup means\nA social scientist might respond that the goal was never to accurately predict the salary of any individual baseball player. Rather, the data on individual players was in service of a more aggregate goal: estimating the mean salary on each team. Noting that the prediction is the same for every player on a team, the social scientist might propose the graph below, in which the unit of analysis is a team instead of a player.\n\n\n\n\n\n\n\n\n\nThe social scientist might argue that the model is quite good for team salaries. If we take the goal to be to estimate the team average salary, then we might create an analogous version of \\(R^2\\) focused on estimation of team-average salaries.2\n\\[\nR^2_\\text{Group}\n= 1 - \\frac{\n    \\overbrace{\n        \\text{E}\\left[\n            \\left(\n                \\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\n            \\right)^2\n        \\right]\n    }^{\\substack{\\text{Expected Squared}\\\\\\text{Estimation Error}}}\n}\n{\n    \\underbrace{\n        \\text{E}\\left[\\left(\\text{E}(Y\\mid\\vec{X}) - \\text{E}(Y)\\right)^2\\right]\n    }_\\text{If Predicted $\\text{E}(Y)$ for Everyone}\n}\n\\] By that metric, the model seems quite good, predicting away 57.1% of the expected squared error at the team level. Surprisingly, a model that was not very good at predicting for individuals might be quite good at predicting the team-average outcomes!\nOne might respond that prediction and estimation are simply different goals, with little to do with one another. But in fact the two are mathematically linked. Given two models to choose from, the one that predicts better (in squared error loss) will also be a better estimator of the subgroup means.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/8_data_driven_selection.html#prediction-and-estimation-are-connected",
    "href": "topics/8_data_driven_selection.html#prediction-and-estimation-are-connected",
    "title": "Data-Driven Estimator Selection",
    "section": "Prediction and estimation are connected",
    "text": "Prediction and estimation are connected\nTo formalize the problem of choosing an estimator, suppose we have two prediction functions \\(\\hat{f}_1\\) and \\(\\hat{f}_2\\). Each function takes in a vector of features \\(\\vec{x}\\) and returns a predicted value, \\(\\hat{f}_1(\\vec{x})\\) or \\(\\hat{f}_2(\\vec{x})\\). We will assume for simplicity that each function has already been learned on a simple random sample from our population, and that the remaining units available to us are those that were not used in the learning process.\nSuppose we draw a random unit with features \\(\\vec{X}\\) and outcome \\(Y\\). For this unit, algorithm one would have a squared prediction error \\((Y - \\hat{f}_1(\\vec{X}))^2\\). We might score each algorithm’s performance by the average squared prediction error, with the average taken across units.\n\\[\n\\underbrace{\\text{ESPE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Prediction Error}}} = \\text{E}\\left[\\left(Y - \\hat{f}(\\vec{X})\\right)^2\\right]\n\\] In our baseball example, the algorithm \\(\\hat{f}_1\\) makes an error when Mookie Betts has a salary of 21.2m but the algorithm only predicts 7.0m. The expected squared prediction error is the squared difference between these two values, taken on average over all players.\nOur social scientist has already replied that we rarely care about predicting the salary of an individual player. Instead, our questions are really about estimating subgroup means, such as the mean pay on each team. The social scientist might instead want to know about estimation error, \\[\n\\underbrace{\\text{ESEE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Estimation Error}}} = \\text{E}\\left[\\left(\\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right)^2\\right]\n\\] where \\(\\hat{f}(\\vec{X})\\) is the predicted salary of a player on team \\(\\vec{X}\\) and \\(\\text{E}(Y\\mid\\vec{X})\\) is the true population average salary on that team. This social scientist does not care about predicting for individual salaries \\(Y\\), but rather about accurately estimating the mean salary \\(\\text{E}(Y\\mid\\vec{X})\\) in each team.\nA little math (proof at the end of this section) can show that these two goals are actually closely linked. Expected squared prediction error equals expected squared estimation error plus expected within-group variance.\n\\[\n\\underbrace{\\text{ESPE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Prediction Error}}}  = \\underbrace{\\text{ESEE}(\\hat{f})}_{\\substack{\\text{Expected Squared}\\\\\\text{Estimation Error}}} + \\underbrace{\\text{E}\\left[\\text{V}(Y\\mid\\vec{X})\\right]}_{\\substack{\\text{Expected Within-}\\\\\\text{Group Variance}}}\n\\] Taking our baseball example, there are two sources of prediction error for Mookie Betts.\nFirst, salaries vary among the Dodger players (\\(\\text{V}(Y\\mid\\vec{X} = \\text{Dodgers})\\)). Because Mookie Betts and Freddie Freeman are both players on the Dodgers, they are identical from the perspective of the model (they have identical \\(\\vec{X}\\) values) and it has to make the same prediction for both of them. Just as there is variance within the Dodgers, there is variance within all MLB teams. The within-team variance averaged over teams (weighted by size) is the term at the right of the decomposition.\nSecond, the expected squared estimation error is the average squared difference between each player’s predicted salary and the true mean pay on that player’s team, \\(\\text{E}(Y\\mid\\vec{X})\\). In the case of Mookie Betts, this is the difference between the prediction for Mookie Betts and the true mean salary on his team, the Dodgers. Estimation error corresponds to our error if our goal is to estimate the mean salary on each team, instead of predicting the salary for each individual.\nNow suppose two prediction algorithms \\(\\hat{f}_1\\) and \\(\\hat{f}_2\\) have different performance. For example, maybe the first algorithm is a better predictor: \\(\\text{ESPE}(\\hat{f}_1) &lt; \\text{ESPE}(\\hat{f}_2)\\). Regardless of which algorithm is used, the within-group variance component of the decomposition is unchanged. Therefore, if algorithm 1 is the better predictor, then it must also be the better estimator: \\(\\text{ESEE}(\\hat{f}_1) &lt; \\text{ESEE}(\\hat{f}_2)\\).\nIn fact, suppose an algorithm was omniscient and managed to predict the true conditional mean function for every observation, \\(\\hat{f}_\\text{Omniscient}(\\vec{X}) = \\text{E}(Y\\mid\\vec{X})\\). Then estimation error would be zero for this function. Prediction error would equal the expected within-group variance. The best possible prediction function (with squared error loss) is the conditional mean. This is one intuitive reason why an algorithm with good predictive performance is also a good estimator.\nThese facts motivate an idea for choosing an estimation function: to estimate conditional means well, choose the algorithm that minimizes squared prediction error.\nAppendix to section. A proof of the decomposition is provided below, but the ideas above are more important than the details of the proof.\n\\[\n\\begin{aligned}\n\\text{ESPE}(\\hat{f})\n&= \\text{E}\\left[\\left(Y - \\hat{f}(\\vec{X})\\right)^2\\right] \\\\\n&\\text{Add zero} \\\\\n&= \\text{E}\\left[\\left(Y - \\text{E}(Y\\mid\\vec{X}) + \\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right)^2\\right] \\\\\n&= \\underbrace{\n  \\text{E}\\left[\\left(Y - \\text{E}(Y\\mid\\vec{X})\\right) ^ 2\\right]\n}_{=\\text{E}[\\text{V}(Y\\mid\\vec{X})]}\n  + \\underbrace{\n  \\text{E}\\left[\\left(\\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right) ^ 2\\right]\n  }_{=\\text{ESEE}(\\hat{f})}\n  \\\\\n  &\\qquad + \\underbrace{\n  2\\text{E}\\left[\\left(Y - \\text{E}(Y\\mid\\vec{X})\\right)\\left(\\text{E}(Y\\mid\\vec{X}) - \\hat{f}(\\vec{X})\\right)\\right]\n  }_{\\substack{=\\text{Cov}[Y - \\text{E}(Y\\mid\\vec{X}), E(Y\\mid\\vec{X} - \\hat{f}(\\vec{X}))]=0\\\\\\text{covariance of within-group error and estimation error,}\\\\\\text{equals zero if the test case }Y\\text{ is not used to learn }\\hat{f}}} \\\\\n  &= \\text{ESEE}(\\hat{f}) + \\text{E}\\left[\\text{V}(Y\\mid\\vec{X})\\right]\n\\end{aligned}\n\\]",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/8_data_driven_selection.html#why-out-of-sample-prediction-matters",
    "href": "topics/8_data_driven_selection.html#why-out-of-sample-prediction-matters",
    "title": "Data-Driven Estimator Selection",
    "section": "Why out-of-sample prediction matters",
    "text": "Why out-of-sample prediction matters\nThe connection between prediction and estimation opens a powerful bridge: we can find good estimators by exploring which algorithms predict well. But it is important to remember that this bridge exists only for out-of-sample prediction error: error when predictions are made on a new sample that did not include the initial predictions.\n\nk-nearest neighbors estimator\nTo illustrate in-sample and out-of-sample prediction error, we consider a nearest neighbors estimator. When making a prediction for player \\(i\\), we might worry that we have too few sampled units on the team of player \\(i\\). We might solve this issue by averaging the sampled salaries within the team of player \\(i\\) and also the \\(k\\) nearest teams whose salaries in the past season were most similar.\nFor example, the Dodgers’ past-year average salary was $8.39m. The most similar team to them was the N.Y. Mets, who had an average salary of $8.34m. Because the past salaries are so similar, we might pool information: predict the Dodgers’ mean salary by the average of sampled players on both the Dodgers and the N.Y. Mets. If we wanted to pool more information, we might include the next-most similar team, the N.Y. Yankees with past salary $7.60m. We could pool more by also including the 3rd-nearest neighbor (Philadelphia, $6.50m), the 4th-nearest neighbor (San Diego, $6.39m), and so on. The more neighbors we include, the more pooled our estimate becomes.\n\n\nIn-sample performance\nHow many neighbors should we include? We first consider evaluating by in-sample performance: learn the estimator on a sample and evaluate predictive performance in that same sample. We repeatedly:\n\ndraw a sample of 10 players per team\napply the \\(k\\)-nearest neighbor estimator\nevaluate mean squared prediction error in that same sample\n\nThe blue line in the figure below shows results. In-sample mean squared prediction error is lowest when we pool over 0 neighbors. With in-sample evaluation, the predictions become gradually worse (higher mean squared error) as we pool information over more teams. If our goal were in-sample prediction, we should choose an estimator that does not pool information at all: the Dodgers’ population mean salary would be estimated by the mean among the sampled Dodgers only.\n\n\n\n\n\n\n\n\n\n\n\nOut-of-sample performance\nThe red line in the figure above shows a different performance metric: out-of-sample performance. This line shows what happens when we repeatedly:\n\ndraw a sample of 10 players per team\napply the \\(k\\)-nearest neighbor estimator\nevaluate mean squared prediction error on all units not included in that sample\n\nThe red line of out-of-sample performance looks very different than the blue line of in-sample performance, in two ways.\nFirst, the red line is always higher than the blue line. It is always harder to predict out-of-sample cases than to predict in-sample cases. This is unsurprising—the blue line was cheating by getting to see the outcomes of the very cases it was trying to predict!\nSecond, the red line exhibits a U-shaped relationship. Predictive performance improves (lower mean squared error) as we pool information over a few nearby teams. This is because the variance of the estimator is declining. After reaching an optimal value at around 10 neighbors, predictive performance begins to become worse (higher mean squared error).\nOne way to think about the red and blue lines is in terms of the signal and the noise. In any particular sample of 10 Dodger players, there is some amount of signal (true information about the Dodger population mean) and some amount of noise (randomness in the sample average arising from which 10 players we happened to sample). The distinction is irrelevant for in-sample prediction error, for which a close fit to both the signal and the noise yields low prediction error. But for out-of-sample prediction error, fitting to the signal improves performance while fitting to the noise harms performance. As one moves to the right in the graph, one is getting less of the signal and less of the noise. Thus, the blue line of in-sample performance gets consistently worse. The red line improves at first as the reduction in noise outweighs the reduction in signal, but then gets worse as the reduction in signal begins to outweigh the reduction in noise. In-sample prediction error is a poor metric because fitting closely to the noise can make this metric look misleadingly good. Out-of-sample error avoids this problem. The best value for nearest neighbors is the one that optimizes the tradeoff between signal and noise, where the red curve is minimized.\nAnother way to think about the lines is in terms of a bias-variance tradeoff. As we pool the Dodgers together with the N.Y. Mets and other teams, the variance of the estimator declines because the Dodger predicted salary is averaged over more teams. But the bias of the estimator increases: the N.Y. Mets are not the Dodgers, and including them in the average induces a bias. The minimum of the red curve is the amount of information pooling that optimizes the bias-variance tradeoff.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/8_data_driven_selection.html#sample-splitting",
    "href": "topics/8_data_driven_selection.html#sample-splitting",
    "title": "Data-Driven Estimator Selection",
    "section": "Sample splitting",
    "text": "Sample splitting\nThe illustration above showed that we ideally evaluate an estimator learned in a sample by performance when making predictions for the rest of the population (excluding that sample). But in practice, we often have access only to the sample and not to the rest of the population. To learn out-of-sample predictive performance, we need sample splitting.\nIn its simplest version, sample splitting proceeds in three steps:\n\nRandomly partition sampled cases into training and test sets\nLearn the prediction function among training cases\nEvaluate its performance among test cases\n\nVisually, we begin by randomly assigning cases into training or test sets.\n\n\n\n\n\n\n\n\n\nThen we separate these into two datasets. We use the train set to learn the model and the test set to evaluate the performance of the learned model.\n\n\n\n\n\n\n\n\n\nIn code, we can carry out a train-test split by first loading the baseball population,\n\nbaseball_population &lt;- read_csv(\"https://soc114.github.io/data/baseball_population.csv\")\n\nand drawing a sample of 10 players per team.\n\nbaseball_sample &lt;- baseball_population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 10) |&gt;\n  ungroup()\n\nWe can then create the split. The code below first stratifies by team and then randomly samples 50% of cases to be used for the train set.\n\ntrain &lt;- baseball_sample |&gt;\n  group_by(team) |&gt;\n  slice_sample(prop = .5)\n\nThe code below takes all remaining cases not used for training to be used as the test set.\n\ntest &lt;- baseball_sample |&gt;\n  anti_join(train, by = join_by(player))\n\nWe can then learn a prediction function in the train set and evaluate performance in the test set. For example, the code below uses OLS.\n\nols &lt;- lm(salary ~ team, data = train)\ntest_mse &lt;- test |&gt;\n  # Make predictions\n  mutate(yhat = predict(ols, newdata = test)) |&gt;\n  # Calculate squared errors\n  mutate(squared_error = (salary - yhat) ^ 2) |&gt;\n  # Summarize\n  summarize(mse = mean(squared_error))",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/8_data_driven_selection.html#tuning-parameters-by-sample-splitting",
    "href": "topics/8_data_driven_selection.html#tuning-parameters-by-sample-splitting",
    "title": "Data-Driven Estimator Selection",
    "section": "Tuning parameters by sample splitting",
    "text": "Tuning parameters by sample splitting\nOne way we might use sample splitting is for parameter tuning: to choose the value of some unknown tuning parameter such as the penalty \\(\\lambda\\) in ridge regression.\n\nlibrary(glmnet)\n\nLoaded glmnet 4.1-10\n\nridge_regression &lt;- glmnet(\n  x = model.matrix(~team, data = train),\n  y = train |&gt; pull(salary),\n  alpha = 0\n)\n\nThe glmnet package makes estimates at many values of the penalty parameter \\(\\lambda\\). We can make predictions in the test set from models using these various values of \\(\\lambda\\).\n\npredicted &lt;- predict(\n  ridge_regression,\n  newx = model.matrix(~team, data = test)\n)\n\nWe can extract the penalty parameter values with ridge_regression$lambda, organizing them in a tibble for ease of access.\n\nlambda_tibble &lt;- tibble(\n  lambda = ridge_regression$lambda,\n  lambda_index = colnames(predicted)\n)\n\nFor each \\(\\lambda\\) value, the predictions are a column of predicted. The code below wrangles these predictions into a tidy format.\n\npredicted_tibble &lt;- as_tibble(predicted) |&gt;\n  # Append the actual value of the test outcome\n  mutate(y = test |&gt; pull(salary)) |&gt;\n  pivot_longer(cols = -y, names_to = \"lambda_index\", values_to = \"yhat\") |&gt;\n  left_join(lambda_tibble, by = join_by(lambda_index)) |&gt;\n  mutate(squared_error = (y - yhat) ^ 2)\n\nAt each \\(\\lambda\\) value, we can calculate mean squared error in the test set.\n\nmse &lt;- predicted_tibble |&gt;\n  group_by(lambda) |&gt;\n  summarize(mse = mean(squared_error))\n\nThe figure below visualizes these estimates. As the penalty parameter grows larger, predictive performance improves (lower error) to a point and then begins to get worse. The selected best value of the tuning parameter \\(\\lambda\\) is highlighted in blue.\n\n\nWarning: `label` cannot be a &lt;ggplot2::element_blank&gt; object.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/8_data_driven_selection.html#cross-validation",
    "href": "topics/8_data_driven_selection.html#cross-validation",
    "title": "Data-Driven Estimator Selection",
    "section": "Cross-validation",
    "text": "Cross-validation\nA downside of 50-50 sample splitting is that the train set is only half as large as the full available sample. We might prefer if it were larger, for instance 80% of the data used for training and only 20% used for testing. But then our estimates of performance in the test set might be noisy.\nOne solution to this problem is cross-validation, which proceeds in a series of steps:\n\nRandomize the sampled cases into a set of folds (e.g., 5 folds).\nTake fold 1 as the test set and estimate predictive performance.\nTake fold 2 as the test set and estimate predictive performance.\nIterate until all folds have served as the test set\nAverage predictive performance over the folds\n\n\n\n\n\n\n\n\n\n\nOptionally, repeat for many repetitions of randomly assigning cases to folds to reduce stochastic error.\nCross-validation is so common that it is already packaged into some of the learning algorithms we have considered in class. For example, the code below carries out cross-validation to automatically select the penalty parameter for ridge regression.\n\nridge_regression_cv &lt;- cv.glmnet(\n  x = model.matrix(~team, data = train),\n  y = train |&gt; pull(salary),\n  alpha = 0\n)\n\nWe can use ridge_regression_cv$lambda.min to extract the chosen value of \\(\\lambda\\) that minimizes cross-validated mean squared error (2.7074931^{7}). We can also visualize the performance with a plot() function.\n\nplot(ridge_regression_cv)\n\n\n\n\n\n\n\n\nWe can make predictions at the chosen value of \\(\\lambda\\) by specifying the s argument in the predict() function.\n\npredicted &lt;- test |&gt;\n  mutate(\n    yhat = predict(\n      ridge_regression_cv,\n      s = \"lambda.min\",\n      newx = model.matrix(~team, data = test)\n    )[,1]\n  )",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/8_data_driven_selection.html#choosing-an-algorithm-by-sample-splitting",
    "href": "topics/8_data_driven_selection.html#choosing-an-algorithm-by-sample-splitting",
    "title": "Data-Driven Estimator Selection",
    "section": "Choosing an algorithm by sample splitting",
    "text": "Choosing an algorithm by sample splitting\nIn the example above, we used sample splitting to choose the optimal value of a penalty parameter (\\(\\lambda\\)) within a particular algorithm for prediction (ridge regression). One can also use a train-test split to choose among many very different estimators. For example, we might estimate OLS, a tree, and a forest.\n\nlibrary(rpart)\nlibrary(grf)\n\n\nols &lt;- lm(salary ~ team_past_salary, data = train)\ntree &lt;- rpart(salary ~ team_past_salary, data = train)\nforest &lt;- regression_forest(\n  X = model.matrix(~team_past_salary, data = train),\n  Y = train |&gt; pull(salary)\n)\n\nWe can make predictions for all of these estimators in the test set.\n\npredicted &lt;- test |&gt;\n  mutate(\n    yhat_ols = predict(ols, newdata = test),\n    yhat_tree = predict(tree, newdata = test),\n    yhat_forest = predict(forest, newdata = model.matrix(~team_past_salary, data = test))$predictions\n  ) |&gt;\n  pivot_longer(\n    cols = starts_with(\"yhat\"), names_to = \"estimator\", values_to = \"yhat\"\n  )\n\nThe figure below visualizes the predicted values.\n\n\n\n\n\n\n\n\n\nWe can calculate mean squared error in the test set for each algorithm to determine which one is producing the best predictions.\n\nmse &lt;- predicted |&gt;\n  group_by(estimator) |&gt;\n  mutate(squared_error = (salary - yhat) ^ 2) |&gt;\n  summarize(mse = mean(squared_error))\n\n\n\nWarning: `label` cannot be a &lt;ggplot2::element_blank&gt; object.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/8_data_driven_selection.html#footnotes",
    "href": "topics/8_data_driven_selection.html#footnotes",
    "title": "Data-Driven Estimator Selection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen estimating \\(R^2\\), we often use the training sample mean as an estimator of \\(\\text{E}(Y)\\) for the denominator, similar to how the training sample is used to learn \\(\\hat{f}\\).↩︎\nWhen estimating \\(R^2_\\text{Group}\\), we often use the subgroup training sample mean as an estimator of \\(\\text{E}(Y\\mid\\vec{X})\\) for the denominator, similar to how the training sample is used to learn \\(\\hat{f}\\).↩︎",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Data-Driven Estimator Selection"
    ]
  },
  {
    "objectID": "topics/6_logistic_regression.html",
    "href": "topics/6_logistic_regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Here are slides in website and pdf format.\nLogistic regression is an approach to predict binary outcomes (\\(Y\\) taking the values {0,1} or {FALSE,TRUE}) as a function of one or more predictor variables (a vector \\(\\vec{X}\\)). This page introduces logistic regression using an example from the baseball data.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "topics/6_logistic_regression.html#baseball-data-example",
    "href": "topics/6_logistic_regression.html#baseball-data-example",
    "title": "Logistic Regression",
    "section": "Baseball data example",
    "text": "Baseball data example\nAs an example, we continue to use the data on baseball salaries, with a small twist. The file baseball_population.csv contains the following variables\n\npopulation &lt;- read_csv(\"https://soc114.github.io/data/baseball_population.csv\")\n\n\nplayer is the player name\nsalary is the 2023 salary\nposition is the position played (e.g., LHP for left-handed pitcher)\nteam is the team name\nteam_past_record was the team’s win percentage in the previous season\nteam_past_salary was the team’s average salary in the previous season",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "topics/6_logistic_regression.html#a-binary-outcome",
    "href": "topics/6_logistic_regression.html#a-binary-outcome",
    "title": "Logistic Regression",
    "section": "A binary outcome",
    "text": "A binary outcome\nSuppose we model the probability that a player is a catcher (position == \"C\") as a linear function of player salary. For illustration, we do this on the full population.\n\nols_binary_outcome &lt;- lm(\n  position == \"C\" ~ salary,\n  data = population\n)\n\nCatchers tend to have low salaries, so the probability of being a catcher declines as player salary rises. But the linear model carries this trend perhaps further than it ought to: the estimated probability of being a catcher for a player making $40 million is -2%! This prediction doesn’t make a lot of sense.\n\n\nCode\npopulation |&gt;\n  mutate(yhat = predict(ols_binary_outcome)) |&gt;\n  ggplot(aes(x = salary, y = yhat)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_x_continuous(\n    name = \"Player Salary\",\n    labels = label_currency(scale = 1e-6, suffix = \"m\")\n  ) +\n  scale_y_continuous(\n    name = \"Predicted Probability\\nof Being a Catcher\"\n  ) +\n  theme_minimal() +\n  ggtitle(\"Modeling a binary outcome with a line\")\n\n\n\n\n\n\n\n\n\nLogistic regression is similar to OLS, except that it uses a nonlinear function (the logistic function) to convert between coefficients that can take any negative or positive values and predictions that always fall in the [0,1] interval.\n\n\n\n\n\n\n\n\n\nMathematically, logistic regression replaces \\(\\text{E}(Y\\mid\\vec{X})\\) on the left side of the equation with the logistic function.\n\\[\n\\underbrace{\\log\\left(\\frac{\\text{P}(Y\\mid\\vec{X})}{1 - \\text{P}(Y\\mid\\vec{X})}\\right)}_\\text{Logistic Function} = \\alpha + \\vec{X}'\\vec\\beta\n\\]\nIn our example with the catchers, we can use logistic regression to model the probability of being a catcher using the glm() function. The family = \"binomial\" line tells the function that we want to estimate logistic regression (since “binomial” is a distribution for outcomes drawn at random with a given probability).\n\nlogistic_regression &lt;- glm(\n  position == \"C\" ~ salary,\n  data = population,\n  family = \"binomial\"\n)\n\nWe can predict exactly as with OLS, except that we need to add the type = \"response\" argument to ensure that R transforms the predicted values into the space of predicted probabilities [0,1] instead of the space in which the coefficients are defined (\\(-\\inf,\\inf\\)).\n\n\nCode\npopulation |&gt;\n  mutate(yhat = predict(logistic_regression, type = \"response\")) |&gt;\n  distinct(salary, yhat) |&gt;\n  ggplot(aes(x = salary, y = yhat)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_x_continuous(\n    name = \"Player Salary\",\n    labels = label_currency(scale = 1e-6, suffix = \"m\")\n  ) +\n  scale_y_continuous(\n    name = \"Predicted Probability\\nof Being a Catcher\"\n  ) +\n  theme_minimal() +\n  ggtitle(\"Modeling a binary outcome with logistic regression\")",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "topics/6_logistic_regression.html#predicting-at-new-values",
    "href": "topics/6_logistic_regression.html#predicting-at-new-values",
    "title": "Logistic Regression",
    "section": "Predicting at new values",
    "text": "Predicting at new values\nSuppose we had a new player whose salary was $5 million. What is the probability that this player is a catcher? We can define data at which to make a prediction.\n\nto_predict &lt;- tibble(salary = 5e6)\n\nThen we can predict, just as with OLS. Importantly, we use the type = \"response\" argument to specify that we want to predict the probability of being a catcher, not the log odds of being a catcher.\n\npredict(\n  logistic_regression,\n  newdata = to_predict,\n  type = \"response\"\n)\n\n         1 \n0.07255671 \n\n\nWhat about a player with a salary of $40 million? With a salary so high, this player has a much lower probability of being a catcher.\n\nto_predict_40 &lt;- tibble(salary = 40e6)\n\n\npredict(\n  logistic_regression,\n  newdata = to_predict_40,\n  type = \"response\"\n)\n\n         1 \n0.01090266 \n\n\nA player with a salary of $40 million has only a 1-in-100 chance of being a catcher, according to our model.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "topics/6_logistic_regression.html#comparison-linear-and-logistic-regression",
    "href": "topics/6_logistic_regression.html#comparison-linear-and-logistic-regression",
    "title": "Logistic Regression",
    "section": "Comparison: Linear and logistic regression",
    "text": "Comparison: Linear and logistic regression\nTo summarize, linear regression and logistic regression both use an assumed model to share information across units with different values of \\(\\vec{X}\\). This model involves a linear predictor \\(\\hat\\mu = \\vec{X}'\\hat{\\vec\\beta} = \\hat\\beta_0 + X_1\\hat\\beta_1 + X_2\\hat\\beta_2 + \\cdots\\).\nThe difference is that\n\nin OLS, the predicted value of \\(Y\\) is \\(\\hat\\mu\\)\nin logistic regression, the predicted value of \\(Y\\) is \\(\\text{logit}^{-1}(\\hat\\mu)\\), which is a nonlinear function that ensures predicted values always fall between 0 and 1\n\nIn many practical settings, either OLS or logistic regression are good options with binary outcomes. The difference arises mainly when probabilities approach 0 and 1, in applications where it would be problematic to make predictions outside the (0,1) interval.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "topics/4_ci.html",
    "href": "topics/4_ci.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Here are slides in website and pdf format. This page goes beyond the content of the slides and is more detailed and mathematical than the slides. Slides provide more support for coding. Notation and ideas on this page loosely draw on Efron & Hastie (2016) Ch 10–11.\nAs researchers adopt algorithmic estimation methods for which analytical standard errors do not exist, methods to produce standard errors by resampling become all the more important. We will discuss the bootstrap for simple random samples and extensions to allow resampling-based standard error estimates in complex survey samples.\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(foreach)\nset.seed(90095)",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#a-motivating-problem",
    "href": "topics/4_ci.html#a-motivating-problem",
    "title": "Confidence Intervals",
    "section": "A motivating problem",
    "text": "A motivating problem\nOut of the population of baseball salaries on Opening Day 2023, imagine that we have a sample of 10 Dodger players.\nWe calculate the mean salary among the sampled Dodgers to be $3.8 million. How much should we trust this estimate?\nFor the sake of discussion, we provide the following information.\n\n\n# A tibble: 3 × 2\n  `Salary Among Sampled Dodgers`    Value\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 sample_mean                    3829119.\n2 sample_standard_deviation      6357851.\n3 sample_size                         10",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#classical-inference",
    "href": "topics/4_ci.html#classical-inference",
    "title": "Confidence Intervals",
    "section": "Classical inference",
    "text": "Classical inference\nTo know how confident to be in our sample-based estimate, we need to reason about why our sample-based estimate might differ from the true (but unknown) population parameter. Let \\(\\hat\\mu\\) denote our estimate for the sample mean of \\(Y\\).\n\\[\\hat\\mu = \\frac{1}{n}\\sum_{i}Y_i\\]\nAcross repeated samples from the population, the estimate \\(\\hat\\mu\\) equals the population mean on average but differs in any particular sample due to random sampling variance. The sample variance of the mean has a known formula.\n\\[\nV(\\hat\\mu) = V\\left(\\frac{1}{n}\\sum_i Y_i\\right) = \\frac{1}{n^2}\\sum_i V(Y_i) = \\frac{V(Y)}{n}\n\\]\nThe sample-to-sample variance of \\(\\hat\\mu\\) will be greater to the degree that \\(Y\\) varies substantially across individuals in the population (larger \\(V(Y)\\)), and will be smaller to the degree that many individuals are included in the sample (larger \\(n\\)).\nYou might be more familiar with this equation expressed as the standard deviation of the estimator, sometimes referred to as the standard error, which is the square root of the variance of the estimator,\n\\[\n\\text{SD}(\\hat\\mu) = \\sqrt{\\text{V}(\\hat\\mu)} = \\frac{\\text{SD}(Y)}{\\sqrt{n}}\n\\] where \\(\\text{SD}()\\) is the standard deviation of \\(Y\\) across individuals in the population.\nFrom the Central Limit Theorem, we know that even if \\(Y\\) is not Normally distributed the sample mean of \\(Y\\) converges to a Normal distribution as the sample size grows. Because we have formulas for these parameters, we can write down a formula for that sampling distribution.\n\\[\n\\hat\\mu \\rightarrow \\text{Normal}\\left(\\text{Mean} = \\text{E}(Y),\\quad \\text{SD} = \\frac{\\text{SD}(Y)}{\\sqrt{n}}\\right)\n\\] The graph below visualizes the sampling variability of the sample mean. Across repeated samples, the sample mean \\(\\hat\\mu\\) is normally distributed about its true population value. The middle 95% of sample estimates \\(\\hat\\mu\\) fall within a region that can be derived with known formulas,\n\n\nWarning in annotate(geom = \"label\", x = 0, y = 0.2 * dnorm(0), label = \"'Middle\n95% of Sample-Based Estimates'~hat(mu)\", : Ignoring unknown parameters:\n`label.size`\n\n\n\n\n\n\n\n\n\nwhere \\(\\Phi^{-1}()\\) is the inverse CDF of the standard Normal distribution.\nYou might be concerned: can a Normal distribution be a good approximation when Dodger player salaries are highly right-skewed? After all this is the distribution of Dodger player salaries.\n\n\n\n\n\n\n\n\n\nBut the sample mean among 10 sampled Dodgers is actually quite close to a normal sampling distribution. This is because of the Central Limit Theorem.\n\n\n\n\n\n\n\n\n\n\nPlug-in estimators\nWe have a formula for the standard deviation of the sample mean, but it involves the term \\(\\text{SD}(Y)\\) which is the unknown population standard deviation of \\(Y\\). It is common to plug in the sample estimate of this value in order to arrive at a sample estimate of the standard deviation of the estimator.\n\\[\n\\widehat{\\text{SD}}(\\hat\\mu) = \\frac{\\widehat{\\text{SD}}(Y)}{\\sqrt{n}} = \\sqrt{\\frac{\\frac{1}{n-1}\\sum_i (Y_i - \\bar{Y})^2}{n}}\n\\]\nThe idea of a plug-in estimator may seem obvious, but soon we will see that the step at which plug-ins occur changes dramatically when we move to resampling methods for statistical inference.\n\n\nClassical confidence intervals\nA 95% confidence interval \\((\\hat\\mu_\\text{Lower},\\hat\\mu_\\text{Upper})\\) is an interval that has the property that across repeated samples the probability that \\(\\hat\\mu_\\text{Lower} &lt; \\mu &lt; \\hat\\mu_\\text{Upper}\\) is 0.95. One way to think about this is that two properties should hold: the probability that the lower limit is too high and the probability that the upper limit is too low are each 0.025.\n\\[\n\\begin{aligned}\n\\text{P}(\\hat\\mu_\\text{Lower} &gt; \\mu) &= .025 \\\\\n\\text{P}(\\hat\\mu_\\text{Upper} &lt; \\mu) &= .025\n\\end{aligned}\n\\]\nYou may know from statistics that a 95% confidence interval for the sample mean can be derived as follows.\n\\[\n\\hat\\mu \\pm \\Phi^{-1}(.975)\\widehat{\\text{SD}}(\\hat\\mu)\n\\] where \\(\\Phi^{-1}(.975)\\) is the value 1.96 that you might look up in the back of a statistics textbook. We can show that these confidence limits have the desired properties. For example, taking the lower limit:\n\\[\n\\begin{aligned}\n&\\text{P}\\left(\\hat\\mu_\\text{Lower} &gt; \\mu\\right)\\\\\n&=\\text{P}\\left(\\hat\\mu - \\Phi^{-1}(.975)\\widehat{\\text{SD}}(\\hat\\mu) &gt; \\mu\\right)\\\\\n&= \\text{P}\\left(\\hat\\mu - \\mu &gt; \\Phi^{-1}(.975)\\widehat{\\text{SD}}(\\hat\\mu)\\right)\\\\\n&= \\text{P}\\left(\\frac{\\hat\\mu - \\mu}{\\widehat{\\text{SD}}(\\hat\\mu)} &gt; \\Phi^{-1}(.975)\\right)\\\\\n&= .025\n\\end{aligned}\n\\]\nwhere the last line holds because \\(\\frac{\\hat\\mu - \\mu}{\\text{SD}(\\hat\\mu)}\\) follows a standard Normal distribution. The proof for the upper limit is similar.\nAcross repeated samples, a 95% confidence interval constructed in this way should contain the true mean 95% of the time. We can visualize this behavior by taking repeated samples of 10 Dodger players from our data.\n\n\n\n\n\n\n\n\n\nIn this particular simulation, we have slight undercoverage and the upper confidence limit is often the one that is incorrect. These problems may arise because our asymptotic normality of mean salaries is an imperfect approximation at a sample size of \\(n = 10\\).",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#analytic-vs-computational-inference-procedures",
    "href": "topics/4_ci.html#analytic-vs-computational-inference-procedures",
    "title": "Confidence Intervals",
    "section": "Analytic vs computational inference procedures",
    "text": "Analytic vs computational inference procedures\nAnalytical confidence intervals (derived by math) are the default for many researchers. Yet the exercise above reveals some of their shortcomings. First, there is a lot of math! Second, despite the math we still relied on the plug-in principle: for unknown quantities such as \\(\\text{SD}(Y)\\) we plug in sample-based estimates \\(\\widehat{\\text{SD}}(Y)\\) and act as though these were known. Third, our results may still yield imperfect coverage because underlying assumptions may be only approximately met. For example, our confidence intervals may have undercovered because the asymptotics of the Central Limit Theorem are unreliable at \\(n = 10\\).\nNow suppose you had a complicated data science approach, such as a predicted value \\(\\hat{Y}_{\\vec{x}}=\\hat{\\text{E}}(Y\\mid \\vec{X} = \\vec{x})\\) from a LASSO regression. How would you place a confidence interval on that predicted value?\nComputational inference procedures take a different approach. These procedures focus on a generic estimator \\(s()\\) applied to data. Instead of deriving properties of the estimator by math, computational approaches seek to simulate what would happen when \\(s()\\) is applied to samples from the population, often by using a plug-in principle at an earlier step.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#the-estimator-function-s",
    "href": "topics/4_ci.html#the-estimator-function-s",
    "title": "Confidence Intervals",
    "section": "The estimator function \\(s()\\)",
    "text": "The estimator function \\(s()\\)\nAt the core of a resampling-based inference procedure is a broad sense of how our estimate comes to be. First, the world has some cumulative distribution function \\(F\\) over data that could be generated. A particular sample \\(\\texttt{data}\\) is drawn from the probability distribution of the world. The researcher then applies an estimator function \\(s()\\) that takes in and returns an estimate \\(s(\\texttt{data})\\).\n\\[F\\rightarrow \\texttt{data} \\rightarrow s(\\texttt{data})\\]\nIn our baseball example, the estimator function is the sample mean of the salary variable.\n\nestimator &lt;- function(data) {\n  data |&gt;\n    summarize(estimate = mean(salary)) |&gt;\n    pull(estimate)\n}\n\nWe would like to repeatedly simulate \\(\\texttt{data}\\) from the world and see the performance of the estimator. But this is only possible in illustrations like the baseball example where the population data are known. When \\(F\\) is unknown and we only see one \\(\\texttt{data}\\), we need a new procedure.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#the-nonparametric-bootstrap",
    "href": "topics/4_ci.html#the-nonparametric-bootstrap",
    "title": "Confidence Intervals",
    "section": "The nonparametric bootstrap",
    "text": "The nonparametric bootstrap\nThe nonparametric bootstrap simulates repeated-sample behavior by a plug-in principle.\n\nPlug in the empirical distribution \\(\\hat{F}\\) of our sample data as an estimate of the true distribution \\(F\\) for the unobserved full population of data\nGenerate a bootstrap sample \\(\\texttt{data}^*\\) by sampling from our empirical data with replacement.\nGenerate an estimate \\(s(\\texttt{data}^*)\\) using the bootstrap data.\nRepeat steps (2) and (3) many times to generate a large number \\(B\\) of bootstrap replicate estimates.\n\nVisually, this procedure is analogous to the above.\n\\[\\hat{F}\\rightarrow \\texttt{data}^* \\rightarrow s(\\texttt{data}^*)\\]",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#nonparametric-bootstrap-standard-errors",
    "href": "topics/4_ci.html#nonparametric-bootstrap-standard-errors",
    "title": "Confidence Intervals",
    "section": "Nonparametric bootstrap standard errors",
    "text": "Nonparametric bootstrap standard errors\nIn classical statistics, the standard error of an estimator is typically a mathematical expression derived for that particular estimator and then estimated by the plug-in principle. For example, the standard error of the mean is \\(\\text{SD}(\\hat\\mu) = \\text{SD}(Y) / \\sqrt{n}\\).\nWith the bootstrap, we avoid this altogether because we have \\(B\\) bootstrap replicate estimates. We can estimate the standard deviation of the estimator over repeated samples by the empirical standard deviation across bootstrap replicates.\n\\[\n\\widehat{\\text{SD}}(s) = \\frac{1}{B-1}\\sum_{r=1}^B \\bigg(s(\\texttt{data}^*_r) - s(\\texttt{data}^*_\\bullet)\\bigg)^2\n\\] where \\(s(\\texttt{data}^*_\\bullet)\\) is the mean of the estimate across the bootstrap samples. Note that just like the analytic standard errors, these have also relied on a plug-in principle: we plugged in the empirical distribution \\(\\hat{F}\\) for the population distribution \\(F\\) when generating bootstrap samples from our empirical data instead of actual samples from the population.\nIn our baseball example, we first load data and draw a sample of 10 Dodger players.\n\npopulation &lt;- read_csv(\"http://soc114.github.io/data/baseball_population.csv\")\nsample &lt;- population |&gt;\n  filter(team == \"L.A. Dodgers\") |&gt;\n  sample_n(size = 10) |&gt;\n  select(player, team, salary)\n\n\n\n# A tibble: 10 × 3\n   player           team           salary\n   &lt;chr&gt;            &lt;chr&gt;           &lt;dbl&gt;\n 1 Barnes, Austin   L.A. Dodgers  3500000\n 2 Reyes, Alex*     L.A. Dodgers  1100000\n 3 Betts, Mookie    L.A. Dodgers 21158692\n 4 Vargas, Miguel   L.A. Dodgers   722500\n 5 May, Dustin      L.A. Dodgers  1675000\n 6 Bickford, Phil   L.A. Dodgers   740000\n 7 Jackson, Andre   L.A. Dodgers   722500\n 8 Thompson, Trayce L.A. Dodgers  1450000\n 9 Pepiot, Ryan*    L.A. Dodgers   722500\n10 Peralta, David   L.A. Dodgers  6500000\n\n\nThe code below generates a bootstrap sample from these 10 players by sampling 10 players with replacement. You will see that some players in the original sample do not appear, and others appear more than once.\n\none_sample &lt;- sample |&gt;\n  slice_sample(prop = 1, replace = TRUE) |&gt;\n  print()\n\n# A tibble: 10 × 3\n   player         team           salary\n   &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;\n 1 Betts, Mookie  L.A. Dodgers 21158692\n 2 Peralta, David L.A. Dodgers  6500000\n 3 Barnes, Austin L.A. Dodgers  3500000\n 4 Pepiot, Ryan*  L.A. Dodgers   722500\n 5 Jackson, Andre L.A. Dodgers   722500\n 6 May, Dustin    L.A. Dodgers  1675000\n 7 Reyes, Alex*   L.A. Dodgers  1100000\n 8 May, Dustin    L.A. Dodgers  1675000\n 9 Vargas, Miguel L.A. Dodgers   722500\n10 Peralta, David L.A. Dodgers  6500000\n\n\nWe can apply our estimator function to this sample to get one bootstrap estimate.\n\nestimator &lt;- function(data) {\n  data |&gt;\n    summarize(estimate = mean(salary)) |&gt;\n    pull(estimate)\n}\nestimator(one_sample)\n\n[1] 4427619\n\n\nThe code below carries out 500 bootstrap samples and estimates the sample mean in each one.\n\nbootstrap_estimates &lt;- foreach(r = 1:1000, .combine = \"c\") %do% {\n  sample |&gt;\n    # Draw a bootstrap sample\n    slice_sample(prop = 1, replace = TRUE) |&gt;\n    # Apply the estimator\n    estimator()\n}\n\nThe figure below shows how that the bootstrap distribution of the estimator compares to the actual sampling distribution of the estimator (known in this case since the population is known).\n\n\n\n\n\n\n\n\n\nThe bootstrap distribution is more heaped on 10 distinct salary values: the particular 10 Dodger player salaries included in our sample. When the variable being summarized takes continuous values, it will generally be more discretized in the bootstrap setting because there are only the sample size \\(n\\) distinct values instead of the population size \\(N\\) of distinct values. Otherwise, the two distributions are similar.\nThe bootstrap estimate of the standard error in this case is\n\nbootstrap_estimates |&gt; sd()\n\n[1] 1965073\n\n\nwhich is 86% of the size of the theoretical standard error of 2.2969632^{6}. Like all sample-based analogs to theoretical standard errors, the bootstrap estimate of the standard error can itself be sensitive to sampling variability.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#parallel-processing",
    "href": "topics/4_ci.html#parallel-processing",
    "title": "Confidence Intervals",
    "section": "Parallel processing",
    "text": "Parallel processing\nThe bootstrap is a computer-age approach to inference: present-day computers are what make it possible to apply the estimator thousands of repeated times.\nParallel processing can make the bootstrap faster. Note that each bootstrap replicate is independent: the first bootstrap replicate results are not needed to carry out the second, and so on. If you had 5 or 10 computers, you could split the bootstrap samples across those computers and carry it out 5 or 10 times as fast!\nYou may be surprised that you likely do have several effectively independent computers (called cores) within your computer. The doParallel package makes it possible to use these cores.\n\nlibrary(doParallel)\n\nWith the doParallel package loaded, you can see how many cores you have with the detectCores() function.\n\ndetectCores()\n\n[1] 12\n\n\nIn my case, there are 12 cores on my computer. This means I can split my bootstrap over 8 parallel processors! First, initialize a cluster with these cores.\n\ncl &lt;- makeCluster(spec = detectCores())\n\nThen tell your computer to use that computing cluster for parallel processing.\n\nregisterDoParallel(cl)\n\nFinally, you can carry out your foreach loop using parallel processing, with the %dopar% operator clarifying that the loop should be done with parallel processing. You will also need the .packages argument to pass to the computing cores any packages that are used within the procedure.\n\nbootstrap_estimates &lt;- foreach(\n  r = 1:1000, \n  .combine = \"c\", \n  .packages = c(\"tidyverse\")\n) %dopar% {\n  sample |&gt;\n    # Draw a bootstrap sample\n    slice_sample(prop = 1, replace = TRUE) |&gt;\n    # Apply the estimator\n    estimator()\n}\n\nIn my case, parallel processing will make the entire procedure about 12 times faster. As computers have become faster with more cores, bootstrapping has become an increasingly feasible method of statistical inference.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#bootstrap-confidence-intervals",
    "href": "topics/4_ci.html#bootstrap-confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Bootstrap confidence intervals",
    "text": "Bootstrap confidence intervals\nThe discussion above has focused on using the bootstrap to estimate standard errors. There are many methods to construct confidence intervals using bootstrap procedures. Two of the most common are the Normal approximation method and the percentile method.\n\nNormal approximation\nThe beginning of this page reviewed classical statistics in which we routinely rely on the Central Limit Theorem which ensures that sample mean estimators are asymptotically Normal. Likewise with the bootstrap, if we believe that \\(s(\\texttt{data})\\) has a Normal sampling distribution, then we can construct a confidence interval by the Normal approximation with the bootstrap estimate of the standard error.\n\\[\ns(\\texttt{data}) \\pm \\Phi^{-1}(.975)\\text{SD}\\big(s(\\text{data}^*)\\big)\n\\]\n\nestimator(sample) + c(-1,1) * qnorm(.975) * sd(bootstrap_estimates)\n\n[1]  131954.5 7526283.9\n\n\n\n\nPercentile method\nThe bootstrap also offers another way to calculate the confidence interval: the middle 95% of the bootstrap estimates.\n\nquantile(bootstrap_estimates, probs = c(.025, .975))\n\n   2.5%   97.5% \n1132531 7892739 \n\n\n\nThe percentile method can work better than the Normal approximation method in cases where normality does not hold. For example, in the beginning of this page we used analytic intervals that seemed imperfect in part because the Central Limit Theorem had not adequately yielded normality at a sample size of 10.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#bootstrap-for-machine-learning-algorithms",
    "href": "topics/4_ci.html#bootstrap-for-machine-learning-algorithms",
    "title": "Confidence Intervals",
    "section": "Bootstrap for machine learning algorithms",
    "text": "Bootstrap for machine learning algorithms\nSuppose a researcher carries out the following procedure.\n\nSample \\(n\\) units from the population\nLearn an algorithm \\(\\hat{f}:\\vec{X}\\rightarrow Y\\) to minimize squared error\nReport a prediction \\(\\hat{\\text{E}}(Y\\mid\\vec{X} = \\vec{x}) = \\hat{f}(\\vec{x})\\)\n\nHow would the researcher use the bootstrap to carry out this process?\n\nDraw a bootstrap sample \\(\\texttt{data}^*\\) of size \\(n\\)\nLearn the algorithm \\(\\hat{f}^*\\) in the bootstrap sample\nStore the bootstrap estimate \\(\\hat{f}^*(\\vec{x})\\)\n\nThen the researcher could create a confidence interval with either the Normal approximation or the percentile method. Note that the bootstrap confidence interval may have undercoverage if the estimator is biased; see the words of warning at the end of this page.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#discussion-what-belongs-in-s",
    "href": "topics/4_ci.html#discussion-what-belongs-in-s",
    "title": "Confidence Intervals",
    "section": "Discussion: What belongs in \\(s()\\)?",
    "text": "Discussion: What belongs in \\(s()\\)?\nIn each example, describe the steps the researcher might use to bootstrap this estimate while capturing all sources of uncertainty.\n\nA researcher first truncates the values of a skewed predictor variable \\(x\\) at the 1st and 99th percentile. Then the researcher learns a regression model and reports \\(\\hat\\beta\\).\nA researcher first uses cross-validation to select the tuning parameter \\(\\lambda\\) for ridge regression. Then, they estimate ridge regression with the chosen \\(\\lambda\\) value and make a prediction \\(\\hat{f}(\\vec{x})\\) at some predictor value \\(\\vec{x}\\) of interest.\nA researcher first learns a prediction function \\(\\hat{f}:\\vec{X}\\rightarrow Y\\) and then sees which subgroup \\(\\vec{x}\\) has the highest predicted value \\(\\hat{f}(\\vec{x})\\), which the researcher reports.\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\nMany steps of the analysis involve uncertainty. It can be ideal to include them all in your bootstrap! Write your estimator function to take in your raw data and return an estimate. The estimator function would include steps like truncating predictors at sample quantiles, choosing tuning parameters, and choosing subgroups of interest on which to focus.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#beyond-simple-random-samples",
    "href": "topics/4_ci.html#beyond-simple-random-samples",
    "title": "Confidence Intervals",
    "section": "Beyond simple random samples",
    "text": "Beyond simple random samples\nThe bootstrap in its simplest form is designed for simple random samples. Straightforward generalizations make it possible to move beyond simple random samples to more complex sampling designs.\n\nStratified bootstrap\nSuppose we draw a sample of players stratified by team: 10 players per team. No matter which random sample is drawn, there will always be 10 Dodgers, 10 Angels, 10 Yankees, and so on. Stratified sampling is often a more efficient estimator than simple random sampling, and our estimator should reflect that!\nAs an example, suppose we have a stratified sample of 10 players per team.\n\nstratified_sample &lt;- population |&gt;\n  group_by(team) |&gt;\n  slice_sample(n = 10) |&gt;\n  ungroup()\n\nWe would generate a stratified bootstrap sample1 by stratifying by team, exactly as the data were generated.\n\nstratified_bootstrap_sample &lt;- stratified_sample |&gt;\n  group_by(team) |&gt;\n  slice_sample(prop = 1, replace = T)\n\nStratified bootstrapping can be important. Using our baseball example, suppose our estimator is the predicted mean salary of the Dodgers from a linear regression.\n\nestimator &lt;- function(data) {\n  ols &lt;- lm(salary ~ team_past_salary, data = data)\n  to_predict &lt;- population |&gt; \n    filter(team == \"L.A. Dodgers\") |&gt; \n    distinct(team_past_salary)\n  predicted &lt;- predict(ols, newdata = to_predict)\n  return(predicted)\n}\n\nWe get different estimates if we carry out simple vs stratified bootstrap sampling.\n\n\n\n\n\n\n\n\n\n\n\nCluster bootstrap\nSuppose we draw a sample of players clustered by team: all players on 10 sampled teams. Clustered sampling is often less expensive than simple random sampling because it can be easier for the person carrying out the survey. This often comes at a cost of statistical efficiency.\nAs an example, suppose we have a clustered sample of 10 teams.\n\nclustered_sample &lt;- population |&gt;\n  distinct(team) |&gt;\n  slice_sample(n = 10) |&gt;\n  left_join(population, by = join_by(team))\n\nWe would generate a clustered bootstrap sample by resampling teams instead of players, exactly as the data were sampled.\n\nchosen_teams &lt;- clustered_sample |&gt;\n  distinct(team) |&gt;\n  slice_sample(prop = 1, replace = T)\nclustered_bootstrap_sample &lt;- foreach(i = 1:nrow(chosen_teams), .combine = \"rbind\") %do% {\n  chosen_teams[i,] |&gt;\n    left_join(clustered_sample, by = join_by(team))\n}\n\nAs before, we get different estimated standard errors if we carry out clustered bootstrap sampling vs standard bootstrap sampling.\n\n\n\n\n\n\n\n\n\n\n\nComplex survey samples\nMany surveys involve complex samples, such as samples stratified by state and then clustered in regions within states. Often the variables that define sampling strata or clusters are geographic, and therefore they are often redacted from the data made available to researchers due to privacy concerns.\nThankfully, many surveys make replicate weights available to researchers. The goal of replicate weights is to enable you to resample the data in a way that mimics the (hidden) ways in which the sample was originally drawn. The rest of this section walks through the use of replicate weights, first in a hypothetical example and then in real data.\nWhen we download data, we typically download a column of weights. For simplicity, suppose we are given a sample of four people. The weight column tells us how many people in the population each person represents. The employed column tells us whether each person employed.\n\n\n     name weight employed\n1    Luis      4        1\n2 William      1        0\n3   Susan      1        0\n4  Ayesha      4        1\n\n\nIf we take an unweighted mean, we would conclude that only 50% of the population is employed. But with a weighted mean, we would conclude that 80% of the population is employed! This might be the case if the sample was designed to oversample people at a high risk of unemployment.\n\n\n\n\n\n\n\n\n\nEstimator\nMath\nExample\nResult\n\n\n\n\nUnweighted mean\n\\(=\\frac{\\sum_{i=1}^n Y_i}{n}\\)\n\\(=\\frac{1 + 0 + 0 + 1}{4}\\)\n= 50% employed\n\n\nWeighted mean\n\\(=\\frac{\\sum_{i=1}^n w_iY_i}{\\sum_{i=1}^n w_i}\\)\n\\(=\\frac{4*1 + 1*0 + 1*0 + 4*1}{4 + 1 + 1 + 4}\\)\n= 80% employed\n\n\n\nIn R, the weighted.mean(x, w) function will calculate weighted means where x is an argument for the outcome variable and w is an argument for the weight variable.\nWhen you face a complex survey sample, those who administer the survey might provide\n\na vector of \\(n\\) weights for making a point estimate\na matrix of \\(n\\times k\\) replicate weights for making standard errors\n\nBy providing \\(k\\) different ways to up- and down-weight various observations, the replicate weights enable you to generate \\(k\\) estimates that vary in a way that mimics how the estimator might vary if applied to different samples from the population. For instance, our employment sample might come with 3 replicate weights.\n\n\n     name weight employed repwt1 repwt2 repwt3\n1    Luis      4        1      3      5      3\n2 William      1        0      1      2      2\n3   Susan      1        0      3      1      1\n4  Ayesha      4        1      5      3      4\n\n\nThe procedure to use replicate weights depends on how they are constructed. Often, it is relatively straightforward:\n\nuse weight to create a point estimate \\(\\hat\\tau\\)\nuse repwt* to generate \\(k\\) replicate estimates \\(\\hat\\tau^*_1,\\dots,\\hat\\tau^*_k\\)\ncalculate the standard error of \\(\\hat\\tau\\) using the replicate estimates \\(\\hat\\tau^*\\). The formula will depend on how the replicate weights were constructed, but it will likely involve the standard deviation of the \\(\\hat\\tau^*\\) multiplied by some factor\nconstruct a confidence interval2 by a normal approximation \\[(\\text{point estimate}) \\pm 1.96 * (\\text{standard error estimate})\\]\n\nIn our concrete example, the point estimate is 80% employed. The replicate estimates are 0.67, 0.73, 0.70. Variation across the replicate estimates tells us something about how the estimate would vary across hypothetical repeated samples from the population.\n\n\nComputational strategy for replicate weights\nUsing replicate weights can be computationally tricky! It becomes much easier if you write an estimator() function. Your function accepts two arguments\n\ndata is the tibble containing the data\nweight_name is the name of a column containing the weight to be used (e.g., “repwt1”)\n\nExample. If our estimator is the weighted mean of employment,\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    summarize(\n      estimate = weighted.mean(\n        x = employed,\n        # extract the weight column\n        w = sim_rep |&gt; pull(weight_name)\n      )\n    ) |&gt; \n    # extract the scalar estimate\n    pull(estimate)\n}\n\nIn the code above, sim_rep |&gt; pull(weight_name) takes the data frame sim_rep and extracts the weight variable that is named weight_name. There are other ways to do this also.\nWe can now apply our estimator to get a point estimate with the main sampling weight,\n\nestimate &lt;- estimator(data = sim_rep, weight_name = \"weight\")\n\nwhich yields the point estimate 0.80. We can use the same function to produce the replicate estimates,\n\nreplicate_estimates &lt;- c(\n  estimator(data = sim_rep, weight_name = \"repwt1\"),\n  estimator(data = sim_rep, weight_name = \"repwt2\"),\n  estimator(data = sim_rep, weight_name = \"repwt3\")\n)\n\nyielding the three estimates: 0.67, 0.73, 0.70. In real data, you will want to apply this in a loop because there may be dozens of replicate weights.\nThe standard error of the estimator will be some function of the replicate estimates, likely involving the standard deviation of the replicate estimates. Check with the data distributor for a formula for your case. Once you estimate the standard error, a 95% confidence interval can be constructed with a Normal approximation, as discussed above.\n\n\nApplication in the CPS\nStarting in 2005, the CPS-ASEC samples include 160 replicate weights. If you download replicate weights for many years, the file size will be enormous. We illustrate the use of replicate weights with a question that can be explored with only one year of data: among 25-year olds in 2023, how did the proportion holding four-year college degrees differ across those identifying as male and female?\nWe first load some packages, including the foreach package which will be helpful when looping through replicate weights.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(foreach)\n\nTo answer our research question, we download 2023 CPS-ASEC data including the variables sex, educ, age, the weight variable asecwt, and the replicate weights repwtp*.\n\ncps_data &lt;- read_dta(\"../data_raw/cps_00079.dta\")\n\nWe then define an estimator to use with these data. It accepts a tibble data and a character weight_name identifying the name of the weight variable, and it returns a tibble with two columns: sex and estimate for the estimated proportion with a four-year degree.\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    # Define focal_weight to hold the selected weight\n    mutate(focal_weight = data |&gt; pull(weight_name)) |&gt;\n    # Restrict to those age 25+\n    filter(age &gt;= 25) |&gt;\n    # Restrict to valid reports of education\n    filter(educ &gt; 1 & educ &lt; 999) |&gt;\n    # Define a binary outcome: a four-year degree\n    mutate(college = educ &gt;= 110) |&gt;\n    # Estimate weighted means by sex\n    group_by(sex) |&gt;\n    summarize(estimate = weighted.mean(\n      x = college,\n      w = focal_weight\n    ))\n}\n\nWe produce a point estimate by applying that estimator with the asecwt.\n\nestimate &lt;- estimator(data = cps_data, weight_name = \"asecwt\")\n\n\n\n# A tibble: 2 × 2\n  sex        estimate\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;\n1 1 [male]      0.369\n2 2 [female]    0.397\n\n\nUsing the foreach package, we apply the estimator 160 times—once with each replicate weight—and use the argument .combine = \"rbind\" to stitch results together by rows.\n\nlibrary(foreach)\nreplicate_estimates &lt;- foreach(r = 1:160, .combine = \"rbind\") %do% {\n  estimator(data = cps_data, weight_name = paste0(\"repwtp\",r))\n}\n\n\n\n# A tibble: 320 × 2\n   sex        estimate\n   &lt;dbl+lbl&gt;     &lt;dbl&gt;\n 1 1 [male]      0.368\n 2 2 [female]    0.396\n 3 1 [male]      0.371\n 4 2 [female]    0.400\n 5 1 [male]      0.371\n 6 2 [female]    0.397\n 7 1 [male]      0.369\n 8 2 [female]    0.397\n 9 1 [male]      0.370\n10 2 [female]    0.398\n# ℹ 310 more rows\n\n\nWe estimate the standard error of our estimator by a formula \\[\\text{StandardError}(\\hat\\tau) = \\sqrt{\\frac{4}{160}\\sum_{r=1}^{160}\\left(\\hat\\tau^*_r - \\hat\\tau\\right)^2}\\] where the formula comes from the survey documentation. We carry out this procedure within groups defined by sex, since we are producing estimate for each sex.\n\nstandard_error &lt;- replicate_estimates |&gt;\n  # Denote replicate estimates as estimate_star\n  rename(estimate_star = estimate) |&gt;\n  # Merge in the point estimate\n  left_join(estimate,\n            by = join_by(sex)) |&gt;\n  # Carry out within groups defined by sex\n  group_by(sex) |&gt;\n  # Apply the formula from survey documentation\n  summarize(standard_error = sqrt(4 / 160 * sum((estimate_star - estimate) ^ 2)))\n\n\n\n# A tibble: 2 × 2\n  sex        standard_error\n  &lt;dbl+lbl&gt;           &lt;dbl&gt;\n1 1 [male]          0.00280\n2 2 [female]        0.00291\n\n\nFinally, we combine everything and construct a 95% confidence interval by a Normal approximation.\n\nresult &lt;- estimate |&gt;\n  left_join(standard_error, by = \"sex\") |&gt;\n  mutate(ci_min = estimate - 1.96 * standard_error,\n         ci_max = estimate + 1.96 * standard_error)\n\n\n\n# A tibble: 2 × 5\n  sex        estimate standard_error ci_min ci_max\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1 [male]      0.369        0.00280  0.364  0.375\n2 2 [female]    0.397        0.00291  0.391  0.403\n\n\nWe use ggplot() to visualize the result.\n\nresult |&gt;\n  mutate(sex = as_factor(sex)) |&gt;\n  ggplot(aes(\n    x = sex, \n    y = estimate,\n    ymin = ci_min, \n    ymax = ci_max,\n    label = scales::percent(estimate)\n  )) +\n  geom_errorbar(width = .2) +\n  geom_label() +\n  scale_x_discrete(\n    name = \"Sex\", \n    labels = str_to_title\n  ) +\n  scale_y_continuous(name = \"Proportion with 4-Year College Degree\") +\n  ggtitle(\n    \"Sex Disparities in College Completion\",\n    subtitle = \"Estimates from the 2023 CPS-ASEC among those age 25+\"\n  )\n\n\n\n\n\n\n\n\nWe conclude that those identifying as female are more likely to hold a college degree. Because we can see the confidence intervals generated using the replicate weights, we are reasonably confident in the statistical precision of our point estimates.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#a-word-of-warning",
    "href": "topics/4_ci.html#a-word-of-warning",
    "title": "Confidence Intervals",
    "section": "A word of warning",
    "text": "A word of warning\nThe bootstrap is a powerful tool, but there are notable cases in which it fails.\nFirst, all frequentist confidence intervals that are based solely on sampling variance may suffer undercoverage if applied to biased estimators. For example, many machine learning algorithms induce bias through regularization. This means that even if we correctly approximate sampling variance, the center of our confidence intervals may be systematically misaligned from the true population parameter, yielding undercoverage.\nSecond, the bootstrap can exhibit unexpected performance with statistics such as the maximum or minimum value, since these statistics can be sensitive to a particular data point. Taking the maximum as an example, the value \\(\\text{max}(\\vec{y}^*)\\) in a bootstrap sample will never be higher than \\(\\text{max}(\\vec{y})\\) in the sample from which that bootstrap was drawn. The entire bootstrap distribution of \\(\\text{max}(\\vec{y}^*)\\) will be at or below the original estimate of \\(\\text{max}(\\vec{y})\\). Like the max or min, quantiles of \\(\\vec{y}\\) can also lead to unexpected bootstrap behavior. Generally the bootstrap will have the best performance for statistics such as the mean for which no particular unit plays an especially determining role.\nNote: In class we will use many_samples.csv. This note is just here so that the data will exist on this page.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/4_ci.html#footnotes",
    "href": "topics/4_ci.html#footnotes",
    "title": "Confidence Intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCalled the “multi-sample bootstrap” in Efron & Hastie.↩︎\nIf we hypothetically drew many complex survey samples from the population in this way, an interval generated this way would contain the true population mean 95% of the time.↩︎",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Confidence Intervals"
    ]
  },
  {
    "objectID": "topics/2b_summary_statistic.html",
    "href": "topics/2b_summary_statistic.html",
    "title": "Summary Statistics",
    "section": "",
    "text": "Here are slides in web and pdf format. This page is part 2 of the slides.\nWe often want to take a distribution and convert it to a summary statistic: a one-number parameter that summarizes a fact about the distribution. A summary statistic collapses the entire distribution down to a single number.\nIn this example, we will continue with the household income data from the previous page. You can load the data with the code below.\nlibrary(tidyverse)\nincomeSimulated &lt;- read_csv(\"https://soc114.github.io/data/incomeSimulated.csv\")\nOne summary statistic is the median: the value at which 50% of households have higher incomes and 50% of households have lower incomes.\nYou can produce the median using the simulated data incomeSimulated.csv with the code below.\nincomeSimulated |&gt;\n  summarize(estimated_median = median(hhincome))\n\n# A tibble: 1 × 1\n  estimated_median\n             &lt;dbl&gt;\n1           69035.\nThis code starts with the incomeSimulated data and uses the pipe operator |&gt; to pass the data to the summarize() function. The summarize function creates a new variable estimated_median which contains the median value of hhincome, calculated by applying the median() function to this variable. The median() function takes a vector of values and returns a single summary. The summarize() function converts our data that had 1000 rows into a new format that has only 1 row containing the summary statistic.\nOne can also produce several summary statistics. The median is a useful measure of central tendency: it gives a sense of the income value in the middle of the distribution. But it may not give us a good sense of inequality, which requires some sense of the spread of the distribution. One may therefore want other summary statistics, such as the 90th and 10th percentiles. The 90th percentile is the household income value such that 90% of households have lower incomes. The 10th percentile is the value such that 10% of households have lower incomes.\nBelow, we produce these summaries in the simulated data. Note that the summarize() function can produce several new variables containing several estimated summary statistics. Here we use the quantile function to produce the percentile estimates.\nincomeSimulated |&gt;\n  summarize(\n    estimated_median = median(x = hhincome),\n    estimated_10th_percentile = quantile(x = hhincome, probs = .1),\n    estimated_90th_percentile = quantile(x = hhincome, probs = .9)\n  )\n\n# A tibble: 1 × 3\n  estimated_median estimated_10th_percentile estimated_90th_percentile\n             &lt;dbl&gt;                     &lt;dbl&gt;                     &lt;dbl&gt;\n1           69035.                    17972.                   228488.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Summary Statistics"
    ]
  },
  {
    "objectID": "topics/2b_summary_statistic.html#how-to-choose-a-summary-statistic",
    "href": "topics/2b_summary_statistic.html#how-to-choose-a-summary-statistic",
    "title": "Summary Statistics",
    "section": "How to choose a summary statistic?",
    "text": "How to choose a summary statistic?\nThe choice of summary statistic involves a subjective (non-empirical) choice about what aspect of the distribution is important to report. If you are normatively concerned with the growth of high incomes, you might want to summarize the 90th or even 99th percentile of the distribution. If you are concerned with the typical household, you might want to summarize by the median. If you are interested in patterns of inequality among low earners, you might summarize with the 10th percentile.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Summary Statistics"
    ]
  },
  {
    "objectID": "topics/1c_r_basics.html",
    "href": "topics/1c_r_basics.html",
    "title": "Basics of R",
    "section": "",
    "text": "Now that you have installed R, it is time to try using R for the first time. Because there already exist many excellent resources on this topic, we will walk together in class through the material in R4DS Ch 2. This material will prepare you to complete Problem Set 1.\nAfter the basics in R4DS Ch 2, return to this page and continue below to learn about an object we will often use in this course: a data frame.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Basics of R"
    ]
  },
  {
    "objectID": "topics/1c_r_basics.html#using-a-data-frame",
    "href": "topics/1c_r_basics.html#using-a-data-frame",
    "title": "Basics of R",
    "section": "Using a data frame",
    "text": "Using a data frame\nThe class of object we will use in this course most often is a data.frame or tibble, which is a particular type of data frame. A data frame is an object like the rectangular spreadsheet below.\n\nA hypothetical data table with three households\n\n\nAddress\nIncome\nNumber of People\n\n\n\n\n5462 Park St\n54,896\n2\n\n\n4596 Ocean Ave Apt B\n22,465\n1\n\n\n6831 River Dr\n134,297\n4\n\n\n\nThe code below will load basicData.csv into an object in R.\n\nlibrary(tidyverse)\nbasicData &lt;- read_csv(\"https://soc114.github.io/data/basicData.csv\")\n\nIf you type basicData in your R console, you will see a printout of the data.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Basics of R"
    ]
  },
  {
    "objectID": "topics/1c_r_basics.html#data-frames-and-empirical-questions",
    "href": "topics/1c_r_basics.html#data-frames-and-empirical-questions",
    "title": "Basics of R",
    "section": "Data frames and empirical questions",
    "text": "Data frames and empirical questions\nThe structure of these data correspond to elements of empirical questions we study.\n\nThe outcome is a variable to be summarized. It appears as a column in the data. Here, the outcome might be income.\nThe unit of analysis is the unit for which the outcome is defined. Each unit corresponds to a row of the data. Here, the unit of analysis is a household.\n\nThe data may also contain other columns, such as the address of the household in this example. The other columns may represent predictor variables or variables that define population subgroups.\nNow that you understand a data frame object, we will next use one to produce a visualization.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Basics of R"
    ]
  },
  {
    "objectID": "topics/1a_empirical_questions.html",
    "href": "topics/1a_empirical_questions.html",
    "title": "Research Questions in Social Data Science",
    "section": "",
    "text": "Slides\nThe most powerful applications of data science begin with a clear understanding of what questions data can (and cannot) answer. The questions data science can most directly answer are empirical questions, which involve quantities that could (at least hypothetically) be measured in the world. Empirical questions include:\nThis course will focus on two classes of empirical questions.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Research Questions in Social Data Science"
    ]
  },
  {
    "objectID": "topics/1a_empirical_questions.html#descriptive-questions",
    "href": "topics/1a_empirical_questions.html#descriptive-questions",
    "title": "Research Questions in Social Data Science",
    "section": "Descriptive questions",
    "text": "Descriptive questions\nThis class will focus on two categories of empirical questions. Descriptive questions describe the world as it is. The first two questions above are descriptive: they summarize the proportion employed and the median household income. If we everyone would tell us their employment and income, we could estimate these quantities directly.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Research Questions in Social Data Science"
    ]
  },
  {
    "objectID": "topics/1a_empirical_questions.html#causal-questions",
    "href": "topics/1a_empirical_questions.html#causal-questions",
    "title": "Research Questions in Social Data Science",
    "section": "Causal questions",
    "text": "Causal questions\nCausal questions involve counterfactual outcomes that do not exist, but would be realized if some aspect of the world were different in a specific way. The third question above is causal because it involves a counterfactual state of the world: what would happen if a new tax policy were implemented.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Research Questions in Social Data Science"
    ]
  },
  {
    "objectID": "topics/1a_empirical_questions.html#questions-that-are-not-empirical",
    "href": "topics/1a_empirical_questions.html#questions-that-are-not-empirical",
    "title": "Research Questions in Social Data Science",
    "section": "Questions that are not empirical",
    "text": "Questions that are not empirical\nData science is best used to answer questions that can be answered with data: empirical questions. One must recognize that data science is less helpful for a second class of questions that are not empirical. One common category of non-empirical questions is normative questions that involve a judgment about what is right or what ought to be. Normative questions include:\n\nThe median household income in the U.S. is too low.\nThe U.S. should address inequality with the policies common in Sweden.\n\nData alone cannot answer non-empirical questions. It is impossible to prove with data that the median household income is too low, for example. You might personally believe that it is too low, and your own subjective beliefs might motivate your analysis. If you want to convince people that median incomes are too low, you might answer empirical questions about the median income and the median value of various costs such as rent, educational expenses, and food. Data science provides empirical answers to these questions. But data science stops short of value judgments about the results.\nThis class takes inequality as a motivating class of examples. Inequality is a topic for which many people bring normative commitments. As a scholar of inequality, you might hope that your objective evidence might spur people to action to promote equality and fairness. Our own normative commitments are often a good motivator for new empirical investigations. But for the credibility of the analysis it is also important to keep normative judgments separate from empirical claims. The most credible data science claims are precise and focus on the questions that data can most directly answer.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Research Questions in Social Data Science"
    ]
  },
  {
    "objectID": "topics/16_rd.html",
    "href": "topics/16_rd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Topic for 3/4.\n\n\n\n\n Back to top",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Unmeasured Confounding",
      "Regression Discontinuity"
    ]
  },
  {
    "objectID": "topics/14_models_for_causal.html",
    "href": "topics/14_models_for_causal.html",
    "title": "Models for Causal Inference",
    "section": "",
    "text": "Topic for 2/23.\nModels are useful when we need subgroup summaries but we do not observe very many units in each subgroup. This situation is common in causal inference: we assume that \\(\\vec{X}\\) is a sufficient adjustment set so that conditional exchangeability holds, and this allows us to identify the causal quantity \\(\\text{E}(Y^a\\mid \\vec{X} = \\vec{x})\\) by the statistical quantity \\(\\text{E}(Y\\mid A = a, \\vec{X} = \\vec{x})\\). But that empirical quantity—the subgroup mean among those with treatment value \\(a\\) and adjustment set value \\(\\vec{x}\\)—may be the mean of a subgroup that is unpopulated. This is especially true in practice because the adjustment set \\(\\vec{X}\\) is often most plausible when it includes many variables, leading to a curse of dimensionality and small subgroup sample sizes. For this reason, causal inference approaches that adjust for measured variables often require us to estimate the means in many subgroups that are sparsely populated.\nThis page introduces outcome models for causal inference. To run the code on this page, you will need the tidyverse.\nlibrary(tidyverse)",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Models for Causal Inference"
    ]
  },
  {
    "objectID": "topics/14_models_for_causal.html#motivating-example",
    "href": "topics/14_models_for_causal.html#motivating-example",
    "title": "Models for Causal Inference",
    "section": "Motivating example",
    "text": "Motivating example\nTo what extent does completing a four-year college degree by age 25 increase the probability of having a spouse or residential partner with a four-year college degree at age 35, among the population of U.S. residents who were ages 12–16 at the end of 1996?\nWe used this example on the Why Model? page and will continue with it here. For those jumping in on this page, here is a refresher.\nThis causal question draws on questions in sociology and demography about assortative mating: the tendency of people with high education, income, or status to form households together1. One reason to care about assortative mating is that it can contribute to inequality across households: if people with high earnings potential form households together, then income inequality across households will be greater than it would be if people formed households randomly.\nOur question is causal: to what extent is the probability of marrying a four-year college graduate higher if one were hypothetically to finish a four-year degree, versus if that same person were hypothetically to not finish a college degree? But in data that exist in the world, we see only one of these two potential outcomes. The people for whom we see the outcome under a college degree are systematically different from those for whom we see the outcome under no degree: college graduates come from families with higher incomes, higher wealth, and higher parental education, for example. All of these factors may directly shape the probability of marrying a college graduate even in the absence of college. Thus, it will be important to adjust for a set of measured confounders, represented by \\(\\vec{X}\\) in our DAG.\n\n\n\n\n\n\n\n\n\nBy adjusting for the variables \\(\\vec{X}\\), we block all non-causal paths between the treatment \\(A\\) and the outcome \\(Y\\) in the DAG. If this DAG is correct, then conditional exchangeability holds with this adjustment set: \\(\\{Y^1,Y^0\\}\\unicode{x2AEB} A \\mid\\vec{X}\\).\nTo estimate, we use data from the National Longitudinal Survey of Youth 1997, a probability sample of U.S. resident children who were ages 12–16 on Dec 31, 1996. The study followed these children and interviewed them every year through 2011 and then every other year after that.\nWe will analyze a simulated version of these data (nlsy97_simulated.csv), which you can access with this line of code.\n\nall_cases &lt;- read_csv(\"https://soc114.github.io/data/nlsy97_simulated.csv\")\n\n\n\n\n\n\n\nExpand to learn how to get the actual data\n\n\n\n\n\nTo access the actual data, you would need to register for an account, log in, upload the nlsy97.NLSY97 tagset that identifies our variables, and then download. Unzip the folder and put the contents in a directory on your computer. Then run our code file prepare_nlsy97.R in that folder. This will produce a new file d.RDS, contains the data. You could analyze that file. In the interest of transparency, we wrote the code nlsy97_simulated.R to convert these real data to simulated data that we can share.\n\n\n\nThe data contain several variables\n\nid is an individual identifier for each person\na is the treatment, containing the respondent’s education coded treated if the respondent completed a four-year college degree and untreated if not.\ny is the outcome: TRUE if has a spouse or residential partner at age 35 who holds a college degree, and FALSE if no spouse or partner or if the spouse or partner at age 35 does not have a degree.\nThere are several pre-treatment variables\n\nsex is coded Female and Male\nrace is race/ethnicity and is coded Hispanic, Non-Hispanic Black, and Non-Hispanic Non-Black.\nmom_educ is the respondent’s mother’s education as reported in 1997. It takes the value No mom if the child had no residential mother in 1997, and otherwise is coded with her education: &lt; HS, High school, Some college, or College.\ndad_educ is the respondent’s father’s education as reported in 1997. It takes the value No dad if the child had no residential father in 1997, and otherwise is coded with his education: &lt; HS, High school, Some college, or College.\nlog_parent_income is the log of gross household income in 1997\nlog_parent_wealth is the log of household net worth in 1997\ntest_percentile is the respondent’s percentile score on a test of math and verbal skills administered in 1999 (the Armed Services Vocational Aptitude Battery).\n\n\nWhen values are missing, we have replcaed them with predicted values. In the simulated data, no row represents a real person because values have been drawn randomly from a probability distribution designed to mimic what exists in the real data. As discussed above, we did this in order to share the file with you by a download on this website.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Models for Causal Inference"
    ]
  },
  {
    "objectID": "topics/14_models_for_causal.html#outcome-modeling",
    "href": "topics/14_models_for_causal.html#outcome-modeling",
    "title": "Models for Causal Inference",
    "section": "Outcome modeling",
    "text": "Outcome modeling\nBecause the causal effect of A on Y is identified by adjusting for the confounders, we can estimate by outcome modeling. There are three general steps.\n\nModel \\(E(Y\\mid A, \\vec{X})\\), the conditional mean of \\(Y\\) given the treatment and confounders\nPredict potential outcomes\n\nPredict \\(Y^1\\) for every unit\nPredict \\(Y^0\\) for every unit\n\nAggregate to the average causal effect\n\n\nWith one confounder\nWe first illustrate the steps as though there were only one confounding variable: test percentile. The first step is to create a data object containing only the treated observations and a data object containing only the untreated observations.\n\nuntreated_cases &lt;- all_cases |&gt; filter(a == \"untreated\")\ntreated_cases &lt;- all_cases |&gt; filter(a == \"treated\")\n\nWe use the untreated cases to estimate a model for \\(Y^0\\) as a function of \\(X\\). If our data include sampling weights, then we weight this model by the sampling weights.\n\nmodel_for_y0 &lt;- lm(\n  y ~ test_percentile, \n  data = untreated_cases,\n  weights = sampling_weight\n)\n\nWhat happened above? The lm() function estimates a linear model, which is stored in the model object. The first argument is the model formula, which defines the function by which we model the conditional mean of the outcome given the predictors. The second argument is the data we use to learn the model.\nWe can likewise use the treated cases to estimate a model for \\(Y^1\\) as a function of \\(X\\).\n\nmodel_for_y1 &lt;- lm(\n  y ~ test_percentile, \n  data = treated_cases,\n  weights = sampling_weight\n)\n\n\n\n\n\n\n\n\n\n\nIn math, these models could be written as, \\[\n\\begin{aligned}\n\\text{E}(Y\\mid A = 0, X) &= \\alpha_0 + \\beta_0 X \\\\\n\\text{E}(Y\\mid A = 1, X) &= \\alpha_1 + \\beta_1 X\n\\end{aligned}\n\\] where \\(\\alpha_0\\) and \\(\\beta_0\\) are the intercept and slope of the line among the untreated, and \\(\\alpha_1\\) and \\(\\beta_1\\) are the intercept and slope of the line among the treated.\nWe can now use our models to predict for our target population. For the average treatment effect, the target population is all cases. For every case, we can predict probabilities under treatment and under control.\n\\[\n\\begin{aligned}\n\\hat{Y}_1 &= \\hat{\\text{E}}(Y\\mid A = 1, X) = \\hat\\alpha_1 + \\hat\\beta_1 X \\\\\n\\hat{Y}_0 &= \\hat{\\text{E}}(Y\\mid A = 0, X) = \\hat\\alpha_0 + \\hat\\beta_0 X \\\\\n\\end{aligned}\n\\]\nIn code, we make those predictions with the predict() function, storing them in new variables yhat1 and yhat0.\n\npredicted_potential_outcomes &lt;- all_cases |&gt;\n  mutate(\n    yhat1 = predict(model_for_y1, newdata = all_cases),\n    yhat0 = predict(model_for_y0, newdata = all_cases),\n    effect = yhat1 - yhat0\n  )\n\n\n\n# A tibble: 7,688 × 6\n     id sampling_weight a         yhat1  yhat0 effect\n  &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1           0.989 untreated 0.403 0.171   0.232\n2     2           0.999 treated   0.645 0.317   0.328\n3     3           0.967 untreated 0.283 0.0990  0.184\n# ℹ 7,685 more rows\n\n\nVisually, the target population is all the gray points: everyone regardless of treatment. For each point, we predict the outcome probability under treatment and under no treatment.\n\n\n\n\n\n\n\n\n\nThe average treatment effect (ATE) is the weighted average of the case-specific effect estimates, weighted by sampling weights.\n\npredicted_potential_outcomes |&gt;\n  summarize(ate = weighted.mean(effect, w = sampling_weight))\n\n# A tibble: 1 × 1\n    ate\n  &lt;dbl&gt;\n1 0.224\n\n\nWe could also estimate among any subgroup, for example the average treatment effect among the treated and among the untreated.\n\npredicted_potential_outcomes |&gt;\n  group_by(a) |&gt;\n  summarize(conditional_average_effect = weighted.mean(effect, w = sampling_weight))\n\n# A tibble: 2 × 2\n  a         conditional_average_effect\n  &lt;chr&gt;                          &lt;dbl&gt;\n1 treated                        0.278\n2 untreated                      0.211\n\n\n\n\nWith many confounders\nOutcome modeling generalizes easily from one confounder to many confounders. The only change is to include more confounders in the outcome model formulas.\n\nmodel_for_y0 &lt;- lm(\n  y ~ sex + race + mom_educ + dad_educ + log_parent_income +\n    log_parent_wealth + test_percentile, \n  data = untreated_cases,\n  weights = sampling_weight\n)\nmodel_for_y1 &lt;- lm(\n  y ~ sex + race + mom_educ + dad_educ + log_parent_income +\n    log_parent_wealth + test_percentile, \n  data = treated_cases,\n  weights = sampling_weight\n)\n\nOtherwise, outcome modeling with many confounders follows the same process. If our goal is to estimate the average treatment effect in all_cases,\n\npredicted_potential_outcomes &lt;- all_cases |&gt;\n  mutate(\n    yhat1 = predict(model_for_y1, newdata = all_cases),\n    yhat0 = predict(model_for_y0, newdata = all_cases),\n    effect = yhat1 - yhat0\n  ) |&gt;\n  select(id, sampling_weight, a, yhat1, yhat0, effect) |&gt;\n  print(n = 3)\n\n# A tibble: 7,688 × 6\n     id sampling_weight a         yhat1   yhat0 effect\n  &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1     1           0.989 untreated 0.255  0.0889  0.166\n2     2           0.999 treated   0.727  0.441   0.285\n3     3           0.967 untreated 0.149 -0.0139  0.163\n# ℹ 7,685 more rows\n\n\nThe average treatment effect (ATE) is the weighted average of the case-specific effect estimates, weighted by sampling weights.\n\npredicted_potential_outcomes |&gt;\n  summarize(ate = weighted.mean(effect, w = sampling_weight))\n\n# A tibble: 1 × 1\n    ate\n  &lt;dbl&gt;\n1 0.199\n\n\nWe could also estimate among any subgroup, for example the average treatment effect among the treated and among the untreated.\n\npredicted_potential_outcomes |&gt;\n  group_by(a) |&gt;\n  summarize(conditional_average_effect = weighted.mean(effect, w = sampling_weight))\n\n# A tibble: 2 × 2\n  a         conditional_average_effect\n  &lt;chr&gt;                          &lt;dbl&gt;\n1 treated                        0.253\n2 untreated                      0.187\n\n\n\n\nGeneralizing to logistic regression\nIn the illustration above, we might be concerned that our outcome is binary (taking values 0 or 1) and yet the predictions are sometimes below 0 or above 1.\n\n\n\n\n\n\n\n\n\nBoth models are predicting that some people have negative probabilities of having a college-degree-holding spouse or partner! We might want to solve this by estimating logistic regression models. We do this with the glm() function with the argument family = binomial.\n\nIf logistic regression is new to you, see the bottom of What is a model?.\n\n\nlogistic_model_for_y0 &lt;- glm(\n  y ~ sex + race + mom_educ + dad_educ + log_parent_income +\n    log_parent_wealth + test_percentile, \n  family = binomial,\n  data = untreated_cases,\n  weights = sampling_weight\n)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nlogistic_model_for_y1 &lt;- glm(\n  y ~ sex + race + mom_educ + dad_educ + log_parent_income +\n    log_parent_wealth + test_percentile, \n  family = binomial,\n  data = treated_cases,\n  weights = sampling_weight\n)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nThese models return a warning that there is a non-integer number of successes. This is normal and not a concern when estimating logistic regression models with weights.\nJust as with linear regression, we can use our logistic regression to predict potential outcome values. When making predictions, it is important to use the type = \"response\" argument to predict the probability of \\(Y = 1\\) instead of the log odds of \\(Y = 1\\).\n\nlogistic_predicted_potential_outcomes &lt;- all_cases |&gt;\n  mutate(\n    yhat1 = predict(\n      logistic_model_for_y1, \n      newdata = all_cases, \n      type = \"response\"\n    ),\n    yhat0 = predict(\n      logistic_model_for_y0, \n      newdata = all_cases, \n      type = \"response\"\n    ),\n    effect = yhat1 - yhat0\n  )\n\n\n\n# A tibble: 7,688 × 6\n     id sampling_weight a         yhat1  yhat0 effect\n  &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1           0.989 untreated 0.254 0.0861  0.168\n2     2           0.999 treated   0.726 0.562   0.164\n3     3           0.967 untreated 0.177 0.0261  0.151\n# ℹ 7,685 more rows\n\n\nIn math, we can see why the type = \"response\" is needed. By default, using predict() after logistic regression will predict the log odds of the outcome. For example for the outcome under treatment:\n\\[\n\\log\\left(\\frac{\\hat{\\text{P}}(Y = 1\\mid A = 1, X)}{1 - \\hat{\\text{P}}(Y = 1\\mid A = 1, X)}\\right) = \\hat\\alpha_1 + \\hat\\beta_1 X\n\\]\nBut we really estimated the model to estimate \\(\\hat{P}(Y = 1\\mid A = 1, X)\\), not a complicated function of that quantity. By typing type = \"response\", you tell R to invert the logit function and return predicted probabilities.\n\\[\n\\hat{\\text{P}}(Y = 1\\mid A = 1, X) = \\frac{e^{\\hat\\alpha_1 + \\hat\\beta_1 X}}{1 + e^{\\hat\\alpha_1 + \\hat\\beta_1 X}}\n\\]\nBy using type = \"response\", you can think about the probabilities on the left side of the equation and not the math on the right side of the equation. We can visualize that with logistic regression, all predicted probabilities fall within the [0,1] range.\n\n\n\n\n\n\n\n\n\nExactly as with linear regression, we can aggregate the predicted potential outcomes to estimate the average treatment effect over all cases (ATT),\n\nlogistic_ate_estimate &lt;- logistic_predicted_potential_outcomes |&gt;\n  summarize(ate = weighted.mean(effect, w = sampling_weight)) |&gt;\n  print()\n\n# A tibble: 1 × 1\n    ate\n  &lt;dbl&gt;\n1 0.204\n\n\nor among those who were factually treated or untreated,\n\nlogistic_predicted_potential_outcomes |&gt;\n  group_by(a) |&gt;\n  summarize(conditional_average_effect = weighted.mean(effect, w = sampling_weight))\n\n# A tibble: 2 × 2\n  a         conditional_average_effect\n  &lt;chr&gt;                          &lt;dbl&gt;\n1 treated                        0.240\n2 untreated                      0.195\n\n\nor among any subpopulation by grouping by any confounding variables.\nWe estimate that completing college increases the probability of having a college-educated by 0.204. This causal conclusion relies both on our causal assumptions (the DAG) and our statistical assumptions (the chosen model).",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Models for Causal Inference"
    ]
  },
  {
    "objectID": "topics/14_models_for_causal.html#outcome-modeling-with-a-continuous-treatment-variable",
    "href": "topics/14_models_for_causal.html#outcome-modeling-with-a-continuous-treatment-variable",
    "title": "Models for Causal Inference",
    "section": "Outcome modeling with a continuous treatment variable",
    "text": "Outcome modeling with a continuous treatment variable\nMost examples previously have focused on binary treatment variables (e.g., complete college versus not). A treatment variable can also be continuous (e.g., family income at age 17). This class will generalize our logic from binary to continuous treatments and discuss one approach to study continuous treatments: the average effect of an additive shift to the factual treatment values. The example for this part comes from Lundberg and Brand (in progress).",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Models for Causal Inference"
    ]
  },
  {
    "objectID": "topics/14_models_for_causal.html#treatment-modeling",
    "href": "topics/14_models_for_causal.html#treatment-modeling",
    "title": "Models for Causal Inference",
    "section": "Treatment modeling",
    "text": "Treatment modeling\n\nWhile this is included for reference, we plan to skip this topic in class.\n\nInstead of modeling the outcome, another way of using models for causal inference is to model the probability of treatment assignment. This approach is more analogous to sampling from a population.\nIn a probability sample, we observe the outcome \\(Y_i\\) for any sampled unit \\((S_i=1)\\) which is seen with some probability of sampling, \\(P(S=1\\mid\\vec{X} = \\vec{x}_i)\\) that may differ across subgroups with different values of some variables \\(\\vec{X}\\). As discussed in population sampling, the sampling weight is the inverse of these probabilities. A person who is sampled with a 20% probability represents 1 / .2 = 5 people in the population (the other 4 being unsampled).\nIn a conditionally randomized experiment, we observe the outcome under treatment \\(Y_i^1\\) for any treated unit \\(A_i=1\\), which might be assigned with some probability \\(P(A_i=1\\mid\\vec{X} = \\vec{x}_i)\\) that differs across subgroups defined by an adjustment set \\(\\vec{X}\\). In a conditionally randomized experiment, these probabilities are known and the overall expected outcome under treatment \\(\\text{E}(Y^1)\\) can be estimated by the average of the observed outcomes under treatment, weighted by the inverse probability of being treated. A treated unit who had a 20% probability of being treated represents 1 / .2 = 5 people (the other 4 being untreated).\nIn an observational study, we don’t know the probability of being treated given the variables in our sufficient adjustment set. We need to model that probability. There are three general steps.\n\nModel treatment probabilities given an adjustment set\nConstruct a weight for each unit\nEstimate by weighted means within each treatment group\n\n\n1) Model treatment probabilities\nOne way to model the probability of treatment is with logistic regression. If logistic regression is new to you, see the bottom of What is a model?.\n\\[\n\\log\\left(\\frac{P(A = 1 \\mid\\vec{X})}{1-P(A = 1\\mid\\vec{X})}\\right) = \\alpha + \\vec{X}'\\vec\\beta\n\\]\n\ntreatment_model &lt;- glm(\n  I(a == \"treated\") ~ sex + race + mom_educ + dad_educ + log_parent_income +\n    log_parent_wealth + test_percentile,\n  family = binomial,\n  data = all_cases\n)\n\nFor every unit, we can then predict the probability of being treated given the adjustment set.\n\npredicted_treatment_probabilities &lt;- all_cases |&gt;\n  mutate(p_treated = predict(treatment_model, type = \"response\")) |&gt;\n  select(id, a, y, p_treated)\n\n\n\n# A tibble: 7,688 × 4\n     id a         y     p_treated\n  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;     &lt;dbl&gt;\n1     1 untreated FALSE    0.0720\n2     2 treated   TRUE     0.777 \n3     3 untreated FALSE    0.0318\n# ℹ 7,685 more rows\n\n\nThe type = \"response\" argument is essential, because this tells R to predict the probability of treatment instead of the log odds of treatment.\n\n\n2) Construct weights\nFor each unit, we can construct a weight that is the inverse probability of that unit’s treatment assignment. Recall that if a unit is treated and had a 0.2 probability of treatment, then we could think of this unit as representing 1 / 0.2 = 5 units: itself and 4 others like it who were not treated. The weight on each unit is the inverse probability of the treatment value that happened for that unit.\n\\[\nw_i = \\begin{cases}\n\\frac{1}{\\text{P}(A = 1\\mid \\vec{X} = \\vec{x}_i)} &\\text{if treated} \\\\\n\\frac{1}{1 - \\text{P}(A = 1\\mid \\vec{X} = \\vec{x}_i)} &\\text{if untreated}\n\\end{cases}\n\\]\nIn code, we can use case_when() to assign this weight as 1 / p_treated for treated units and 1 / (1 - p_treated) for untreated units.\n\ninverse_probability_weights &lt;- predicted_treatment_probabilities |&gt;\n  mutate(\n    weight = case_when(\n      a == \"treated\" ~ 1 / p_treated,\n      a == \"untreated\" ~ 1 / (1 - p_treated)\n    )\n  )\n\n\n\n# A tibble: 7,688 × 5\n     id a         y     p_treated weight\n  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     1 untreated FALSE    0.0720   1.08\n2     2 treated   TRUE     0.777    1.29\n3     3 untreated FALSE    0.0318   1.03\n# ℹ 7,685 more rows\n\n\n\n\n3) Estimate by weighted means\nFinally, we use the weights to take the treated units and draw inference about what would happen to all units if they were hypothetically treated, and to use the untreated units and draw inference about what would happen to all units if they were hypothetically untreated.\n\ninverse_probability_weights |&gt;\n  # Within each treatment group\n  group_by(a) |&gt;\n  # Take the mean weighted by inverse probability of treatment weights\n  summarize(estimate = weighted.mean(y, w = weight)) |&gt;\n  # Pivot wider and difference to estimate the effect\n  pivot_wider(names_from = a, values_from = estimate, names_prefix = \"if_\") |&gt;\n  mutate(effect = if_treated - if_untreated)\n\n# A tibble: 1 × 3\n  if_treated if_untreated effect\n       &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1      0.382        0.166  0.217",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Models for Causal Inference"
    ]
  },
  {
    "objectID": "topics/14_models_for_causal.html#why-each-strategy",
    "href": "topics/14_models_for_causal.html#why-each-strategy",
    "title": "Models for Causal Inference",
    "section": "Why each strategy?",
    "text": "Why each strategy?\nWhy would we choose outcome modeling or treatment modeling?\nIn favor of outcome modeling, this is how many social scientists have thought about modeling social processes: build a regression model to predict outcomes. In favor of treatment modeling, this connects directly with how many researchers are trained to draw inferences about a population from a sample selected with unequal probabilities.\nOne argument in favor of treatment modeling is that it makes more transparent which units get a very large amount of weight. To illustrate this, the figure below shows a simulation in a very dystopian world. In this world, 99% of the children of college graduates complete college compared with only 1% of the children of non-graduates. We could fit an outcome model to predict each child’s future income under college (\\(Y^1\\)) and under no college (\\(Y^0\\)). Alternatively, we could estimate by inverse probability of treatment weights. An advantage of the treatment weighting is that we can see that two observations are very influential: the first-generation college graduate has a very unlikely treatment status, and counts for 100 people! Likewise the child of college graduates who did not make it through college has a weight of 100.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Models for Causal Inference"
    ]
  },
  {
    "objectID": "topics/14_models_for_causal.html#concluding-thoughts",
    "href": "topics/14_models_for_causal.html#concluding-thoughts",
    "title": "Models for Causal Inference",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\nOutcome modeling is a powerful strategy because it bridges nonparametric causal identification to longstanding strategies where outcomes are modeled by parametric regression.\nInverse probability of treatment weighting is a powerful strategy because it bridges nonparametric causal identification to longstanding strategies from survey sampling where units from a population are sampled with known probabilities of inclusion. The analogy is that outcomes under treatment are sampled with estimated inclusion probabilities (the probability of treatment). Just as in a population sample we would need to think carefully about the probability of sampling, treatment modeling encourages us to model the probability of receiving the observed treatment.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Models for Causal Inference"
    ]
  },
  {
    "objectID": "topics/14_models_for_causal.html#footnotes",
    "href": "topics/14_models_for_causal.html#footnotes",
    "title": "Models for Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor reviews, see Mare 1991 and Schwartz 2013.↩︎",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Models for Causal Inference"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html",
    "href": "topics/12_DAGs.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "Topic for 2/18. Here are slides.\nDirected Acyclic Graphs (DAGs) formalize causal assumptions mathematically in graphs. One way DAGs are useful in observational studies is by helping us to identify a sufficient adjustment set (\\(\\vec{X}\\)) such that conditional exchangeability holds.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html#nodes-edges-and-paths",
    "href": "topics/12_DAGs.html#nodes-edges-and-paths",
    "title": "Directed Acyclic Graphs",
    "section": "Nodes, edges, and paths",
    "text": "Nodes, edges, and paths\nThe previous page introduced a conditionally randomized experiment in which the researcher assigned participants to the treatment condition of (four-year college degree) vs (high school degree) with probabilities that depended on high school class rank. In this experiment, being in the top 25% of one’s high school class caused a higher chance of receiving the treatment. We will also assume that both high school performance and college completion may causally shape employment at age 40.\nWe can formalize these ideas in a graph where each node (a letter) is a variable and each edge (\\(\\rightarrow\\)) is a causal relationship.\n\n\n\n\n\n\n\n\n\nBetween each pair of nodes, you can enumerate every path or sequence of edges connecting the nodes.\n\nPath. A path between nodes \\(A\\) and \\(B\\) is any set of edges that starts at \\(A\\) and ends at \\(B\\). Paths can involve arrows in either direction.\n\nIn our DAG above, there are two paths between \\(A\\) and \\(Y\\).\n\n\\(A\\rightarrow Y\\)\n\\(A\\leftarrow X \\rightarrow Y\\)\n\nWe use paths to determine the reasons why \\(A\\) and \\(Y\\) might be statistically dependent or independent.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html#causal-paths",
    "href": "topics/12_DAGs.html#causal-paths",
    "title": "Directed Acyclic Graphs",
    "section": "Causal paths",
    "text": "Causal paths\nThe first type of path that we consider is a causal path.\n\nCausal path. A path in which all arrows point in the same direction. For example, \\(A\\) and \\(D\\) could be connected by a causal path \\(A\\rightarrow B \\rightarrow C \\rightarrow D\\).\n\nA causal path creates statistical depends between the nodes because the first node causes the last node, possibly through a sequence of other nodes.\nIn our example, there is a causal path \\(A\\rightarrow Y\\): being assigned to a four-year college degree affects employment at age 40. Because of this causal path, people who are assigned to a four-year degree have different rates of employment at age 40 than those who are not.\nA causal path can go through several variables. For example, if we listed the paths between \\(X\\) and \\(Y\\) we would include the path \\(X\\rightarrow A \\rightarrow Y\\). This is a causal path because being in the top 25% of one’s high school class increases the probability of assignment to a four-year degree in our experiment, which in turn increases the probability of employment at age 40.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html#fork-structures",
    "href": "topics/12_DAGs.html#fork-structures",
    "title": "Directed Acyclic Graphs",
    "section": "Fork structures",
    "text": "Fork structures\nTo reason about non-causal paths, we have to think about several structures that can exist along these paths. The first such structure is a fork structure.\n\nFork structure. A fork structure is a sequence of edges in which two variables (\\(A\\) and \\(B\\)) are both caused a third variable (\\(C\\)): \\(A\\leftarrow C \\rightarrow B\\).\n\nIn our example, the path \\(A\\leftarrow X \\rightarrow Y\\) involves a fork structure. being in the top 25% of one’s high school class causally affects both the treatment (college degree) and the outcome (employment at age 40).\nA fork structure creates statistical dependence between \\(A\\) and \\(Y\\) that does not correspond to a causal effect of \\(A\\) on \\(Y\\). In our example, people who are assigned to the treatment value (college degree) are more likely to have been in the top 25% of their high school class, since this high class rank affected treatment assignment in our experiment. A high class rank also affected employment at age 40. Thus, in our experiment the treatment would be associated with the outcome even if finishing college had no causal effect on employment.\nFork structures can be blocked by conditioning on the common cause. In our example, suppose we filter our data to only include those in the top 25% of their high school class. We sometimes use a box to denote conditioning on a variable, (\\(A\\leftarrow\\boxed{X}\\rightarrow Y\\)). Conditioning on \\(X\\) blocks the path because within this subgroup \\(X\\) does not vary, so it cannot cause the values of \\(A\\) and \\(Y\\) within the subgroup. In our example, if we looked among those in the top 25% of their high school classes the only reason college enrollment would be related to employment at age 40 would be the causal effect \\(A\\rightarrow Y\\).\nTo emphasize ideas, it is also helpful to consider a fork structure in an example where the variables have no causal relationship.\nSuppose a beach records for each day the number of ice cream cones sold and the number of rescues by lifeguards. There is no causal effect between these two variables; eating ice cream does not cause more lifeguard rescues and vice versa. But the two are correlated because they share a common cause: warm temperatures cause high ice cream sales and also high lifeguard rescues. A fork structure formalizes this notion: \\((\\text{ice cream sales}) \\leftarrow (\\text{warm temperature}) \\rightarrow (\\text{lifeguard rescues})\\).\nIn the population of days as a whole, this fork structure means that ice cream sales are related to lifeguard rescues. But if we condition on having a warm temperature by filtering to days when the temperature took a particular value, ice cream sales would be unrelated to lifeguard rescues across those days. This is the sense in which conditioning on the common cause variable blocks the statistical associations that would otherwise arise from a fork structure.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html#collider-structures",
    "href": "topics/12_DAGs.html#collider-structures",
    "title": "Directed Acyclic Graphs",
    "section": "Collider structures",
    "text": "Collider structures\nIn contrast to a fork structure where one variable affects two others (\\(\\bullet\\leftarrow\\bullet\\rightarrow\\bullet\\)), a collider structure is a structure where one variable is affected by two others.\n\nCollider structure. A collider structure is a sequence of edges in which two variables (\\(A\\) and \\(B\\)) both cause a third variable (\\(C\\)). We say that \\(C\\) is a collider on the path \\(A\\rightarrow C \\leftarrow B\\).\n\nFork and collider structures have very different properties, as we will illustrate through an example.\nSuppose that every day I observe whether the grass on my lawn is wet. I have sprinklers that turn on with a timer at the same time every day, regardless of the weather. It also sometimes rains. When the grass is wet, it is wet because either the sprinklers have been on or it has been raining.\n\\[\n(\\text{sprinklers on}) \\rightarrow (\\text{grass wet}) \\leftarrow (\\text{raining})\n\\]\nIf I look across all days, the variable (sprinklers on) is unrelated to the variable (raining). After all, the sprinklers are just on a timer! Formally, we say that even though (sprinklers on) and (raining) are connected by the path above, this path is blocked by the collider structure. A path does not create dependence between two variables when it contains a collider structure.\nIf I look only at the days when the grass is wet, a different pattern emerges. If the grass is wet and the sprinklers have not been on, then it must have been raining: the grass had to get wet somehow. If the grass is wet and it has not been raining, then the sprinklers must have been on. Once I look at days when the grass is wet (or condition on the grass being wet), the two input variables become statistically associated.\nA collider blocks a path when that collider is left unadjusted, but conditioning on the collider variable opens the path containing the collider.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html#open-and-blocked-paths",
    "href": "topics/12_DAGs.html#open-and-blocked-paths",
    "title": "Directed Acyclic Graphs",
    "section": "Open and blocked paths",
    "text": "Open and blocked paths\nA central purpose of a DAG is to connect causal assumptions to implications about associations that should be present (or absent) in data under those causal assumptions. To make this connection, we need a final concept of open and blocked paths.\n\nA path is blocked if it contains an unconditioned collider or a conditioned non-collider. Otherwise, the path is open. An open path creates statistical dependence between its terminal nodes whereas a blocked path does not.\n\nAs examples for a path with no colliders,\n\n\\(A\\leftarrow B \\rightarrow C \\rightarrow D\\) is an open path because no variables are conditioned and it contains no colliders.\n\\(A\\leftarrow \\boxed{B}\\rightarrow C \\rightarrow D\\) is a blocked path because we have conditioned on the non-collider \\(B\\).\n\\(A\\leftarrow  B\\rightarrow \\boxed{C} \\rightarrow D\\) is a blocked path because we have conditioned on the non-collider \\(C\\).",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html#determining-statistical-dependence",
    "href": "topics/12_DAGs.html#determining-statistical-dependence",
    "title": "Directed Acyclic Graphs",
    "section": "Determining statistical dependence",
    "text": "Determining statistical dependence\nWe are now ready to use DAGs to determine if \\(A\\) and \\(B\\) are statistically dependent. The process involves three steps.\n\nList all paths between \\(A\\) and \\(B\\).\nCross out any paths that are blocked.\nThe causal assumptions imply that \\(A\\) and \\(B\\) may be statistically dependent only if any open paths remain.\n\nAs an example, below we consider a hypothetical DAG.\n\n\n\n\n\n\n\n\n\n1. Marginal dependence\nMarginally without any adjustment, are \\(A\\) and \\(D\\) statistically dependent? We first write out all paths connecting \\(A\\) and \\(D\\).\n\n\\(A\\rightarrow B \\leftarrow C\\rightarrow D\\)\n\\(A\\leftarrow X\\rightarrow D\\)\n\nWe then cross out the paths that are blocked\n\n\\(\\cancel{A\\rightarrow B \\leftarrow C\\rightarrow D}\\) (blocked by unconditioned collider \\(B\\))\n\\(A\\leftarrow X\\rightarrow D\\)\n\nBecause an open path remains, \\(A\\) and \\(D\\) are statistically dependent.\n2. Dependence conditional on \\(X\\)\nIf we condition on \\(X\\), are \\(A\\) and \\(D\\) statistically dependent? We first write out all paths connecting \\(A\\) and \\(D\\).\n\n\\(A\\rightarrow B \\leftarrow C\\rightarrow D\\)\n\\(A\\leftarrow \\boxed{X}\\rightarrow D\\)\n\nWe then cross out the paths that are blocked\n\n\\(\\cancel{A\\rightarrow B \\leftarrow C\\rightarrow D}\\) (blocked by unconditioned collider \\(B\\))\n\\(\\cancel{A\\leftarrow \\boxed{X}\\rightarrow D}\\) (blocked by conditioned non-collider \\(X\\))\n\nBecause no open path remains, \\(A\\) and \\(D\\) are statistically independent.\n3. Dependence conditional on \\(\\{X,B\\}\\)\nIf we condition on \\(X\\) and \\(B\\), are \\(A\\) and \\(D\\) statistically dependent? We first write out all paths connecting \\(A\\) and \\(D\\).\n\n\\(A\\rightarrow B \\leftarrow C\\rightarrow D\\)\n\\(A\\leftarrow \\boxed{X}\\rightarrow D\\)\n\nWe then cross out the paths that are blocked\n\n\\(A\\rightarrow \\boxed{B} \\leftarrow C\\rightarrow D\\) (open since collider \\(B\\) is conditioned)\n\\(\\cancel{A\\leftarrow \\boxed{X}\\rightarrow D}\\) (blocked by conditioned non-collider \\(X\\))\n\nBecause an open path remains, \\(A\\) and \\(D\\) are statistically dependent.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html#causal-identification-with-dags",
    "href": "topics/12_DAGs.html#causal-identification-with-dags",
    "title": "Directed Acyclic Graphs",
    "section": "Causal identification with DAGs",
    "text": "Causal identification with DAGs\nWhen our aim is to identify the average causal effect of \\(A\\) on \\(Y\\), we want to choose a set of variables for adjustment so that all remaining paths are causal paths. We call this a sufficient adjustment set.\n\nA sufficient adjustment set for the causal effect of \\(A\\) on \\(Y\\) is a set of nodes that, when conditioned, block all non-causal paths between \\(A\\) and \\(Y\\).\n\nIn our example from the top of this page, there were two paths between \\(A\\) and \\(Y\\):\n\n\\((A\\text{: college degree})\\rightarrow (Y\\text{: employed at age 40})\\)\n\\((A\\text{: college degree})\\leftarrow (X\\text{: top 25\\% of high school class})\\rightarrow (Y\\text{: employed at age 40})\\)\n\nIn this example, \\(X\\) is a sufficient adjustment set. Once we condition on \\(X\\) by e.g. filtering to those in the top 25% of their high school class, the only remaining path between \\(A\\) and \\(Y\\) is the causal path \\(A\\rightarrow Y\\). Thus, the difference in means in \\(Y\\) across \\(A\\) within subgroups defined by \\(X\\) identifies the conditional average causal effect of \\(A\\) on \\(Y\\).\nA more difficult example.\nSufficient adjustment sets can be much more complicated. As an example, consider the DAG below.\n\n\n\n\n\n\n\n\n\nWe first list all paths between \\(A\\) and \\(Y\\).\n\n\\(A\\rightarrow Y\\)\n\\(A\\rightarrow M\\rightarrow Y\\)\n\\(A\\leftarrow X_1\\rightarrow X_3 \\rightarrow Y\\)\n\\(A\\leftarrow X_1\\rightarrow X_3 \\leftarrow X_2\\rightarrow Y\\)\n\\(A\\leftarrow X_3 \\rightarrow Y\\)\n\\(A\\leftarrow X_3\\leftarrow X_2 \\rightarrow Y\\)\n\nThe first two paths are causal, and the others are non-causal. We want to find a sufficient adjustment set to block all the non-causal paths.\nIn order to block paths (3), (5), and (6) we might condiiton on \\(X_3\\). But doing so opens path (2), which was otherwise blocked by the collider \\(X_3\\). In order to also block path (2), we might additionally condition on \\(X_1\\). In this case, our sufficient adjustment set is \\(\\{X_1,X_3\\}\\).\n\n\\(A\\rightarrow Y\\)\n\\(A\\rightarrow M\\rightarrow Y\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_1}\\rightarrow \\boxed{X_3} \\rightarrow Y}\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_1}\\rightarrow \\boxed{X_3} \\leftarrow X_2\\rightarrow Y}\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_3} \\rightarrow Y}\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_3}\\leftarrow X_2 \\rightarrow Y}\\)\n\nThen the only open paths are paths (1) and (2), both of which are causal paths from \\(A\\) to \\(Y\\).\nSometimes there are several sufficient adjustment sets. In this example, sufficient adjustment sets include:\n\n\\(\\{X_1,X_3\\}\\)\n\\(\\{X_2,X_3\\}\\)\n\\(\\{X_1,X_2,X_3\\}\\)\n\nWe sometimes call the first two minimal sufficient adjustment sets because they are the smallest.\n\nA minimal sufficient adjustment set is an adjustment set that achieves causal identification by conditioning on the fewest number of variables possible.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html#how-to-draw-a-dag",
    "href": "topics/12_DAGs.html#how-to-draw-a-dag",
    "title": "Directed Acyclic Graphs",
    "section": "How to draw a DAG",
    "text": "How to draw a DAG\nSo far, we have focused on causal identification with a DAG that has been given. But how do you draw one for yourself?\nWhen drawing a DAG, there is an important rule: any node that would have edges pointing into any two nodes already represented in the DAG must be included. This is because what you omit from the DAG is far more important than what you include in the DAG. Many of your primary assumptions are about the nodes and edges that you leave out.\nFor example, suppose we are estimating the causal effect of a college degree on employment at age 40. After beginning our DAG with only these variables, we have to think about any other variables that might affect these two. High school performance is one example. Then we have to include any nodes that affect any two of {high school performance, college degree, employment at age 40}. Perhaps a person’s parents’ education affects that person’s high school performance and college degree attainment. Then parents’ education should be included as an additional node. The cycle continues, so that in observational causal inference settings you are likely to have a DAG with many nodes.\nIn practice, you may not have data on all the nodes that comprise the sufficient adjustment set in your graph. In this case, we recommend that you first draw a graph under which you can form a sufficient adjustment set with the measured variables. This allows you to state one set of causal beliefs under which your analysis can answer your causal question. Then, also draw a second DAG that includes the other variables you think are relevant. This will enable you to reason about the sense in which your results could be misleading because of omitting important variables.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/12_DAGs.html#closing-thoughts",
    "href": "topics/12_DAGs.html#closing-thoughts",
    "title": "Directed Acyclic Graphs",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nDAGs are a powerful tool for causal inference, because they are both visually intuitive and mathematically precise. They translate our theories about the world into a formal language with implications for causal identification.\nIf you’d like to learn more about DAGs, here are a few good resources:\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. New York: Basic Books.\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. New York: Wiley.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/10_define_causal.html",
    "href": "topics/10_define_causal.html",
    "title": "Defining Causal Effects",
    "section": "",
    "text": "Topic for 2/9.\nslides\nThe course so far has focused on descriptive claims. For example, we have estimated the average value of an outcome in the population, or among population subgroups. This lecture pivots to causal claims: what would happen if a population were exposed to a hypothetical intervention.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Defining Causal Effects"
    ]
  },
  {
    "objectID": "topics/10_define_causal.html#fundamental-problem-of-causal-inference",
    "href": "topics/10_define_causal.html#fundamental-problem-of-causal-inference",
    "title": "Defining Causal Effects",
    "section": "Fundamental problem of causal inference",
    "text": "Fundamental problem of causal inference\nHealth professionals often advise people to eat a Mediterranean diet high in healthy fats such as olive oil, whole grains, and fruits. There is descriptive evidence that lifespans are longer among people who eat a Mediterranean diet compared with among people who eat a standard diet. But does eating a Mediterranean diet cause longer lifespan? The figure below visualizes this question in the potential outcomes framework.\n\nIn this hypothetical example, each row corresponds to a person. Person 1 follows a Mediterranean diet and is observed to have a lifespan indicated in blue. Person 2 does not follow a Mediterranean diet and is observed to have a lifespan indicated in green. The descriptive evidence is that lifespans are longer among those eating a Mediterranean diet (blue outcomes on the left) compared with those eating standard diets (green outcomes on the left).\nThe right side of the figure corresponds to the causal claim, which is different. Person 1 has two potential outcomes: a lifespan that would be realized under a Mediterranean diet and a lifespan that would be realized under a standard diet. The causal effect for Person 1 is the difference between the lifespans that would be realized for that person under each of the two diets. But there is a fundamental problem: person 1 ate a Mediterranean diet, and we did not get to observe their outcome under a standard diet. The fundamental problem of causal inference (Holland 1986) is that causal claims involve a contrast between potential outcomes, but for each unit only one of these potential outcomes is realized. The other is counterfactual and cannot be directly observed.\nWe will need additional argument and assumptions to use the factual data (left side of the figure) in order to produce answers about causal effects (right side of the figure). Causal inference is a missing data problem insofar as many of the potential outcomes we need are missing.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Defining Causal Effects"
    ]
  },
  {
    "objectID": "topics/10_define_causal.html#mathematical-notation",
    "href": "topics/10_define_causal.html#mathematical-notation",
    "title": "Defining Causal Effects",
    "section": "Mathematical notation",
    "text": "Mathematical notation\nBecause each person has more than one potential outcome, we need new mathematical notation to formalize causal claims. We will use subscripts to indicate units (rows of our data). Let \\(Y_i\\) be the outcome for person \\(i\\), such as whether person \\(i\\) survived. Let \\(A_i\\) be the treatment of person \\(i\\), for example taking the value or the value . To refer more abstractly to a value the treatment could take, we use the lower case notation \\(a\\) for a treatment value. Define potential outcomes \\(Y_i^\\text{MediterraneanDiet}\\) and \\(Y_i^\\text{StandardDiet}\\) as the lifespan outcomes that person \\(i\\) would realize under each of the treatment conditions. More generally, let \\(Y_i^a\\) denote the potential outcome for unit \\(i\\) that would be realized if assigned to treatment value \\(a\\).\nThe causal effect is a contrast across potential outcomes. For example, the causal effect on Ian’s lifespan of eating a Mediterranean diet versus a standard diet is \\[Y_\\text{Ian}^\\text{MediterraneanDiet} - Y_\\text{Ian}^\\text{StandardDiet}\\]\nTo connect causal claims to ideas we have already covered from sampling, we will adopt a framework in which potential outcomes are fixed quantities with randomness arising from sampling and/or from random treatment assignment. Each person has a fixed outcome \\(Y_i^\\text{MediterraneanDiet}\\) that would be observed if they were sampled and assigned a Mediterranean diet. This is just like how every baseball player from last week had a salary that would be observed if they were sampled. We will sometimes omit the \\(i\\) subscript to refer to the random variable for the potential outcome of a randomly-sampled person from the population, \\(Y^\\text{MediterraneanDiet}\\).",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Defining Causal Effects"
    ]
  },
  {
    "objectID": "topics/10_define_causal.html#the-consistency-assumption",
    "href": "topics/10_define_causal.html#the-consistency-assumption",
    "title": "Defining Causal Effects",
    "section": "The consistency assumption",
    "text": "The consistency assumption\nWe want to make causal claims about potential outcomes \\(Y_i^a\\), but what we observe are factually realized outcome \\(Y_i\\). To draw the connection, we need to assume that the factual outcomes are consistent with what would be observed if the person in question were assigned to the treatment condition that factually observed. Using \\(A_i\\) to denote the factual treatment for person \\(i\\), we assume\n\\[\nY_i^{A_i} = Y_i \\qquad\\text{(consistency assumption)}\n\\] This assumption is often obviously true, but we will see later examples where it is violated. One example is when person \\(i\\)’s outcome depends not only on their own treatment but also on the treatment of some neighboring person \\(j\\). In many of our initial examples, we will simply assume the consistency assumption holds.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Defining Causal Effects"
    ]
  },
  {
    "objectID": "topics/10_define_causal.html#potential-outcomes-in-math-and-in-words",
    "href": "topics/10_define_causal.html#potential-outcomes-in-math-and-in-words",
    "title": "Defining Causal Effects",
    "section": "Potential outcomes in math and in words",
    "text": "Potential outcomes in math and in words\nWe will often use potential outcomes within mathematical statements. For example, we might write about the expected outcome if assigned to a Mediterranean diet, \\(E(Y^\\text{MediterraneanDiet})\\). Recall that the expectation operator \\(E()\\) says to take the population mean of the random variable within the parentheses. We will also use conditional expectations, such as \\(E(Y\\mid A = \\text{MediterraneanDiet})\\) which reads “the expected value of \\(Y\\) given that \\(A\\) took the value .” The vertical bar says that we are taking the expected value of the variable on the left of the bar within the subgroup defined on the right side of the bar.\nIn class, we practiced writing statements in math and in English. For example, the mathematical statement \\[\nE(\\text{Earning} \\mid \\text{CollegeDegree} = \\texttt{TRUE}) &gt; E(\\text{Earning} \\mid \\text{CollegeDegree} = \\texttt{FALSE})\n\\] is a descriptive statement that says the expected value of earnings is higher among the subgroup with college degrees than among those without college degrees. We made these kinds of descriptive claims already in code by using group_by() and summarize().\nThe mathematical statement \\[\nE\\left(\\text{Earning}^{\\text{CollegeDegree} = \\texttt{TRUE}}\\right) &gt; E\\left(\\text{Earning}^{\\text{CollegeDegree} = \\texttt{FALSE}}\\right)\n\\] is a causal claim that the expected value of earnings that a random person would realize if assigned to a college degree is higher than the expected value if a random person were assigned to no college degree. Because there is no vertical bar (\\(\\mid\\)), the causal claim is an average over the entire population on both sides of the inequality. The descriptive claim, by contrast, is an average over two different sets of units. The figure below visualizes the difference between these descriptive and causal claims using a visual analogous to the one used to introduce a Mediterranean diet.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Defining Causal Effects"
    ]
  },
  {
    "objectID": "topics/10_define_causal.html#closing-thoughts",
    "href": "topics/10_define_causal.html#closing-thoughts",
    "title": "Defining Causal Effects",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nThe fundamental problem of causal inference is a deep challenge; causal claims will always involve outcomes that are missing and which can only be learned through assumptions and argument. The next class will introduce randomized experiments and show how they provide one setting in which the assumptions required for causal inference are especially tenable.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Defining Causal Effects"
    ]
  },
  {
    "objectID": "topics/10_define_causal.html#what-to-read",
    "href": "topics/10_define_causal.html#what-to-read",
    "title": "Defining Causal Effects",
    "section": "What to read",
    "text": "What to read\nNo reading is required after this class, but if you would like to learn more about causal inference, there are a few places to look. We will repeatedly use college completion as an example of a causal treatment, drawing on examples from the following book.\n\nBrand, Jennie E. 2023. Overcoming the Odds: The Benefits of Completing College for Unlikely Graduates. Russell Sage Foundation. Here is a link to read online through the UCLA Library.\n\nFor a mathematical introduction with examples from epidemiology, see Chapter 1 of this book.\n\nHern'an, Miguel A. and James M. Robins. Causal Inference: What If?. Boca Raton: Chapman & Hall / CRC.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Defining Causal Effects"
    ]
  },
  {
    "objectID": "slides/m_discussion_poststratification_dag.html",
    "href": "slides/m_discussion_poststratification_dag.html",
    "title": "m_discussion_poststratification_dag",
    "section": "",
    "text": "DAGs are not just useful for causal inference: they can be useful whenever we need to know whether one variable is statistically independent of another. This is true, for example, when drawing inference about a population from a sample.\nA researcher uses an opt-in online web survey to draw inference about support for President Biden. They ask respondents: ``Do you approve of President Biden’s performance in office?’’ with the answer choices Yes/No. The researcher also gathers data on two demographic characteristics: whether the respondent completed college and current employment. They write:\n\nMy sample is not representative. Suppose for every person in the population, \\(S\\) denotes whether they are included in my sample. Then \\(S\\) is related to their approval of President Biden (\\(Y\\)).\nHowever, I believe my sample is representative when I look at a set of people who all take the same value along college completion and employment, such as those who finished college and are currently employed. If these variables are \\(X_1,X_2\\), I believe this independence statement: \\(S\\) is independent of \\(Y\\) given \\(X_1,X_2\\). I will therefore get population estimates by a procedure with several steps: use my sample to estimate the mean outcome \\(E(Y\\mid \\vec{X} = \\vec{x})\\) in each stratum, then use Census data to estimate the size of each stratum \\(P(\\vec{X} = \\vec{x})\\) in the population, then estimate \\(E(Y) = \\sum_{\\vec{x}}E(Y\\mid \\vec{X} = \\vec{x})P(\\vec{X} = \\vec{x})\\).\n\nThis researcher’s reasoning is a common strategy known as post-stratification. This question is about formalizing a set of conditions under which the researcher is right and wrong.\nBefore you begin, we want to emphasize one aspect of the researcher’s assumption that is different from the exchangeability assumption for causal inference.\n\nfor causal claims, we assume conditional exchangeability: \\(A\\) independent of \\(Y^a\\) given \\(\\vec{X}\\)\n\ninvolves the potential outcome \\(Y^a\\)\nholds if the only unblocked paths between \\(A\\) and \\(Y\\) are causal paths\n\nfor sample-to-population inference, we assume conditionally independent sampling \\(S\\) independent of \\(Y\\) given \\(\\vec{X}\\)\n\ninvolves the factual outcome \\(Y\\); there is no intervention here\nholds if there are no unblocked paths between \\(S\\) and \\(Y\\)\n\n\nAlthough the assumption is different, the principles of DAGs are still relevant.\n\n\nDraw a DAG under which the researcher’s claim is valid. Use \\(S,Y,X_1,X_2\\).\n\n\n\nIn a sentence or two, explain your DAG from 3.1 to the researcher. Tell us in words what is meant by each edge in your DAG.\n\n\n\nDraw a DAG showing a counterexample under which the researcher’s claim is invalid.\n\n\n\nIn a sentence or two, explain your DAG from 3.3 to the researcher. Tell us particularly about the path that creates a statistical dependence between \\(S\\) and \\(Y\\)."
  },
  {
    "objectID": "slides/m_discussion_poststratification_dag.html#using-dags-in-a-new-context",
    "href": "slides/m_discussion_poststratification_dag.html#using-dags-in-a-new-context",
    "title": "m_discussion_poststratification_dag",
    "section": "",
    "text": "DAGs are not just useful for causal inference: they can be useful whenever we need to know whether one variable is statistically independent of another. This is true, for example, when drawing inference about a population from a sample.\nA researcher uses an opt-in online web survey to draw inference about support for President Biden. They ask respondents: ``Do you approve of President Biden’s performance in office?’’ with the answer choices Yes/No. The researcher also gathers data on two demographic characteristics: whether the respondent completed college and current employment. They write:\n\nMy sample is not representative. Suppose for every person in the population, \\(S\\) denotes whether they are included in my sample. Then \\(S\\) is related to their approval of President Biden (\\(Y\\)).\nHowever, I believe my sample is representative when I look at a set of people who all take the same value along college completion and employment, such as those who finished college and are currently employed. If these variables are \\(X_1,X_2\\), I believe this independence statement: \\(S\\) is independent of \\(Y\\) given \\(X_1,X_2\\). I will therefore get population estimates by a procedure with several steps: use my sample to estimate the mean outcome \\(E(Y\\mid \\vec{X} = \\vec{x})\\) in each stratum, then use Census data to estimate the size of each stratum \\(P(\\vec{X} = \\vec{x})\\) in the population, then estimate \\(E(Y) = \\sum_{\\vec{x}}E(Y\\mid \\vec{X} = \\vec{x})P(\\vec{X} = \\vec{x})\\).\n\nThis researcher’s reasoning is a common strategy known as post-stratification. This question is about formalizing a set of conditions under which the researcher is right and wrong.\nBefore you begin, we want to emphasize one aspect of the researcher’s assumption that is different from the exchangeability assumption for causal inference.\n\nfor causal claims, we assume conditional exchangeability: \\(A\\) independent of \\(Y^a\\) given \\(\\vec{X}\\)\n\ninvolves the potential outcome \\(Y^a\\)\nholds if the only unblocked paths between \\(A\\) and \\(Y\\) are causal paths\n\nfor sample-to-population inference, we assume conditionally independent sampling \\(S\\) independent of \\(Y\\) given \\(\\vec{X}\\)\n\ninvolves the factual outcome \\(Y\\); there is no intervention here\nholds if there are no unblocked paths between \\(S\\) and \\(Y\\)\n\n\nAlthough the assumption is different, the principles of DAGs are still relevant.\n\n\nDraw a DAG under which the researcher’s claim is valid. Use \\(S,Y,X_1,X_2\\).\n\n\n\nIn a sentence or two, explain your DAG from 3.1 to the researcher. Tell us in words what is meant by each edge in your DAG.\n\n\n\nDraw a DAG showing a counterexample under which the researcher’s claim is invalid.\n\n\n\nIn a sentence or two, explain your DAG from 3.3 to the researcher. Tell us particularly about the path that creates a statistical dependence between \\(S\\) and \\(Y\\)."
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#linear-regression-learning-goals",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#linear-regression-learning-goals",
    "title": "Linear Regression",
    "section": "Linear regression: Learning goals",
    "text": "Linear regression: Learning goals\nSome things you may know\n\nHow to fit a linear model\nHow to make predictions\n\nData science ideas\n\nWhy model at all?\nPenalized linear regression"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#data-for-illustration",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#data-for-illustration",
    "title": "Linear Regression",
    "section": "Data for illustration",
    "text": "Data for illustration\nU.S. adult income by\n\nsex (male, female)\nage (30–50)\nyear (2010–2019)\n\namong those working 35+ hours per week for 50+ weeks per year. Data are simulated based on the 2010–2019 American Community Survey (ACS)."
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#data-for-illustration-1",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#data-for-illustration-1",
    "title": "Linear Regression",
    "section": "Data for illustration",
    "text": "Data for illustration\nThe function below will simulate data\n\nsimulate &lt;- function(n = 100) {\n  read_csv(\"https://ilundberg.github.io/description/assets/truth.csv\") |&gt;\n    slice_sample(n = n, weight_by = weight, replace = T) |&gt;\n    mutate(income = exp(rnorm(n(), meanlog, sdlog))) |&gt;\n    select(year, age, sex, income)\n}"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#data-for-illustration-2",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#data-for-illustration-2",
    "title": "Linear Regression",
    "section": "Data for illustration",
    "text": "Data for illustration\n\nsimulated &lt;- simulate(n = 3e4)\n\n\n\n# A tibble: 30,000 × 4\n   year   age sex    income\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  2011    48 female 93676.\n2  2012    38 female 98805.\n3  2013    38 female 52330.\n# ℹ 29,997 more rows"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#conditional-expectation",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#conditional-expectation",
    "title": "Linear Regression",
    "section": "Conditional expectation",
    "text": "Conditional expectation\n\nMean of an outcome within a population subgroup.\n\n\n\nexpectation refers to taking a mean\n\n\n\n\nconditional refers to within a subgroup\n\n\n\nExample: Mean income among females age 47 in 2019\n\n\nTask. Estimate this in our data."
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-find-the-subgroup",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-find-the-subgroup",
    "title": "Linear Regression",
    "section": "Code: Find the subgroup",
    "text": "Code: Find the subgroup\nfilter() restricts our data to cases meeting requirements:\n\nthe sex variable equals the value female\nthe age variable equals the value 47\nthe year variable equals the value 2019\n\n\nsubgroup &lt;- simulated |&gt;\n  filter(sex == \"female\") |&gt;\n  filter(age == 47) |&gt;\n  filter(year == 2019)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-estimate-the-mean",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-estimate-the-mean",
    "title": "Linear Regression",
    "section": "Code: Estimate the mean",
    "text": "Code: Estimate the mean\nsummarize() aggregates to the mean\n\nsubgroup |&gt;\n  summarize(conditional_expectation = mean(income))\n\n# A tibble: 1 × 1\n  conditional_expectation\n                    &lt;dbl&gt;\n1                  71530."
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-mean-in-many-subgroups",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-mean-in-many-subgroups",
    "title": "Linear Regression",
    "section": "Code: Mean in many subgroups",
    "text": "Code: Mean in many subgroups\n\nWith group_by, you can summarize many subgroups\n\nsimulated |&gt;\n  group_by(sex, age, year) |&gt;\n  summarize(conditional_expectation = mean(income))\n\n# A tibble: 420 × 4\n# Groups:   sex, age [42]\n  sex      age  year conditional_expectation\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;                   &lt;dbl&gt;\n1 female    30  2010                  45928.\n2 female    30  2011                  43688.\n3 female    30  2012                  42714.\n# ℹ 417 more rows"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#conditional-expectation-math",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#conditional-expectation-math",
    "title": "Linear Regression",
    "section": "Conditional expectation: Math",
    "text": "Conditional expectation: Math\n\nThe conditional expectation function is the subgroup mean of \\(Y\\) within a subgroup with the predictor values \\(\\vec{X} = \\vec{x}\\).\n\\[\nf(\\vec{x}) = \\text{E}(Y\\mid\\vec{X} = \\vec{x})\n\\]\nTo learn \\(f(\\vec{x})\\) from data is a central task in statistical learning."
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#a-subgroup-is-small",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#a-subgroup-is-small",
    "title": "Linear Regression",
    "section": "A subgroup is small",
    "text": "A subgroup is small\n\n\nsimulated |&gt;\n  filter(sex == \"female\") |&gt;\n  filter(year == 2019) |&gt;\n  filter(age == 47)\n\n# A tibble: 67 × 4\n   year   age sex    income\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  2019    47 female 15761.\n2  2019    47 female 32995.\n3  2019    47 female 83967.\n# ℹ 64 more rows\n\n\n\n\nVery few cases \\(\\rightarrow\\) statistically uncertain\n\n\nHow to better estimate for 47-year-old females in 2019?"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#pooling-information-across-subgroups",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#pooling-information-across-subgroups",
    "title": "Linear Regression",
    "section": "Pooling information across subgroups",
    "text": "Pooling information across subgroups\n\nWe have many female respondents in 2019. Few are age 47.\n\n\n\nsimulated |&gt;\n  filter(sex == \"female\") |&gt;\n  filter(year == 2019)\n\n# A tibble: 1,427 × 4\n   year   age sex    income\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  2019    32 female 52130.\n2  2019    46 female 17465.\n3  2019    41 female 66012.\n# ℹ 1,424 more rows\n\n\n\n\nCould we use them to learn about the 47-year-olds?"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#pooling-information-across-subgroups-1",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#pooling-information-across-subgroups-1",
    "title": "Linear Regression",
    "section": "Pooling information across subgroups",
    "text": "Pooling information across subgroups"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#pooling-information-across-subgroups-2",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#pooling-information-across-subgroups-2",
    "title": "Linear Regression",
    "section": "Pooling information across subgroups",
    "text": "Pooling information across subgroups"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#pooling-information-across-subgroups-3",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#pooling-information-across-subgroups-3",
    "title": "Linear Regression",
    "section": "Pooling information across subgroups",
    "text": "Pooling information across subgroups"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#practice-question",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#practice-question",
    "title": "Linear Regression",
    "section": "Practice question",
    "text": "Practice question\n\\[\n\\text{E}(Y\\mid X) = \\beta_0 + \\beta_1 X\n\\]\nSuppose \\(\\beta_0 = 5\\) and \\(\\beta_1 = 3\\)\n\nWhat is the conditional mean when \\(X = 0\\)?\nWhat is the conditional mean when \\(X = 1\\)?\nWhat is the conditional mean when \\(X = 2\\)?\nHow much does the conditional mean change for each unit increase in \\(X\\)?"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code",
    "title": "Linear Regression",
    "section": "Code",
    "text": "Code\nThe next slides explain how to code a model in R."
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-simulate-data",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-simulate-data",
    "title": "Linear Regression",
    "section": "Code: Simulate data",
    "text": "Code: Simulate data\n\nGenerate some data\n\nsimulated &lt;- simulate(n = 3e4)\n\n\n\nRestrict to female respondents in 2019\n\nfemale_2019 &lt;- simulated |&gt;\n  filter(sex == \"female\") |&gt;\n  filter(year == 2019)\n\n\n\n(Below is simulate if you did not copy it before)\n\nsimulate &lt;- function(n = 100) {\n  read_csv(\"https://ilundberg.github.io/description/assets/truth.csv\") |&gt;\n    slice_sample(n = n, weight_by = weight, replace = T) |&gt;\n    mutate(income = exp(rnorm(n(), meanlog, sdlog))) |&gt;\n    select(year, age, sex, income)\n}"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-learn-a-model",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-learn-a-model",
    "title": "Linear Regression",
    "section": "Code: Learn a model",
    "text": "Code: Learn a model\n\nmodel &lt;- lm(\n  formula = income ~ age, \n  data = female_2019\n)\n\n\nmodel is an object of class lm for linear model\nlm() function creates this object\nformula argument is a model formula\n\noutcome ~ predictor is the syntax\n\ndata is a dataset containing outcome and predictor"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-examine-the-learned-model",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-examine-the-learned-model",
    "title": "Linear Regression",
    "section": "Code: Examine the learned model",
    "text": "Code: Examine the learned model\n\nsummary(model)\n\n\nCall:\nlm(formula = income ~ age, data = female_2019)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-52689 -29518 -12682  16013 400507 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  47242.6     7518.3   6.284 4.37e-10 ***\nage            233.7      185.7   1.259    0.208    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 43360 on 1437 degrees of freedom\nMultiple R-squared:  0.001102,  Adjusted R-squared:  0.0004064 \nF-statistic: 1.585 on 1 and 1437 DF,  p-value: 0.2083"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-predict-for-a-new-x-value",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-predict-for-a-new-x-value",
    "title": "Linear Regression",
    "section": "Code: Predict for a new X value",
    "text": "Code: Predict for a new X value\n\nDefine X value at which to predict\n\nto_predict &lt;- tibble(age = 47)\n\n\n\nPredict for that subgroup\n\npredict(model, newdata = to_predict)\n\n       1 \n58228.57 \n\n\n\n\nRecap: Our model pooled information:\n\nPeople of all ages contributed to model\nThen we predicted at a single age"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-three-steps",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-three-steps",
    "title": "Linear Regression",
    "section": "Code: Three steps",
    "text": "Code: Three steps\n\nEstimate a model\nDefine \\(x\\) to predict\nPredict \\(\\hat{Y} = \\hat{\\text{E}}(Y\\mid X = x)\\)\n\nWhat if you were going to do this many times on different data?"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-three-steps-in-a-function",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-three-steps-in-a-function",
    "title": "Linear Regression",
    "section": "Code: Three steps in a function",
    "text": "Code: Three steps in a function\n\n\nestimator &lt;- function(data) {\n  # Learn the model from the data\n  model &lt;- lm(formula = income ~ age, data = data)\n  # Define our target subgroup\n  to_predict &lt;- tibble(age = 47)\n  # Predict\n  estimate &lt;- predict(model, newdata = to_predict)\n  # Return the estimate\n  return(estimate)\n}"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#code-all-together",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#code-all-together",
    "title": "Linear Regression",
    "section": "Code: All together",
    "text": "Code: All together\n\nestimator(data = female_2019)\n\n       1 \n58228.57 \n\n\n\n\nfemale_2019 |&gt;\n  estimator()\n\n       1 \n58228.57 \n\n\n\n\n\nsimulated |&gt; \n  filter(sex == \"female\") |&gt;\n  filter(year == 2010) |&gt;\n  estimator()\n\n      1 \n54637.9"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#practice-question-1",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#practice-question-1",
    "title": "Linear Regression",
    "section": "Practice question",
    "text": "Practice question\nBelow is the line fit to the population data. Suppose we want to learn \\(\\text{E}(\\log(Y)\\mid X = 30)\\).\n\n\nWhy might this model make a misleading estimate?\nWhy might the model still be useful?"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models",
    "title": "Linear Regression",
    "section": "Two models",
    "text": "Two models"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-1",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-1",
    "title": "Linear Regression",
    "section": "Two models",
    "text": "Two models"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-2",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-2",
    "title": "Linear Regression",
    "section": "Two models",
    "text": "Two models"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-interaction",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-interaction",
    "title": "Linear Regression",
    "section": "Two models: Interaction",
    "text": "Two models: Interaction\n\\[\n\\begin{aligned}\n\\text{E}(Y\\mid X, \\text{Female}) &= \\beta_0^\\text{Female} + \\beta_1^\\text{Female}\\times \\text{Age} \\\\\n\\text{E}(Y\\mid X, \\text{Male}) &= \\beta_0^\\text{Male} + \\beta_1^\\text{Male}\\times \\text{Age} \\\\\n\\end{aligned}\n\\]\n\nEquivalently, \\[\\text{E}(Y \\mid X, \\text{Sex}) = \\gamma_0 + \\gamma_1(\\text{Female}) + \\gamma_2(\\text{Age}) + \\gamma_3 (\\text{Age} \\times \\text{Female})\\] . . .\nwhere \\[\\begin{aligned}\n\\gamma_0 &= \\beta_0^\\text{Male}\n&\\gamma_1 &= \\beta_0^\\text{Female} - \\beta_0^\\text{Male} \\\\\n\\gamma_2 &= \\beta_1^\\text{Male}\n&\\gamma_3 &= \\beta_1^\\text{Female} - \\beta_1^\\text{Male}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-interaction-in-code",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-interaction-in-code",
    "title": "Linear Regression",
    "section": "Two models: Interaction in code",
    "text": "Two models: Interaction in code\nGenerate data in 2019 that vary in both sex and age\n\nall_2019 &lt;- simulated |&gt;\n  filter(year == 2019)\n\n\n\n# A tibble: 3,204 × 4\n   year   age sex   income\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  2019    41 male  50285.\n2  2019    45 male  31057.\n3  2019    34 male  66166.\n# ℹ 3,201 more rows"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-interaction-in-code-1",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-interaction-in-code-1",
    "title": "Linear Regression",
    "section": "Two models: Interaction in code",
    "text": "Two models: Interaction in code\n\nThe * operator allows slopes to differ across groups\n\nmodel &lt;- lm(\n  formula = income ~ sex * age,\n  data = all_2019\n)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-additive-model-in-r",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#two-models-additive-model-in-r",
    "title": "Linear Regression",
    "section": "Two models: Additive model in R",
    "text": "Two models: Additive model in R\nThe + operator assumes slopes are the same across groups\n\nmodel &lt;- lm(\n  formula = income ~ sex + age,\n  data = all_2019\n)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#interactions-make-lots-of-terms",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#interactions-make-lots-of-terms",
    "title": "Linear Regression",
    "section": "Interactions make lots of terms",
    "text": "Interactions make lots of terms\n\nmodel &lt;- lm(\n  formula = income ~ sex * age * year,\n  data = simulated\n)\n\n\n\nsummary(model)\n\n\nCall:\nlm(formula = income ~ sex * age * year, data = simulated)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-81158 -33849 -14946  15839 972817 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)       1.387e+06  2.343e+06   0.592    0.554\nsexmale          -1.943e+06  3.117e+06  -0.623    0.533\nage              -6.273e+04  5.760e+04  -1.089    0.276\nyear             -6.680e+02  1.163e+03  -0.574    0.566\nsexmale:age       6.646e+04  7.675e+04   0.866    0.386\nsexmale:year      9.519e+02  1.547e+03   0.615    0.538\nage:year          3.130e+01  2.859e+01   1.095    0.274\nsexmale:age:year -3.247e+01  3.809e+01  -0.852    0.394\n\nResidual standard error: 57790 on 29992 degrees of freedom\nMultiple R-squared:  0.03332,   Adjusted R-squared:  0.03309 \nF-statistic: 147.7 on 7 and 29992 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#interactions-make-lots-of-terms-1",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#interactions-make-lots-of-terms-1",
    "title": "Linear Regression",
    "section": "Interactions make lots of terms",
    "text": "Interactions make lots of terms"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-1",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-1",
    "title": "Linear Regression",
    "section": "Penalized regression",
    "text": "Penalized regression\nOLS is a linear model\n\\[\\text{E}(Y\\mid\\vec{X}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots\\]\n\nThere are many linear models beyond OLS.\n\n(other ways of estimating the \\(\\beta\\) coefficients)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-2",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-2",
    "title": "Linear Regression",
    "section": "Penalized regression",
    "text": "Penalized regression"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-3",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-3",
    "title": "Linear Regression",
    "section": "Penalized regression",
    "text": "Penalized regression"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#unpenalized-regression-in-math",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#unpenalized-regression-in-math",
    "title": "Linear Regression",
    "section": "Unpenalized regression: In math",
    "text": "Unpenalized regression: In math\nOLS chose \\(\\alpha, \\vec\\beta\\) to minimize this function: \\[\n\\begin{aligned}\n\\underbrace{\\sum_i\\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Sum of Squared Error}\n\\end{aligned}\n\\] where \\(\\hat{Y}_i = \\hat\\alpha + \\sum_j X_j \\hat\\beta_j\\)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-in-math",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-in-math",
    "title": "Linear Regression",
    "section": "Penalized regression: In math",
    "text": "Penalized regression: In math\nPenalized (ridge) regression chose \\(\\alpha, \\vec\\beta\\) to minimize this function: \\[\n\\begin{aligned}\n\\underbrace{\\sum_i\\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda \\sum_{j} \\beta_j^2}_\\text{Penalty Term}\n\\end{aligned}\n\\] where \\(\\hat{Y}_i = \\hat\\alpha + \\sum_j X_j \\hat\\beta_j\\)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code",
    "title": "Linear Regression",
    "section": "Penalized regression: Code",
    "text": "Penalized regression: Code\n\nsimulated &lt;- simulate(n = 1e5)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code-1",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code-1",
    "title": "Linear Regression",
    "section": "Penalized regression: Code",
    "text": "Penalized regression: Code\nThe glmnet package supports penalized regression\n\nlibrary(glmnet)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code-2",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code-2",
    "title": "Linear Regression",
    "section": "Penalized regression: Code",
    "text": "Penalized regression: Code\nCreate a model matrix of predictors\n\nEach column will correspond to a coefficient\n\n\nX &lt;- model.matrix(~ age * sex * year, data = simulated)\n\n\nCreate a vector of the outcomes\n\ny &lt;- simulated |&gt; pull(income)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code-3",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code-3",
    "title": "Linear Regression",
    "section": "Penalized regression: Code",
    "text": "Penalized regression: Code\nUse the cv.glmnet function\n\npenalized &lt;- cv.glmnet(\n  x = X,    # model matrix we created\n  y = y,    # outcome vector we created\n  alpha = 0 # penalize sum of beta ^ 2\n)"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code-4",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#penalized-regression-code-4",
    "title": "Linear Regression",
    "section": "Penalized regression: Code",
    "text": "Penalized regression: Code\n\nyhat &lt;- predict(\n  penalized,\n  newx = X\n)\n\n\nsummary(yhat)\n\n   lambda.1se   \n Min.   :60582  \n 1st Qu.:62568  \n Median :65476  \n Mean   :65063  \n 3rd Qu.:67425  \n Max.   :69405"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#when-to-use-penalized-regression",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#when-to-use-penalized-regression",
    "title": "Linear Regression",
    "section": "When to use penalized regression?",
    "text": "When to use penalized regression?\n\n\nMany predictors and few observations\n\nHigh-variance estimates\n\n\n\n\n\nWhen you are willing to accept bias\n\nModel will be a bit wrong on average"
  },
  {
    "objectID": "slides/lec05_linear_regression/lec05_linear_regression.html#linear-regression-learning-goals-1",
    "href": "slides/lec05_linear_regression/lec05_linear_regression.html#linear-regression-learning-goals-1",
    "title": "Linear Regression",
    "section": "Linear regression: Learning goals",
    "text": "Linear regression: Learning goals\nSome things you may know\n\nHow to fit a linear model\nHow to make predictions\n\nData science ideas\n\nWhy model at all?\nPenalized linear regression"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#review-of-last-class",
    "href": "slides/lec02_visualization/lec02.html#review-of-last-class",
    "title": "Visualization and Summary Statistics",
    "section": "Review of last class",
    "text": "Review of last class\nSay this code in English:\n\nnumbers &lt;- c(1,2,3)\nx_length &lt;- length(numbers)\n\n\n\nStore the vector c(1,2,3) in the object numbers\nUse the length() function to get the length of numbers"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#learning-goals-for-today",
    "href": "slides/lec02_visualization/lec02.html#learning-goals-for-today",
    "title": "Visualization and Summary Statistics",
    "section": "Learning goals for today",
    "text": "Learning goals for today\n\nReason about distributions\n\nand visualize with ggplot()\n\nUnderstand summary statistics\n\nand construct with summarize()\n\nWrite clean code with the pipe |&gt;"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#how-to-visualize",
    "href": "slides/lec02_visualization/lec02.html#how-to-visualize",
    "title": "Visualization and Summary Statistics",
    "section": "How to visualize?",
    "text": "How to visualize?\nHow might we visualize the U.S. income distribution?\nHere are some data:\n\nlibrary(tidyverse)\nincomeSimulated &lt;- read_csv(\n  file = \"https://soc114.github.io/data/incomeSimulated.csv\"\n)\n\n\n\n# A tibble: 1,000 × 2\n     id hhincome\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     1   19170.\n2     2  124474.\n3     3   25114.\n# ℹ 997 more rows"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#visualize-with-a-histogram",
    "href": "slides/lec02_visualization/lec02.html#visualize-with-a-histogram",
    "title": "Visualization and Summary Statistics",
    "section": "Visualize with a histogram",
    "text": "Visualize with a histogram\n\n\nBins of $25,000\nIn each bin, count number of people"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#learning-a-new-function-ggplot",
    "href": "slides/lec02_visualization/lec02.html#learning-a-new-function-ggplot",
    "title": "Visualization and Summary Statistics",
    "section": "Learning a new function: ggplot",
    "text": "Learning a new function: ggplot\n\nggplot(\n  data = incomeSimulated,\n  mapping = aes(x = hhincome)\n)\n\n\n\nTwo arguments get us started:\n\ndata argument contains data\nmapping argument maps data to plot elements\n\n\n\nWithin mapping,\n\naes() defines the aesthetics of the plot\ni.e. which variable goes along x-axis"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#adding-a-layer-geom_histogram",
    "href": "slides/lec02_visualization/lec02.html#adding-a-layer-geom_histogram",
    "title": "Visualization and Summary Statistics",
    "section": "Adding a layer: geom_histogram()",
    "text": "Adding a layer: geom_histogram()\n\nggplot(\n  data = incomeSimulated,\n  mapping = aes(x = hhincome)\n) +\n  geom_histogram()\n\n\n\n\nThe + indicates that a new layer is coming\ngeom_histogram() is the new layer\nInherits the data and mapping of the plot"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#update-axis-titles",
    "href": "slides/lec02_visualization/lec02.html#update-axis-titles",
    "title": "Visualization and Summary Statistics",
    "section": "Update axis titles",
    "text": "Update axis titles\n\nggplot(\n  data = incomeSimulated,\n  mapping = aes(x = hhincome)\n) +\n  geom_histogram() +\n  labs(\n    x = \"Household Income\", \n    y = \"Count of Households in Bin\"\n  )"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#learning-goals-for-today-1",
    "href": "slides/lec02_visualization/lec02.html#learning-goals-for-today-1",
    "title": "Visualization and Summary Statistics",
    "section": "Learning goals for today",
    "text": "Learning goals for today\n\nReason about distributions\n\nand visualize with ggplot()\n\nUnderstand summary statistics\n\nand construct with summarize()\n\nWrite clean code with the pipe |&gt;"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#imagine-3-income-distributions",
    "href": "slides/lec02_visualization/lec02.html#imagine-3-income-distributions",
    "title": "Visualization and Summary Statistics",
    "section": "Imagine 3 income distributions",
    "text": "Imagine 3 income distributions\n\n\n\nHousehold\nDistribution 1\nDistribution 2\nDistribution 3\n\n\n\n\n1\n$10k\n$40k\n$50k\n\n\n2\n$60k\n$65k\n$60k\n\n\n3\n$150k\n$70k\n$65k\n\n\n\nNormative question: Which one is better?"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#summary-statistic",
    "href": "slides/lec02_visualization/lec02.html#summary-statistic",
    "title": "Visualization and Summary Statistics",
    "section": "Summary statistic",
    "text": "Summary statistic\nA summary statistic aggregates a distribution to one number\n\nFor example, the mean \\[\\text{mean}(\\vec{x}) = \\frac{x_1 + x_2 +\\cdots}{n}\\]"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#summary-statistic-mean",
    "href": "slides/lec02_visualization/lec02.html#summary-statistic-mean",
    "title": "Visualization and Summary Statistics",
    "section": "Summary statistic: Mean",
    "text": "Summary statistic: Mean\n\n\n\nHousehold\nDistribution 1\nDistribution 2\nDistribution 3\n\n\n\n\n1\n$10k\n$40k\n$50k\n\n\n2\n$60k\n$65k\n$60k\n\n\n3\n$150k\n$70k\n$65k\n\n\nMean\n$73k\n$58k\n$58k\n\n\n\n\nBy the mean, Distribution 1 seems the best."
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#summary-statistic-median",
    "href": "slides/lec02_visualization/lec02.html#summary-statistic-median",
    "title": "Visualization and Summary Statistics",
    "section": "Summary statistic: Median",
    "text": "Summary statistic: Median\n\nSort households by income.\nFind where 50% of households have higher incomes\n\n\n\n\nHousehold\nDistribution 1\nDistribution 2\nDistribution 3\n\n\n\n\n1\n$10k\n$40k\n$50k\n\n\n2\n$60k\n$65k\n$60k\n\n\n3\n$150k\n$70k\n$65k"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#summary-statistic-median-1",
    "href": "slides/lec02_visualization/lec02.html#summary-statistic-median-1",
    "title": "Visualization and Summary Statistics",
    "section": "Summary statistic: Median",
    "text": "Summary statistic: Median\n\nSort households by income.\nFind where 50% of households have higher incomes\n\n\n\n\nHousehold\nDistribution 1\nDistribution 2\nDistribution 3\n\n\n\n\n1\n$10k\n$40k\n$50k\n\n\n2\n$60k\n$65k\n$60k\n\n\n3\n$150k\n$70k\n$65k\n\n\nMedian\n$60k\n$65k\n$60k\n\n\n\n\nBy the median, Distribution 2 seems the best."
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#aside-percentiles-generalize-the-median",
    "href": "slides/lec02_visualization/lec02.html#aside-percentiles-generalize-the-median",
    "title": "Visualization and Summary Statistics",
    "section": "Aside: Percentiles generalize the median",
    "text": "Aside: Percentiles generalize the median\nThe median is the value in the middle\n\n50% of people have lower values\nAlso called the 50th percentile\n\n\nGeneralizes to other percentiles\n\n10th percentile: Value such that 10% are lower\n90th percentile: Value such that 90% are lower\n\nThese summarize the bottom and top of a distribution."
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#summary-statistic-minimum",
    "href": "slides/lec02_visualization/lec02.html#summary-statistic-minimum",
    "title": "Visualization and Summary Statistics",
    "section": "Summary statistic: Minimum",
    "text": "Summary statistic: Minimum\nFind the lowest value.\n\n\n\nHousehold\nDistribution 1\nDistribution 2\nDistribution 3\n\n\n\n\n1\n$10k\n$40k\n$50k\n\n\n2\n$60k\n$65k\n$60k\n\n\n3\n$150k\n$70k\n$65k"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#summary-statistic-minimum-1",
    "href": "slides/lec02_visualization/lec02.html#summary-statistic-minimum-1",
    "title": "Visualization and Summary Statistics",
    "section": "Summary statistic: Minimum",
    "text": "Summary statistic: Minimum\nFind the lowest value.\n\n\n\nHousehold\nDistribution 1\nDistribution 2\nDistribution 3\n\n\n\n\n1\n$10k\n$40k\n$50k\n\n\n2\n$60k\n$65k\n$60k\n\n\n3\n$150k\n$70k\n$65k\n\n\nMinimum\n$10k\n$40k\n$50k\n\n\n\n\nBy the minimum, Distribution 3 seems the best."
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#which-summary-statistic-to-choose",
    "href": "slides/lec02_visualization/lec02.html#which-summary-statistic-to-choose",
    "title": "Visualization and Summary Statistics",
    "section": "Which summary statistic to choose?",
    "text": "Which summary statistic to choose?\nMinimum? Median? Mean?\n\n\n\nHousehold\nDistribution 1\nDistribution 2\nDistribution 3\n\n\n\n\n1\n$10k\n$40k\n$50k\n\n\n2\n$60k\n$65k\n$60k\n\n\n3\n$150k\n$70k\n$65k"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#choosing-a-summary-statistic",
    "href": "slides/lec02_visualization/lec02.html#choosing-a-summary-statistic",
    "title": "Visualization and Summary Statistics",
    "section": "Choosing a summary statistic",
    "text": "Choosing a summary statistic\nWhich summary to choose is not an empirical question.\n\n\nDepends on what aspect of the distribution matters to you\n\n\n\nThe value of a chosen summary statistic is empirical.\n\nData can tell us a value for the mean, the median, the minimum, etc."
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#the-summarize-function",
    "href": "slides/lec02_visualization/lec02.html#the-summarize-function",
    "title": "Visualization and Summary Statistics",
    "section": "The summarize() function",
    "text": "The summarize() function\nThe summarize() function aggregates data to summaries.\n\nInput is a dataset with \\(n\\) rows\nOutput is a summary with 1 row"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#the-summarize-function-1",
    "href": "slides/lec02_visualization/lec02.html#the-summarize-function-1",
    "title": "Visualization and Summary Statistics",
    "section": "The summarize() function",
    "text": "The summarize() function\n\nincomeSimulated\n\n\n\n# A tibble: 1,000 × 2\n     id hhincome\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     1   19170.\n2     2  124474.\n3     3   25114.\n# ℹ 997 more rows"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#the-summarize-function-2",
    "href": "slides/lec02_visualization/lec02.html#the-summarize-function-2",
    "title": "Visualization and Summary Statistics",
    "section": "The summarize() function",
    "text": "The summarize() function\n\nsummarize(\n  .data = incomeSimulated,\n  estimated_mean = mean(hhincome)\n)\n\n# A tibble: 1 × 1\n  estimated_mean\n           &lt;dbl&gt;\n1        100899.\n\n\n\n.data is input data\nestimated_mean is a variable in output data\nmean(hhincome) is the mean household income"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#the-summarize-function-several-summaries",
    "href": "slides/lec02_visualization/lec02.html#the-summarize-function-several-summaries",
    "title": "Visualization and Summary Statistics",
    "section": "The summarize() function: Several summaries",
    "text": "The summarize() function: Several summaries\n\nsummarize(\n  .data = incomeSimulated,\n  estimated_mean = mean(hhincome),\n  estimated_median = median(hhincome),\n  esitmated_min = min(hhincome)\n)\n\n# A tibble: 1 × 3\n  estimated_mean estimated_median esitmated_min\n           &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n1        100899.           69035.             0"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#learning-goals-for-today-2",
    "href": "slides/lec02_visualization/lec02.html#learning-goals-for-today-2",
    "title": "Visualization and Summary Statistics",
    "section": "Learning goals for today",
    "text": "Learning goals for today\n\nReason about distributions\n\nand visualize with ggplot()\n\nUnderstand summary statistics\n\nand construct with summarize()\n\nWrite clean code with the pipe |&gt;"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#piping-code-with",
    "href": "slides/lec02_visualization/lec02.html#piping-code-with",
    "title": "Visualization and Summary Statistics",
    "section": "Piping code with |>",
    "text": "Piping code with |&gt;\n\nx &lt;- c(1,2,3)\nlength(x)\n\n[1] 3\n\nx |&gt; length()\n\n[1] 3\n\n\nThe pipe |&gt; passes x as the first argument to the length() function."
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#piping-code-with-1",
    "href": "slides/lec02_visualization/lec02.html#piping-code-with-1",
    "title": "Visualization and Summary Statistics",
    "section": "Piping code with |>",
    "text": "Piping code with |&gt;\nStylistically helpful\n\nData is a different kind of argument\nPipes will help us in the future\n\n\nincomeSimulated |&gt;\n  summarize(\n    estimated_mean = mean(hhincome),\n    estimated_median = median(hhincome),\n    esitmated_min = min(hhincome)\n  )\n\n# A tibble: 1 × 3\n  estimated_mean estimated_median esitmated_min\n           &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n1        100899.           69035.             0"
  },
  {
    "objectID": "slides/lec02_visualization/lec02.html#learning-goals-for-today-3",
    "href": "slides/lec02_visualization/lec02.html#learning-goals-for-today-3",
    "title": "Visualization and Summary Statistics",
    "section": "Learning goals for today",
    "text": "Learning goals for today\n\nReason about distributions\n\nand visualize with ggplot()\n\nUnderstand summary statistics\n\nand construct with summarize()\n\nWrite clean code with the pipe |&gt;\n\nYou can now learn more: R4DS Ch 1 and Ch 3"
  },
  {
    "objectID": "psets/pset4.html",
    "href": "psets/pset4.html",
    "title": "Problem Set 4: Statistical Learning",
    "section": "",
    "text": "Gender inequality in employment is much greater among new parents than among non-parents. This exercise seeks to estimate the proportion employed among married men and women1 with a 1-year-old child at home. Our data include those with at least one child age 0–18."
  },
  {
    "objectID": "psets/pset4.html#synthetic-data",
    "href": "psets/pset4.html#synthetic-data",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Synthetic data",
    "text": "Synthetic data\nTo speed data access, we downloaded data from the basic monthly Current Population Survey for all months from 2010–2019. We processed these data, grouped by sex and age of the youngest child, and estimated the proportion employed. We then generated synthetic data: we created a new dataset for you to use with simulated people using these known probabilities.\nSynthetic data is good in our setting for two reasons\n\nwe know the answer\nyou can download the synthetic data right from this website\n\nFor transparency, here is the code with which we created the synthetic data. The line below will load the synthetic data.\n\nparents &lt;- read_csv(\"https://soc114.github.io/data/parents.csv\")\n\nYour synthetic data intentionally omits any parents with child age 1. Here is a graph showing the averages in your data, grouped by child age and sex."
  },
  {
    "objectID": "psets/pset4.html#your-task",
    "href": "psets/pset4.html#your-task",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Your task",
    "text": "Your task\nPredict the proportion employed among female respondents whose youngest child is 1 year old.\nThis subgroup at which to make a prediction is:\n\ntarget_population &lt;- tibble(sex = \"female\", child_age = 1)\n\nYou will estimate several models to predict at_work as a function of sex and child_age."
  },
  {
    "objectID": "psets/pset4.html#linear-regression",
    "href": "psets/pset4.html#linear-regression",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Linear regression",
    "text": "Linear regression\n\nEstimate an additive OLS model for at_work as an additive function of sex and child_age. Store it in ols_additive.\nVisualize the additive model. Create a ggplot() in which the \\(x\\)-axis is child_age, the color is sex, and the \\(y\\)-axis has predictions from ols_additive. Store this plot in ols_additive_plot.\nEstimate an interactive OLS model for at_work as a interactive function of sex and child_age. Store it in an object ols_interactive.\nVisualize the interactive model. Create a ggplot() in which the \\(x\\)-axis is child_age, the color is sex, and the \\(y\\)-axis has predictions from ols_interactive. Store this plot in ols_interactive_plot.\nUse either OLS model to predict the outcome in the target population. Store your predicted value (a number) in an object ols_prediction."
  },
  {
    "objectID": "psets/pset4.html#logistic-regression",
    "href": "psets/pset4.html#logistic-regression",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nEstimate a logistic regression model to predict at_work as a function of sex and child_age. You can use any functional form you want. Store it in an object logistic_regression.\nVisualize the logistic regression model. Create a ggplot() in which the \\(x\\)-axis is child_age, the color is sex, and the \\(y\\)-axis has predictions from logistic_regression. Store this plot in logistic_plot.\nUse your logistic regression model to predict the probability of being at_work in the target population. Store your predicted value (a number) in an object logistic_prediction."
  },
  {
    "objectID": "psets/pset4.html#your-approach",
    "href": "psets/pset4.html#your-approach",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Your approach",
    "text": "Your approach\nUltimately, this problem set is a challenge: who can best predict the outcome in target_population?\nYou can use any approach. A model from above, one of them learned on data from a subgroup (e.g., those with child_age under 5), or one with a different functional form (e.g., you can use nonlinear terms for child_age). You can also estimate model-free by taking some subsample mean. You can use any method. If your approach will use a package we have not used in class, let us know on Piazza so that we can ensure the autograder has installed this package.\n\nStore your predicted probability (a number) for target_population in an object called my_prediction.\n\nWe will extract my_prediction from your submitted problem sets, see who is closest, and announce a winner in class!"
  },
  {
    "objectID": "psets/pset4.html#footnotes",
    "href": "psets/pset4.html#footnotes",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEach married pair need not be of different sex. The data include same-sex couples.↩︎"
  },
  {
    "objectID": "psets/pset3.html",
    "href": "psets/pset3.html",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "",
    "text": "For this problem set, we will summarize wealth gaps using data from the 2022 Survey of Consumer Finances. We accessed these data via the Berkeley Survey Documentation and Analysis website and prepared the data file scf_prepared.csv that you can load directly with the line of code below.\nscf_prepared &lt;- read_csv(\"https://soc114.github.io/data/scf_prepared.csv\")\nA row of these data corresponds to a household. These data contain three variables:\nBelow we have info to get you started. The problem set questions start in the section write an estimator function."
  },
  {
    "objectID": "psets/pset3.html#info-to-get-you-started",
    "href": "psets/pset3.html#info-to-get-you-started",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "Info to get you started",
    "text": "Info to get you started\nThis problem set will summarize wealth by the geometric mean, which is a summary statistic often used for distributions with long right tails. This summary statistic can be understood in steps:\n\nTransform \\(X\\) to \\(\\text{log}(X)\\) to bring in the long right tail\nSummarize the transformed distribution by its mean\nExponentiate the summary to move back to the scale of \\(X\\) (since \\(e^{\\text{log}(X)} = X\\))\n\nThe figure below visualizes these steps to build intuition.\n\n\n\n\n\n\n\n\n\nThe code below applies these steps to the scf_prepared data to estimate the geometric mean of household wealth.\n\nscf_prepared |&gt;\n  # Creates a new variable log_wealth\n  # which is the log of the wealth variable\n  mutate(\n    log_wealth = log(wealth)\n  ) |&gt;\n  # Summarizes log wealth\n  # by its weighted mean\n  # (weight is the inverse probability\n  # of sample inclusion)\n  summarize(\n    mean_log_wealth = weighted.mean(\n      x = log_wealth, \n      w = weight\n    )\n  ) |&gt;\n  # Create a new variable geo_mean which\n  # is the exponentiated mean of the log wealth\n  mutate(\n    geo_mean = exp(mean_log_wealth)\n  ) |&gt;\n  # Pull the result out of the tibble\n  # object to return a number\n  pull(geo_mean)\n\n[1] 161195.7\n\n\nNow it is your turn! The code above will be useful in the problem set parts below."
  },
  {
    "objectID": "psets/pset3.html#write-an-estimator-function",
    "href": "psets/pset3.html#write-an-estimator-function",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "1. Write an estimator function",
    "text": "1. Write an estimator function\nWe want to use the code above many times, on many different samples. To do this, write a function called geo_mean_estimator. Your function should have one argument named data, which will be a tibble object in the format of scf_prepared. Your function should take data and calculate a summary statistic: the geometric mean.\nYou can use the code above within the body of your function. For help, see R4DS Ch 25."
  },
  {
    "objectID": "psets/pset3.html#point-estimate",
    "href": "psets/pset3.html#point-estimate",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "2. Point estimate",
    "text": "2. Point estimate\nApply your geo_mean_estimator function with data = scf_prepared. Store the result in an object called geo_mean_estimate."
  },
  {
    "objectID": "psets/pset3.html#bootstrap-draws",
    "href": "psets/pset3.html#bootstrap-draws",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "3. Bootstrap draws",
    "text": "3. Bootstrap draws\nBootstrap your estimator 1,000 times. Store your bootstrap draws in an object named bootstrap_draws.\nHow you do this is up to you. There are at least two viable options:\n\nYou can initialize a vector bootstrap_draws as in class. Then write a for loop to populate it. See lecture slides on for loops.\nYou can use the replicate(n, expr) function where the n argument is the number of times to call the expression in the expr argument. See this blog.\n\nWhatever method you use, you should use slice_sample(prop = 1, replace = TRUE) to resample the dataset with replacement for each bootstrap draw. See lecture slides on how to generate bootstrap samples."
  },
  {
    "objectID": "psets/pset3.html#confidence-interval",
    "href": "psets/pset3.html#confidence-interval",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "4. Confidence interval",
    "text": "4. Confidence interval\nConstruct a 95% confidence interval for the geometric mean by taking the 2.5 and 97.5 percentiles of the bootstrap draws. Store this in an object named confidence_interval."
  },
  {
    "objectID": "psets/pset3.html#write-a-ratio-estimator",
    "href": "psets/pset3.html#write-a-ratio-estimator",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "5. Write a ratio estimator",
    "text": "5. Write a ratio estimator\nNow write a new function named ratio_estimator that estimates a ratio: \\[\n\\frac{\\text{geometric mean among white respondent households}}{\\text{geometric mean among Black respondent households}}\n\\] In words, this summary is how many dollars of wealth White households hold per dollar held by Black households, as summarized by the geometric mean.\nYou might consider using the geo_mean_estimator() function you already wrote within your ratio_estimator() function."
  },
  {
    "objectID": "psets/pset3.html#point-estimate-for-the-ratio",
    "href": "psets/pset3.html#point-estimate-for-the-ratio",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "6. Point estimate for the ratio",
    "text": "6. Point estimate for the ratio\nApply ratio_estimator with data = scf_prepared to produce a point estimate. Store the result in ratio_estimate."
  },
  {
    "objectID": "psets/pset3.html#bootstrap-the-ratio-estimator",
    "href": "psets/pset3.html#bootstrap-the-ratio-estimator",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "7. Bootstrap the ratio estimator",
    "text": "7. Bootstrap the ratio estimator\nBootstrap the ratio estimator with 1,000 draws. Store the bootstrap estimates in an object called ratio_bootstrap."
  },
  {
    "objectID": "psets/pset3.html#confidence-interval-for-the-ratio",
    "href": "psets/pset3.html#confidence-interval-for-the-ratio",
    "title": "Problem Set 3: Estimator and Bootstrap",
    "section": "8. Confidence interval for the ratio",
    "text": "8. Confidence interval for the ratio\nConstruct a 95% confidence interval for the ratio by taking the 2.5 and 97.5 percentiles of the bootstrap draws. Store this in an object named ratio_confidence_interval."
  },
  {
    "objectID": "psets/pset1.html",
    "href": "psets/pset1.html",
    "title": "Problem Set 1: Code Basics",
    "section": "",
    "text": "Due: 5pm on Friday, January 9.\n\nThe goal of this problem set is to learn the basics of coding in R.\n\nCreate an R script named pset1.R. Write code following the steps below. Submit pset1.R by uploading it on Gradescope (accessible via BruinLearn).\nWe will grade by running the code and evaluating whether it produces the requested objects. You do not have to label the question numbers in your code.\nMuch of the material for this problem set corresponds to R4DS Chapter 2. If you are stuck, look at examples there!\n\nWorking with numbers.\n\nStore the value 1 in a new object called number_one.\nCreate a vector of length two holding two values: 1 and 2. You can use the c() function described in the chapter. Store your vector in an object called one_and_two.\nCreate an object called doubled. Multiply one_and_two by 2 and store the result in this object.\nCreate an object called summed. Use the sum() function to find the sum of one_and_two. Store the result in summed.\n\nWorking with data.\n\nLoad data from the course website with the code read_csv(file = \"https://soc114.github.io/data/lifeCourse.csv\"). Store it in an object called lifeCourse.\n\nFor help, see R4DS Ch 7.2.\nThis code requires that the tidyverse package is installed and loaded. See R4DS 1.1.1.\n\nUse the nrow() function to find the number of rows in the data. Store the result in an object called num_rows.\nUse the ncol() function to find the number of rows in the data. Store the result in an object called num_cols.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "psets/old_visualization_pset.html",
    "href": "psets/old_visualization_pset.html",
    "title": "Problem Set 2: Visualization",
    "section": "",
    "text": "This problem set involves both data analysis and reading."
  },
  {
    "objectID": "psets/old_visualization_pset.html#visualize-40-points",
    "href": "psets/old_visualization_pset.html#visualize-40-points",
    "title": "Problem Set 2: Visualization",
    "section": "1. Visualize (40 points)",
    "text": "1. Visualize (40 points)\n\n\n\n\n\n\nTip\n\n\n\nFunctions named in this problem are links to helper pages that provide documentation.\n\n\nUse ggplot to visualize these data. To denote the different trajectories,\n\nmake your plot using geom_point() or geom_line()\nuse the x-axis for age\nuse the y-axis for income\nuse color for quantity\nuse facet_grid to make a panel of facets where each row is an education value and each column is a cohort value\n\nYou should prepare the graph as though you were going to publish it. Modify the axis titles so that a reader would know what is on the axis. Use appropriate capitalization in all labels. Optionally, try using the label_currency() function from the scales package so that the y-axis uses dollar values.\nYour code should be well-formatted as defined by R4DS. In your produced PDF, no lines of code should run off the page.\nMany different graphs can be equally correct. You will be evaluated by\n\nhaving publication-ready graph aesthetics\ncode that follows style conventions\n\n\n# your code goes here"
  },
  {
    "objectID": "psets/old_visualization_pset.html#understand-components-of-research-question",
    "href": "psets/old_visualization_pset.html#understand-components-of-research-question",
    "title": "Problem Set 2: Visualization",
    "section": "2. Understand components of research question",
    "text": "2. Understand components of research question\nThe graph above answered a descriptive research question. Here are a few components of that question:\n\nAnnual income in 2022 dollars\nAmerican workers in each subgroup defined by birth cohort, age, and education\n10th, 50th, and 90th percentile\nA person\n\nFor 2.1–2.4, write the letter of the corresponding component of the research question.\n2.1 (2.5 points) What is the unit of analysis?\n2.2 (2.5 points) What is the outcome variable?\n2.3 (2.5 points) What are the target populations?\n2.4 (2.5 points) What are the summary statistics?"
  },
  {
    "objectID": "psets/old_visualization_pset.html#recap-and-connections-to-your-project",
    "href": "psets/old_visualization_pset.html#recap-and-connections-to-your-project",
    "title": "Problem Set 2: Visualization",
    "section": "Recap and connections to your project",
    "text": "Recap and connections to your project\nThe visualization you produced in this problem set is an example of the kind of visualization you will produce in the final project. As in this problem set, your visualization in the project should include well-written labels to make the graph readable. A difference between the two is that in this problem set we provided a dataset in which each row was already a summary statistic. In the final project, you will manipulate data in which each row corresponds to a unit of analysis that you aggregate to produce a summary statistic. This is a skill you will practice on the next problem set."
  },
  {
    "objectID": "past_sites.html",
    "href": "past_sites.html",
    "title": "Past Year Sites",
    "section": "",
    "text": "The material on this site draws on the websites of several past courses that I have taught.\n\nUCLA SOC 114 (Winter 2025)\nCornell INFO 3370 (Spring 2024) (Spring 2023)\nCornell INFO 3900 (Fall 2023)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "honors/writing.html",
    "href": "honors/writing.html",
    "title": "Writing",
    "section": "",
    "text": "The text you write to accompany your graph can help to tell the story.\nYou might start with a puzzle or question for which the answer is unknown. You might also start with a surprising finding: in this project, we find that ___. Your goal is to help the reader want to read more.\nTo help us read the graph, one good strategy is to walk us through a particular point. Tell us how to read the information in that particular point. Then, you can tell us about a trend across the points (or bars, or whatever elements create your graph).\nFor example, suppose you have a scatterplot with age on the x-axis and median income on the y-axis. You might explain a particular dot that represents people who are 54 years old (on the x-axis) among whom the median annual income is $63,000 (on the y-axis). Then you might say something about how median incomes trend upwards across ages.\nOnce you finish your writeup, you will upload it and write feedback for peers.\n\nWhat is especially clear in this written summary?\nAre there parts that are confusing?\nWhat might make the writeup more compelling?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "honors/sketch_a_visualization.html",
    "href": "honors/sketch_a_visualization.html",
    "title": "Sketching a visualization",
    "section": "",
    "text": "How do you begin to ask a research question of your own? One way to start is by sketching a visualization that you might like to make.\nTake a pen and paper and draw a possible visualization.\nAs you plan your graph, consider that the elements of the graph often have a temporal order. It is common that the x-axis and colors (if relevant) exist before the variable depicted on the y-axis is realized.\nYou may also consider what kind of visualization will be most compelling: bar graph, line graph, scatterplot, histogram, etc.\nPretend that you already have results and draw the whole graph. How would you label the axes?"
  },
  {
    "objectID": "honors/sketch_a_visualization.html#using-actual-variables",
    "href": "honors/sketch_a_visualization.html#using-actual-variables",
    "title": "Sketching a visualization",
    "section": "Using actual variables",
    "text": "Using actual variables\nAt some point, look through IPUMS to find data that contains the variables that would enable you to make your visualization. This may require modifying your plan in an iterative process."
  },
  {
    "objectID": "honors/sketch_a_visualization.html#exchange-graphs",
    "href": "honors/sketch_a_visualization.html#exchange-graphs",
    "title": "Sketching a visualization",
    "section": "Exchange graphs",
    "text": "Exchange graphs\nThrough an online submission, we will exchange visualization sketches and provide feedback.\n\nWhat is especially clear in this graph?\nAre there parts that are confusing?\nWhat might make the graph more compelling?"
  },
  {
    "objectID": "honors/prepare_data.html",
    "href": "honors/prepare_data.html",
    "title": "Preparing data",
    "section": "",
    "text": "If you want to download a full .qmd instead of copying codes from the website, you can use preparing_data_exercise.qmd.\nThis exercise examines how income inequality has changed over time in the U.S. We will measure inequality by the 10th, 50th, and 90th percentiles of wage and salary income from 1962 to 2022.1 The goal is to produce a graph like this one."
  },
  {
    "objectID": "honors/prepare_data.html#prepare-your-r-environment",
    "href": "honors/prepare_data.html#prepare-your-r-environment",
    "title": "Preparing data",
    "section": "Prepare your R environment",
    "text": "Prepare your R environment\nIn RStudio, create a Quarto document. Save it in your working directory. Run the following code to load two packages we will use. If you do not have these packages, first run install.packages(\"tidyverse\") and install.packages(\"haven\") to install the packages.\n\nlibrary(tidyverse)\nlibrary(haven)"
  },
  {
    "objectID": "honors/prepare_data.html#data-access",
    "href": "honors/prepare_data.html#data-access",
    "title": "Preparing data",
    "section": "Data access",
    "text": "Data access\nThis exercise uses data from the Current Population Survey, provided via IPUMS. We have two versions of the data:\n\nsimulated data made available to you via the course website\nactual data, for which you will register with the data provider\n\nIn lecture, we will use simulated data. In discussion, your TA will walk through the process to access the actual data. You will ultimately need to access the actual data for future assignments in this class.\n\nAccessing simulated data\nThe simulated data are designed to have the same statistical properties as the actual data. To access the simulated data, copy the following line of code into your R script and run it. This line loads the data and stores it in an object called cps_data.\n\n\ncps_data &lt;- read_dta(\"https://soc114.github.io/data/simulated_cps_data.dta\")\n\n\n\nAccessing actual data\nAccessing the actual data is important for future assignments. You may also use these data in your project. Here are instructions to access the data:\n\nRegister for an account at cps.ipums.org\nLog in\nClick “Get Data”\nAdd the following variables to your cart: incwage, educ, wkswork2, age, asecwt\nAdd the 1962–2023 ASEC samples to your cart. Exclude the basic monthly samples\nCreate a data extract\n\nSelect cases to only download people ages 30–45\nChoose to download in Stata (.dta) format\n\nSubmit your extract and download the data!\n\nStore your data in a working directory: a folder on your computer that will hold the data for this exercise. Load the data using the read_dta function in the haven package.\n\ncps_data &lt;- read_dta(\"your_downloaded_file_name.dta\")\n\n\n\n\n\n\n\nTip\n\n\n\n\nChange the file name to the name of the file you downloaded\nIf R says the file does not exist in your current working directory, you may need to set your working directory by clicking Session -&gt; Set Working Directory -&gt; To Source File Location on a Mac or Tools -&gt; Change Working Directory on Windows."
  },
  {
    "objectID": "honors/prepare_data.html#explore-the-data",
    "href": "honors/prepare_data.html#explore-the-data",
    "title": "Preparing data",
    "section": "Explore the data",
    "text": "Explore the data\nType cps_data in the console. Some columns such as educ have a numeric code and a label. The code is how IPUMS has stored the data. The label is what the code means. You can always find more documentation explaining the labels on the IPUMS-CPS website."
  },
  {
    "objectID": "honors/prepare_data.html#filter-to-cases-of-interest",
    "href": "honors/prepare_data.html#filter-to-cases-of-interest",
    "title": "Preparing data",
    "section": "filter() to cases of interest",
    "text": "filter() to cases of interest\n\nIn this step, you will use filter() to convert your cps_data object to a new object called filtered.\n\nThe filter() function keeps only rows in our dataset that correspond to those we want to study. The examples on the documentation page are especially helpful. The R4DS section is also helpful.\nHere are two ways to use filter() to restrict to people working 50+ weeks per year. One way is to call the filter() function and hand it two arguments\n\n.data = cps_data is the dataset\nyear == 1962 is a logical condition coded TRUE for observations in 1962\n\n\nfilter(.data = cps_data, year == 1962)\n\nThe result of this call is a tibble with only the observations from 1962. Another way to do the same operation is with the pipe operator |&gt;\n\ncps_data |&gt;\n  filter(year == 1962)\n\nThis approach begins with the data set cps_data. The pipe operator |&gt; hands this data set on as the first argument to the filter() function in the next line. As before, the second argument is the logical condition year == 1962.\nThe piping approach is often preferable because it reads like a sentence: begin with data, then filter to cases with a given condition. The pipe is also useful\nThe pipe operator |&gt; takes what is on the first line and hands it on as the first argument to the function in the next line. This reads in a sentence: begin with the cps_data tibble and then filter() to cases with year == 1962. The pipe can also string together many operations, with comments allowed between them:\n\ncps_data |&gt;\n  # Restrict to 1962\n  filter(year == 1962) |&gt;\n  # Restrict to ages 40-44\n  filter(age &gt;= 40 & age &lt;= 44)\n\nYour turn. Begin with the cps_data dataset. Filter to\n\npeople working 50+ weeks per year (check documentation for wkswork2)\nvalid report of incwage greater than 0 and less than 99999998\n\n\n\nShow the code answer\nfiltered &lt;- cps_data |&gt;\n  # Subset to cases working full year\n  filter(wkswork2 == 6) |&gt;\n  # Subset to cases with valid income\n  filter(incwage &gt; 0 & incwage &lt; 99999998)\n\n\n\n\n\n\n\n\nNote\n\n\n\nFiltering can be a dangerous business! For example, above we dropped people with missing values of income. But what if the lowest-income people refuse to answer the income question? We often have no choice but to filter to those with valid responses, but you should always read the documentation to be sure you understand who you are dropping and why."
  },
  {
    "objectID": "honors/prepare_data.html#group_by-and-summarize-for-subpopulation-summaries",
    "href": "honors/prepare_data.html#group_by-and-summarize-for-subpopulation-summaries",
    "title": "Preparing data",
    "section": "group_by() and summarize() for subpopulation summaries",
    "text": "group_by() and summarize() for subpopulation summaries\n\nIn this step, you will use group_by() and summarize() to convert your mutated object to a new object called summarized.\n\nEach row in our dataset is a person. We want a dataset where each row is a year. To get there, we will group our data by year and then summarize each group by a set of summary statistics.\n\nIntroducing summarize() with the sample mean\nTo see how summarize() works, let’s first summarize the sample mean income within each year. The input has one row per person. The result has one row per group. For each year, it records the sample mean income.\n\nfiltered |&gt;\n  group_by(year) |&gt;\n  summarize(mean_income = mean(incwage))\n\n# A tibble: 62 × 2\n    year mean_income\n   &lt;dbl&gt;       &lt;dbl&gt;\n 1  1962       6383.\n 2  1963       5831.\n 3  1964       6688.\n 4  1965       6066.\n 5  1966       6438.\n 6  1967       6745.\n 7  1968       7244.\n 8  1969       8465.\n 9  1970       9198.\n10  1971       8490.\n# ℹ 52 more rows\n\n\n\n\n\nUsing summarize() with weighted quantiles\nInstead of the mean, we plan to use three other summary statistics: the 10th, 50th, and 90th percentiles of income. We also want to incorporate the sampling weights provided with the Current Population Survey, in order to summarize the population instead of the sample.\nWe will use the wtd.quantile function to create weighted quantiles. This function is available in the Hmisc package. If you don’t have that package, install it with install.packages(\"Hmisc\"). Using the Hmisc package is tricky, because it has some functions with the same name as functions that we use in the tidyverse. Instead of loading the whole package, we will only load the functions we are using at the time we use them. Whenever we want to calculate a weighted quantile, we will call it with the code packagename::functionname() which in this case is Hmisc::wtd.quantile().\nThe wtd.quantile function will take three arguments:\n\nx is the variable to be summarized\nweights is the variable containing sampling weights\nprobs is the probability cutoffs for the quantiles. For the 10th, 50th, and 90th percentiles we want 0.1, 0.5, and 0.9.\n\nThe code below produces weighted quantile summaries.\n\nsummarized &lt;- filtered |&gt;\n  group_by(year) |&gt;\n  summarize(\n    p10 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.1),\n    p50 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.5),\n    p90 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.9),\n    .groups = \"drop\"\n  )"
  },
  {
    "objectID": "honors/prepare_data.html#pivot_longer-to-reshape-data",
    "href": "honors/prepare_data.html#pivot_longer-to-reshape-data",
    "title": "Preparing data",
    "section": "pivot_longer() to reshape data",
    "text": "pivot_longer() to reshape data\n\nIn this step, you will use pivot_longer() to convert your summarized object to a new object called pivoted. We first explain why, then explain the task.\n\nWe ultimately want to make a ggplot() where income values are placed on the y-axis. We want to plot the 10th, 50th, and 90th percentiles along this axis, distinguished by color. We need them all in one colun! But currently, they are in three columns.\nHere is the task. How our data look:\n\n\n# A tibble: 62 × 4\n   year   p10   p50    p90\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  1962 1826. 4460. 11733.\n2  1963 1770. 4484. 11934.\n# ℹ 60 more rows\n\n\nHere we want our data to look:\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10       1826.\n2  1962 p50       4460.\n3  1962 p90      11733.\n4  1963 p10       1770.\n5  1963 p50       4484.\n6  1963 p90      11934.\n# ℹ 180 more rows\n\n\nThis way, we can use year for the x-axis, quantity for color, and value for the y-axis.\nUse pivot_longer() to change the first data frame to the second.\n\nUse the cols argument to tell it which columns will disappear\nUse the names_to argument to tell R that the names of those variables will be moved to a column called quantity\nUse the values_to argument to tell R that the values of those variables will be moved to a column called income\n\nIf you get stuck, see how we did it at the end of this page.\n\n\nShow the code answer\npivoted &lt;- summarized %&gt;%\n  pivot_longer(\n    cols = c(\"p10\",\"p50\",\"p90\"),\n    names_to = \"quantity\",\n    values_to = \"income\"\n  )"
  },
  {
    "objectID": "honors/prepare_data.html#left_join-an-inflation-adjustment",
    "href": "honors/prepare_data.html#left_join-an-inflation-adjustment",
    "title": "Preparing data",
    "section": "left_join() an inflation adjustment",
    "text": "left_join() an inflation adjustment\n\nIn this step, you will use left_join() to merge in an inflation adjustment\n\nA dollar in 1962 bought a lot more than a dollar in 2022. We will adjust for inflation using the Consumer Price Index, which tracks the cost of a standard basket of market goods. We already took this index to create a file inflation.csv,\n\ninflation &lt;- read_csv(\"https://soc114.github.io/data/inflation.csv\")\n\n\n\n# A tibble: 62 × 2\n   year inflation_factor\n  &lt;dbl&gt;            &lt;dbl&gt;\n1  1962            10.1 \n2  1963             9.95\n3  1964             9.82\n# ℹ 59 more rows\n\n\nThe inflation_factor tells us that $1 in 1962 could buy about as much as $10.10 in 2023. To take a 1962 income and report it in 2023 dollars, we should multiple it by 10.1. We need to join our data\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10       1826.\n2  1962 p50       4460.\n3  1962 p90      11733.\n# ℹ 183 more rows\n\n\ntogether with inflation.csv by the linking variable year. Use left_join() to merge inflation_factor onto the dataset pivoted. Below is a hypothetical example for the structure.\n\n# Hypothetical example\njoined &lt;- data_A |&gt;\n  left_join(\n    data_B,\n    by = join_by(key_variable_in_A_and_B)\n  )\n\n\n\nShow the code answer\njoined &lt;- pivoted |&gt;\n  left_join(\n    inflation,\n    by = join_by(year)\n  )"
  },
  {
    "objectID": "honors/prepare_data.html#mutate-to-adjust-for-inflation",
    "href": "honors/prepare_data.html#mutate-to-adjust-for-inflation",
    "title": "Preparing data",
    "section": "mutate() to adjust for inflation",
    "text": "mutate() to adjust for inflation\n\nIn this step, you will use mutate() to multiple income by the inflation_factor\n\nThe mutate() function modifies columns. It can overwrite existing columns or create new columns at the right of the data set. The new variable is some transformation of the old variables.\n\n# Hypothetical example\nold_data |&gt;\n  mutate(new_variable = old_variable_1 + old_variable_2)\n\nUse mutate() to modify income so that it takes the values income * inflation_factor.\n\n\nShow the code answer\nmutated &lt;- joined |&gt;\n  mutate(income = income * inflation_factor)"
  },
  {
    "objectID": "honors/prepare_data.html#ggplot-to-visualize",
    "href": "honors/prepare_data.html#ggplot-to-visualize",
    "title": "Preparing data",
    "section": "ggplot() to visualize",
    "text": "ggplot() to visualize\nNow make a ggplot() where\n\nyear is on the x-axis\nincome is on the y-axis\nquantity is denoted by color\n\nDiscuss. What do you see in this plot?"
  },
  {
    "objectID": "honors/prepare_data.html#all-together",
    "href": "honors/prepare_data.html#all-together",
    "title": "Preparing data",
    "section": "All together",
    "text": "All together\nPutting it all together, we have a pipeline that goes from data to the plot.\n\ncps_data |&gt;\n  # Subset to cases working full year\n  filter(wkswork2 == 6) |&gt;\n  # Subset to cases with valid income\n  filter(incwage &gt; 0 & incwage &lt; 99999998) |&gt;\n  # Produce summaries\n  group_by(year) |&gt;\n  summarize(\n    p10 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.1),\n    p50 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.5),\n    p90 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.9\n    ),\n    .groups = \"drop\"\n  ) |&gt;\n  pivot_longer(\n    cols = c(\"p10\",\"p50\",\"p90\"),\n    names_to = \"quantity\",\n    values_to = \"income\"\n  ) |&gt;\n  # Join data for inflation adjustment\n  left_join(\n    read_csv(\"https://soc114.github.io/data/inflation.csv\"),\n    by = join_by(year)\n  ) |&gt;\n  # Apply the inflation adjustment\n  mutate(income = income * inflation_factor) |&gt;\n  # Produce a ggplot\n  ggplot(aes(x = year, y = income, color = quantity)) +\n  geom_line() +\n  xlab(\"Year\") +\n  scale_y_continuous(name = \"Annual Wage and Salary Income\\n(2023 dollars)\",\n                     labels = scales::label_dollar()) +\n  scale_color_discrete(name = \"Percentile of\\nDistribution\",\n                       labels = function(x) paste0(gsub(\"p\",\"\",x),\"th\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "honors/prepare_data.html#want-to-do-more",
    "href": "honors/prepare_data.html#want-to-do-more",
    "title": "Preparing data",
    "section": "Want to do more?",
    "text": "Want to do more?\nIf you have finished and want to do more, you could\n\nincorporate the educ variable in your plot. You might want to group by those who do and do not hold college degrees, perhaps using facet_grid()\ntry geom_histogram() for people’s incomes in a specific year\nexplore IPUMS-CPS for other variables and begin your own visualization\n\nNow that you have followed this guided exercise, in the next page you will learn to plan your own research question by sketching a visualization."
  },
  {
    "objectID": "honors/prepare_data.html#footnotes",
    "href": "honors/prepare_data.html#footnotes",
    "title": "Preparing data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to past TA Abby Sachar for designing the base of this exercise.↩︎"
  },
  {
    "objectID": "honors/access_data.html",
    "href": "honors/access_data.html",
    "title": "Accessing data",
    "section": "",
    "text": "We will analyze data managed by the Integrated Public Use Microdata Series (IPUMS), a project at the University of Minnesota that provides access to many survey datasets containing responses from thousands of people. IPUMS is especially helpful because it is easy to use with clear documentation.\nGet started by visiting the website: www.ipums.org\nIPUMS is an aggregate data administrator for many different data sources, each of which has its own site linked from the main site. For example, our initial activity will use IPUMS-CPS (cps.ipums.org), which contains data from the survey that the Bureau of Labor Statistics uses to estimate the unemployment rate. The next page walks through the data access process for IPUMS-CPS with a worked example.\nLater in the quarter, you may choose to use another IPUMS data source. There are many possibilities, covering topics including the labor market, education, health, and time use.\nOn the next page, practice preparing data.\n\n\n\n Back to top"
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "Discussion Meetings",
    "section": "",
    "text": "This page contains a general plan for the topics of the 10 discussion section meetings that will occur this quarter."
  },
  {
    "objectID": "discussion.html#jan-79",
    "href": "discussion.html#jan-79",
    "title": "Discussion Meetings",
    "section": "Jan 7–9",
    "text": "Jan 7–9\nIn this discussion, we will introduce ourselves and ensure our computers are prepared for the class. This will involve checking that we have R, RStudio, tinytex, and tidyverse installed. See R basics."
  },
  {
    "objectID": "discussion.html#jan-1416",
    "href": "discussion.html#jan-1416",
    "title": "Discussion Meetings",
    "section": "Jan 14–16",
    "text": "Jan 14–16\nDue to the fires, this week’s discussions became Zoom office hours."
  },
  {
    "objectID": "discussion.html#jan-2123",
    "href": "discussion.html#jan-2123",
    "title": "Discussion Meetings",
    "section": "Jan 21–23",
    "text": "Jan 21–23\nIn this discussion, you will register for an account to access the Current Population Survey data. Click here for detailed instructions.\nYour TA may walk through how to look at the survey documentation for one or more of the variables. If time allows, we will break into small groups to look for other variables that might be interesting in a project, and then come back together to tell the class about them."
  },
  {
    "objectID": "discussion.html#jan-2830",
    "href": "discussion.html#jan-2830",
    "title": "Discussion Meetings",
    "section": "Jan 28–30",
    "text": "Jan 28–30\nYou should read the following paper before this discussion:\n\nEngland, Paula, Andrew Levine, and Emma Mishel. 2020. Progress toward gender equality in the United States has slowed or stalled, PNAS 117(13):6990–6997.\n\nThe problem set reproduces findings from this paper. The discussion will focus on conceptual questions from the reading. The questions below are only guidelines, and your TA might have other topics to discuss.\n1. The authors write that “change in the gender system has been deeply asymmetric.” Explain this in a sentence or two to someone who hasn’t read the article.\n2. The authors discuss cultural changes that could lead to greater equality. Propose a question that could (hypothetically) be included in the CPS-ASEC questionnaire to help answer questions about cultural changes.\n\n\n\n\n\n\nTip\n\n\n\nIf you are not sure how to word a survey question, here are some examples from the American Time Use Survey, Current Population Survey, and General Social Survey.\n\n\n3. The authors discuss institutional changes that could lead to greater equality. Propose a question that could (hypothetically) be included in the CPS-ASEC questionnaire to help answer questions about institutional changes.\n4. What was one fact presented in this paper that most surprised you?\n5. What about the graphs in this paper is compelling? What would you improve or change if you were producing these graphs?"
  },
  {
    "objectID": "discussion.html#feb-46",
    "href": "discussion.html#feb-46",
    "title": "Discussion Meetings",
    "section": "Feb 4–6",
    "text": "Feb 4–6\nYour TA will introduce the final project, which will be carried out in groups of about 5 students within your discussion section. Your TA will help you to form groups with common interest areas.\nPossible topics include (but are not limited to) inequality by race or gender, trends in poverty or inequality over time, and the causal effect of education or other life experiences on subsequent outcomes. Another possibility is to form groups centered on a particular dataset, with the topic to be decided later.\nAll subsequent discussions are devoted to group work on the final project."
  },
  {
    "objectID": "discussion.html#feb-1113",
    "href": "discussion.html#feb-1113",
    "title": "Discussion Meetings",
    "section": "Feb 11–13",
    "text": "Feb 11–13\nBy the end of this discussion, your group should select a dataset for the final project. If you aren’t sure where to start, we suggest the options at ipums.org.\nIf you finish early, you are always welcome to move on to the tasks below."
  },
  {
    "objectID": "discussion.html#feb-1820",
    "href": "discussion.html#feb-1820",
    "title": "Discussion Meetings",
    "section": "Feb 18–20",
    "text": "Feb 18–20\nBy the end of this discussion, your group should\n\nselect an outcome variable\ndefine the unit of analysis (e.g., a person, a school, a state) for which that outcome variable is defined\ndetermine how you plan to aggregate the outcome variable across units (e.g., by a mean, median, proportion)\n\nIf you finish early, you are always welcome to move on to the tasks below."
  },
  {
    "objectID": "discussion.html#feb-2527",
    "href": "discussion.html#feb-2527",
    "title": "Discussion Meetings",
    "section": "Feb 25–27",
    "text": "Feb 25–27\nBy the end of this discussion, your group should discuss the target population and whether you will need to use sampling weights in your analysis.\nIf you finish early, you are always welcome to move on to the tasks below."
  },
  {
    "objectID": "discussion.html#mar-46",
    "href": "discussion.html#mar-46",
    "title": "Discussion Meetings",
    "section": "Mar 4–6",
    "text": "Mar 4–6\nThis discussion is open for your group to work on any remaining tasks to finish up your final project. Recall that PDF slides and writeup are due by 5pm on Mar 7, with one submission per group."
  },
  {
    "objectID": "discussion.html#mar-1113",
    "href": "discussion.html#mar-1113",
    "title": "Discussion Meetings",
    "section": "Mar 11–13",
    "text": "Mar 11–13\nYour group will present your final project in this discussion. During each presentation by a group that is not your own, you will be asked to note something you appreciated about their presentation."
  },
  {
    "objectID": "discussion.html#other-discussion-possibilities",
    "href": "discussion.html#other-discussion-possibilities",
    "title": "Discussion Meetings",
    "section": "Other discussion possibilities",
    "text": "Other discussion possibilities\nThis part of the page has other possibilities for discussion topics. We may use these in one of the discussions depending on how the course material is progressing over the quarter.\n\nExplain a ggplot function\nLast week’s lecture introduced visualization with the ggplot() function in the tidyverse package. This discussion explores more of what ggplot() can do.\n\nForm groups of about 4 students\nIntroduce yourself to your group members\nChoose a graph that your group likes in Ch 1 of R For Data Science.\n\nLook at the code for that graph\nDiscuss what each element of the code is doing\nChoose someone to present to the class\n\nAt the end of discussion, we will regroup. Each group will present the code for their chosen graph to the class\n\n\n\nExplain a tidyverse function\nThis discussion explores more of what the tidyverse can do.\n\nForm groups of about 4 students\nIntroduce yourself to your group members\nChoose a function in Ch 3 of R For Data Science that we did not cover explicitly in class.\n\nPossibilities include: arrange(), distinct(), rename(), relocate(), the slice_ functions, ungroup()\nFind an example of the function in the chapter.\nDiscuss how the code works.\nPrepare to explain to the class.\n\nAt the end of discussion, we will regroup. Each group will present the code for their chosen graph to the class\n\n\n\nWrite a function\nNow that we have worked with functions in R, it is time to understand them on a deeper level. In this exercise, we will write our own functions.\n\nA basic function\nYou can store a function as an object in your environment, just like any other object. The function below accepts a numeric variable x and returns twice the value of x.\n\ndouble_x &lt;- function(x) {\n  doubled &lt;- 2 * x\n  return(doubled)\n}\n\nThere are a few pieces to the code above\n\nWe created a function that has a name: double_x\nOur function that takes one argument named x\nThe body of the function is the lines within the {}. These lines take the argument, do some things, and then return() a result. The object within return() is what R sends back after running the function.\n\nOnce you create that function, you can run it just like any other function.\n\ndouble_x(x = 2)\n\n[1] 4\n\n\n\n\nA function with two arguments\nA function can also take multiple arguments, such as one that adds x and y.\n\nadd_x_y &lt;- function(x, y) {\n  added &lt;- x + y\n  return(added)\n}\n\nwhich works as follows.\n\nadd_x_y(x = 2, y = 3)\n\n[1] 5\n\n\n\n\nChallenge: Write your own function\nA function does not have to just take numbers as an argument. It can also take a dataset as an argument. Sometimes, we might want an estimator(data) function that takes data as an argument and applies an estimator() to that data, to return an estimate of something. You will create one such function here.\nAs an example, suppose three surveys ask people if they prefer chocolate ice cream (prefers_chocolate = TRUE) or vanilla ice cream (prefers_chocolate = FALSE). The survey also records whether the respondent is a child age = \"child\" or an adult age = \"adult\".\n\nlibrary(tidyverse)\nsample_a &lt;- tibble(\n  age = c(\"child\",\"child\",\"child\",\"adult\",\"adult\"),\n  prefers_chocolate = c(T,T,F,F,F)\n)\nsample_b &lt;- tibble(\n  age = c(\"child\",\"child\",\"adult\",\"adult\"),\n  prefers_chocolate = c(T,F,T,F)\n)\nsample_c &lt;- tibble(\n  age = c(\"child\",\"child\",\"child\",\"adult\",\"adult\",\"adult\"),\n  prefers_chocolate = c(T,T,F,F,F,T)\n)\n\nWe want to know the proportion preferring chocolate among children and adults in each sample. To estimate in sample_a, we would write\n\nestimate &lt;- sample_a |&gt;\n  group_by(age) |&gt;\n  summarize(prefers_chocolate = mean(prefers_chocolate))\n\nNow write that within a function.\n\nestimator &lt;- function(data) {\n  # do some things to data\n  # return() your estimate\n}\n\nand apply the estimator to sample_a, sample_b, and sample_c."
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "Assignment due dates will be visible in BruinLearn and Gradescope. They follow a pattern:\n\nM 5pm: Post-lecture quiz due\nW 5pm: Post-lecture quiz due\nF 5pm: Problem set due"
  },
  {
    "objectID": "calendar.html#assignments",
    "href": "calendar.html#assignments",
    "title": "Calendar",
    "section": "",
    "text": "Assignment due dates will be visible in BruinLearn and Gradescope. They follow a pattern:\n\nM 5pm: Post-lecture quiz due\nW 5pm: Post-lecture quiz due\nF 5pm: Problem set due"
  },
  {
    "objectID": "calendar.html#lecture-topics",
    "href": "calendar.html#lecture-topics",
    "title": "Calendar",
    "section": "Lecture topics",
    "text": "Lecture topics\nBelow is a tentative plan for the dates each topic will be covered. If you are absent, you can review the corresponding course page to learn the material.\n\nSchedule of topics\n\n\nDate\nWebsite Page\n\n\n\n\n1/5\nResearch Questions in Social Data Science\n\n\n\nSoftware Prerequisites\n\n\n\nBasics of R\n\n\n1/7\nVisualizing a Distribution\n\n\n\nSummary Statistics\n\n\n1/12\nPopulation Sampling\n\n\n1/14\nConfidence Intervals\n\n\n1/19\nNo class: Martin Luther King, Jr. Day\n\n\n\nModels for Subgroup Summaries\n\n\n1/21\nLinear Regression\n\n\n1/26\nLogistic Regression\n\n\n1/28\nForests\n\n\n2/2\nData-Driven Estimator Selection\n\n\n2/4\nAre Complex Models Better?\n\n\n\nCausal Inference under Measured Confounding\n\n\n2/9\nDefining Causal Effects\n\n\n2/11\nExchangeability\n\n\n2/16\nNo class: Presidents’ Day\n\n\n2/18\nDirected Acyclic Graphs\n\n\n2/23\nMatching\n\n\n2/25\nModels for Causal Inference\n\n\n\nCausal Inference with Unmeasured Confounding\n\n\n3/2\nDifference in Difference\n\n\n3/4\nRegression Discontinuity\n\n\n3/9\nInstrumental Variables\n\n\n3/11\nCourse Recap"
  },
  {
    "objectID": "forms.html",
    "href": "forms.html",
    "title": "Forms",
    "section": "",
    "text": "Assignment extension form. If you are experiencing an exceptional circumstance that may warrant an extension, use this to request one.\nRegrade request form. If you think we have graded something wrong, you can report it on this form. Responses are only accepted for within 72 hours of grade post.\nExcused absence form. To request an excused absence from lecture or discussion.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "honors/ask_good_questions.html",
    "href": "honors/ask_good_questions.html",
    "title": "Asking questions with data",
    "section": "",
    "text": "slides\nAsking your own research question is fun, and it is also daunting. There is no formula for asking a good question, and much research involves trial and error. In this lecture, we will discuss together what makes a good research question."
  },
  {
    "objectID": "honors/ask_good_questions.html#some-characteristics-of-a-good-question",
    "href": "honors/ask_good_questions.html#some-characteristics-of-a-good-question",
    "title": "Asking questions with data",
    "section": "Some characteristics of a good question",
    "text": "Some characteristics of a good question\nWhile good questions come in many forms, good research questions often has a few characteristics.\n\nA unit of analysis\nThe unit of analysis is the bedrock of a good research question. What will a row of your dataset represent? While the answer may seem straightforward, many units of analysis are possible: a person, a school, a neighborhood, a country, etc. In settings with repeated observations, the unit of analysis might be a person observed at a particular age: Jose at age 29 may be one row of your dataset and Jose at age 30 may be another row. The unit of analysis is the unit at which the outcome is defined, so getting it right is important.\n\n\nA clear population\nA good research question addresses a population that is relevant to the author’s argument. A well-defined population could be a set of units, such as American residents in a certain age range. It could be the set of all students at Cornell. It could also be a set of aggregate entities, such as the population of all four-year colleges and universities in the U.S., where each unit in the population is a college or university.\nWhen the population is well-defined, a reader should be able to imagine making a list of all the units in that population. When presented with a new unit and asked whether that unit is part of the population, you should be able to confidently answer “yes” or “no” and not “maybe.”\nIt is rarely satisfying to answer the question “who is being studied?” with the answer “the people in my sample.” While full-count enumerations of interesting populations exist, usually your sample is a subset of a broader population of interest. Good research tells us who that population is.\n\n\nA precise outcome\nFor each unit in the population, there is an outcome. This might be the income of an individual person, or the graduation rate of an individual college. A good research question tells you what the outcome is, and why it matters.\n\n\nThe potential for surprising results\nThis may be the hardest to pin down. Before you work with data, try to tell a story about why the results might go one way or another way. Perhaps you study a particular population of employed people where there is one set of reasons to expect men to earn more and another set of reasons to expect women to earn more. Try to convince yourself that the results could go either way! This builds excitement about the empirical answer, because it shows that the answer is really unknown before data analysis. Write down your arguments because they will also help readers be excited about your questions.\nQuestions where only one answer is plausible are often tedious. Questions that are exciting are often the ones where results could surprise you by going several possible directions."
  },
  {
    "objectID": "honors/presentation.html",
    "href": "honors/presentation.html",
    "title": "Presenting orally",
    "section": "",
    "text": "Your oral presentation is your chance to tell us about your project. We will discuss some tips that can make for a compelling oral presentation.\n\nPlan ahead for each person to say a part\nYour oral presentation may mirror your writeup, but you should not be reading it\nStand with your feet planted and face the group\nEye contact is a great way to connect with the room while you talk\n\nSome things you might tell us include:\n\nWhat motivates this particular question?\nHow did you choose your data source and population to study?\nHow should we read a point in your graph?\nHow should we read the general trend?\nWhat is your overall finding?\n\nDon’t stress about the presentation. We all want everyone to succeed, and the presentation is a way to celebrate the research we have accomplished this quarter.\n\n\n\n Back to top"
  },
  {
    "objectID": "honors/welcome.html",
    "href": "honors/welcome.html",
    "title": "Welcome to the Honors Section",
    "section": "",
    "text": "See the honors section syllabus.\nAny student enrolled in Soc 114 is welcome to enroll in the honors section (Soc 189 Sem 3). The honors section is graded as a separate 1.0 credit course. It is an opportunity to engage in a small creative research project and interact with the professor and peers in a seminar format."
  },
  {
    "objectID": "honors/welcome.html#our-goal",
    "href": "honors/welcome.html#our-goal",
    "title": "Welcome to the Honors Section",
    "section": "Our goal",
    "text": "Our goal\nOur goal in the honors section is to:\n\ncreate our own social science questions\ndownload actual data relevant to the questions\nuse the tools of data science to produce answers\npresent findings orally and in writing"
  },
  {
    "objectID": "honors/welcome.html#what-comes-next",
    "href": "honors/welcome.html#what-comes-next",
    "title": "Welcome to the Honors Section",
    "section": "What comes next",
    "text": "What comes next\n\nWeek 1: Discuss how to ask good research questions and access data\nWeek 2–3: Prepare data with a guided activity using tidyverse tools\nWeek 4: Plan a possible visualization by drawing a sketch\nWeek 5–6: In-class work to produce your visualization\nWeek 7: Practice how to write results\nWeek 8: Plan to present findings\nWeek 9–10: Presentations in class"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Science",
    "section": "",
    "text": "Together, we will use tools from data science to answer social science questions. As an area of application, we will focus on questions about inequality and social stratification.",
    "crumbs": [
      "Syllabus",
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Social Data Science",
    "section": "Learning goals",
    "text": "Learning goals\nAs a result of participating in this course, students will be able to\n\nconnect theories about inequality to quantitative empirical evidence\nevaluate the effects of hypothetical interventions to reduce inequality\nconduct data analysis using the R programming language",
    "crumbs": [
      "Syllabus",
      "Home"
    ]
  },
  {
    "objectID": "index.html#team",
    "href": "index.html#team",
    "title": "Social Data Science",
    "section": "Team",
    "text": "Team\n\n\n\n\n\nIan Lundberg\nianlundberg@ucla.edu\n(he / him)\nWorking with data to understand inequality brings me joy and meaning, as I first discovered as a college student years ago. I hope to share that joy with you! Other joys of mine include hiking, surfing, and oatmeal with blueberries.\n\n\n\n\n\nTaylor Aquino\ntaquino7@g.ucla.edu\n(she / her)\nI use data to understand the experiences of multiracial people in the U.S., specifically their occupational attainment and dating outcomes. My hobbies include cooking, baking, and watching tv with my dog, Blue.",
    "crumbs": [
      "Syllabus",
      "Home"
    ]
  },
  {
    "objectID": "psets/learning_exercise.html",
    "href": "psets/learning_exercise.html",
    "title": "Learning Exercise",
    "section": "",
    "text": "Gender inequality in employment is much greater among new parents than among non-parents. This exercise seeks to estimate the proportion employed among married men and women1 with a 1-year-old child at home. Our data include those with at least one child age 0–18."
  },
  {
    "objectID": "psets/learning_exercise.html#synthetic-data",
    "href": "psets/learning_exercise.html#synthetic-data",
    "title": "Learning Exercise",
    "section": "Synthetic data",
    "text": "Synthetic data\nTo speed data access, we downloaded data from the basic monthly Current Population Survey for all months from 2010–2019. We processed these data, grouped by sex and age of the youngest child, and estimated the proportion employed. We then generated synthetic data: we created a new dataset for you to use with simulated people using these known probabilities.\nSynthetic data is good in our setting for two reasons\n\nwe know the answer\nyou can download the synthetic data right from this website\n\nFor transparency, here is the code with which we created the synthetic data. The line below will load the synthetic data.\n\nparents &lt;- read_csv(\"https://info3370.github.io/data/parents.csv\")\n\nYour synthetic data intentionally omits any parents with child age 1! Here is a graph showing the averages in your data, grouped by child age and sex."
  },
  {
    "objectID": "psets/learning_exercise.html#your-task-predict-at-child-age-1",
    "href": "psets/learning_exercise.html#your-task-predict-at-child-age-1",
    "title": "Learning Exercise",
    "section": "Your task: Predict at child age 1",
    "text": "Your task: Predict at child age 1\nYour task is to answer the question: what proportion are employed among female respondents whose youngest child is 1 year old?\n\nyou can use a model with the other ages\nyou can use strategies from the statistical learning page\nyou can use the male respondents if you think they are helpful\n\nNear the end of discussion, we will ask every table to make one estimate. Then we will reveal the truth and see who is closest!"
  },
  {
    "objectID": "psets/learning_exercise.html#one-approach-to-the-task",
    "href": "psets/learning_exercise.html#one-approach-to-the-task",
    "title": "Learning Exercise",
    "section": "One approach to the task",
    "text": "One approach to the task\nOne approach is to estimate a linear regression model with child_age interacted with sex. We would first create a fitted model object,\n\nmodel &lt;- lm(at_work ~ sex * child_age, data = parents)\n\nthen define the target population to predict\n\ntarget_population &lt;- tibble(sex = \"female\", child_age = 0)\n\nand report a predicted value for the employment rate of female respondents with a 1-year-old youngest child.\n\npredict(model, newdata = target_population)\n\n        1 \n0.5468421"
  },
  {
    "objectID": "psets/learning_exercise.html#footnotes",
    "href": "psets/learning_exercise.html#footnotes",
    "title": "Learning Exercise",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEach married pair need not be of different sex. The data include same-sex couples.↩︎"
  },
  {
    "objectID": "psets/project.html",
    "href": "psets/project.html",
    "title": "Final Project",
    "section": "",
    "text": "The culmination of the course is a group research project. You will\nYou can answer any question using the ideas we learned in this course."
  },
  {
    "objectID": "psets/project.html#what-you-will-submit",
    "href": "psets/project.html#what-you-will-submit",
    "title": "Final Project",
    "section": "What you will submit",
    "text": "What you will submit\nThere are assignments on BruinLearn. Each will take one submission per group.\nDraft Writeup. Due 5pm Fri Mar 7. Worth 0 points, but an opportunity to send us your work and get feedback from the teaching team. Format is the same as your writeup (see below).\nSlides. Due 5pm Mon Mar 10. A PDF that you will present in discussion. Keep text to a minimum in the slides. You should plan to have all group members speak during the presentation. Your presentation should be 10 minutes or less.\nWriteup. Due 5pm Fri Mar 14. A .qmd document compiled into a PDF. It should include all code that produces your results. This must be no more than 1,000 words and will contain 1 or more visualizations."
  },
  {
    "objectID": "psets/project.html#group-structure",
    "href": "psets/project.html#group-structure",
    "title": "Final Project",
    "section": "Group structure",
    "text": "Group structure\nWe will form groups of about 5 students each within discussion sections. Groups will be formed during discussion in the middle of the quarter."
  },
  {
    "objectID": "psets/project.html#key-components-of-the-project",
    "href": "psets/project.html#key-components-of-the-project",
    "title": "Final Project",
    "section": "Key components of the project",
    "text": "Key components of the project\nYour goal should be to tell us a story using the data. What do we learn by studying this outcome, aggregated this way, in these subgroups from this population?\nA few points should be part of your writeup:\n\nDefine your unit of analysis\nDefine your target population\n\nMotivate: why is this population interesting to study?\n\nDescribe how your sample was chosen from that population\n\nthis may be a probability sample, such as those available via IPUMS. If so, tell us a little bit about the sampling design\nthis may be a convenience sample. If so, why does it speak to the population and what are the limitations?\nthis may be data on the entire population, as in our baseball example\n\nChoose an outcome variable, which is defined for each unit in the population.\n\nexample: annual wage and salary income\n\nChoose one or more variables on which to create population subgroups\n\nexample: subgroups defined by sex (male and female)\n\nChoose a summary statistic, which aggregates the outcome distribution to one summary per subgroup\n\nexamples: proportion, mean, median, 90th percentile\n\nAnswer one descriptive and one causal question (see below)\nPresent results using ggplot()\n\nWith these components in mind, you will answer one descriptive and one causal research question.\nDescriptive research question. For this question, write your results carefully using only descriptive language. As a heuristic, if your goal is descriptive then you should not be using the sentence structure “X [verb] Y”, such as “going to college increases earnings.” This claim suggests a college graduate would have earned less if they had not gone to college—a counterfactual outcome we did not see. For truly descriptive claims, you should phrase more like: “There is a difference in earnings among those who did and did not go to college.” A heuristic to recognize a non-causal claim is that it can be phrased it in an “among” statement: “Among subgroup A, we find ___. Among subgroup B, we find ___.” Or “There is a disparity in Y across subgroups defined by X.”\nCausal research question. Define your potential outcomes. Draw a DAG in which those potential outcomes can be identified by an assumption of conditional exchangeability. If there are unmeasured confounders, discuss how their omission from your DAG may threaten the credibility of your estimates. It is ok if the assumptions are doubtful as long as you write them down clearly."
  },
  {
    "objectID": "psets/project.html#considerations-to-bear-in-mind",
    "href": "psets/project.html#considerations-to-bear-in-mind",
    "title": "Final Project",
    "section": "Considerations to bear in mind",
    "text": "Considerations to bear in mind\n\nWeights. If your sample is drawn from the population with unequal probabilities, you should use sampling weights\nModels. If your question involves many subgroups (e.g., ages) with few observations in each subgroup, you can (but are not required to) use a statistical model to estimate your summary statistic in the subgroup by a predicted value. For example, you could use OLS to predict the proportion mean income at each age. If you do this, you should report the predicted value of the summary statistic, not the coefficients of the model.\nAggregation. Your data must begin with units (e.g., people) who you aggregate into subgroups (e.g., age groups). Your data might come pre-aggregated, such as data where each row contains data for all students in a particular college or university. Then you would need to aggregate further, such as to produce summaries for private versus public universities.\nDropped cases. As you move from raw data to the data that produce your graph, you might drop cases on the way. For example, some cases may have missing values on key predictors. Report how many are dropped, and why. Our goal here is transparent, open science."
  },
  {
    "objectID": "psets/project.html#rubrics",
    "href": "psets/project.html#rubrics",
    "title": "Final Project",
    "section": "Rubrics",
    "text": "Rubrics\n\nWriteup rubric\n\nHere are slides for discussion of the writeup rubric.\n\n\n\n\n\n\n\n\nCriteria\nPoints\n\n\n\n\nThe descriptive research question is well motivated, feasible, and avoids causal language.\n10 pts\n\n\nThe causal research question is well motivated, feasible, and states appropriate causal assumptions.\n10 pts\n\n\nUnit of analysis is defined clearly\n5 pts\n\n\nTarget population is defined clearly\n5 pts\n\n\nPredictor(s) are defined clearly\n5 pts\n\n\nOutcome is defined clearly\n5 pts\n\n\nSummary statistic is defined clearly (e.g., mean, median, proportion)\n5 pts\n\n\nData source is described\n5 pts\n\n\nSample restrictions are precisely stated, with the number of cases dropped at each step\n10 pts\n\n\nWeights are used appropriately, if applicable\n5 pts\n\n\nFigure(s) are clearly labeled\n10 pts\n\n\nText explains and interprets the figure(s)\n10 pts\n\n\nWriting is concise and grammatically correct\n5 pts\n\n\nCode is well-formatted, well-documented, and easy to follow\n5 pts\n\n\nCode lines fit on the PDF page\n5 pts\n\n\nTotal\n100 pts\n\n\n\n\n\nPresentation rubric\n\n\n\n\n\n\n\nCriteria\nPoints\n\n\n\n\nSlides are a PDF submitted in Canvas\n5 pts\n\n\nSlides use minimal text\n5 pts\n\n\nPresentation motivates the research question\n5 pts\n\n\nPresentation states the question precisely\n5 pts\n\n\nPresentation walks through how to read the graph\n5 pts\n\n\nAll group members speak (excused absences will not be penalized)\n5 pts\n\n\nPresentation finishes on time\n5 pts\n\n\nGroup listens to classmates and answers questions clearly\n5 pts\n\n\nTotal\n40 pts\n\n\n\n\n\nWithin-group peer evaluation\nWe want to learn about your experience working in your group, for two reasons.\n\nWe want to assess for the future how this group project went.\nAs a secondary goal, in the rare case that a group member was consistently uninvolved, we want to assess some percentage penalty on that group member’s score. We only anticipate doing this in rare cases.\n\nAfter the project has been submitted, we will ask everyone to complete this within-group peer evaluation form once to tell us about the contributions of each member of their group. We have taken this rubric from a website by the Center for Teaching Innovation at Cornell University."
  },
  {
    "objectID": "psets/project.html#have-fun",
    "href": "psets/project.html#have-fun",
    "title": "Final Project",
    "section": "Have fun",
    "text": "Have fun\nAs a teaching team, the project is our favorite part of the course. Preparing you to succeed in the project has been (in some sense) the entire goal of all that precedes the project in the course. We hope you will find joy in answering questions with data, as we do."
  },
  {
    "objectID": "psets/pset2.html",
    "href": "psets/pset2.html",
    "title": "Problem Set 2: Visualization",
    "section": "",
    "text": "This problem set draws on the following paper on gender1 equality.\nYour task is to produce a figure similar to Figure 1 from the paper.\nWe have simulated data ready for you! We downloaded data from the 1962–2024 Annual Social and Economic Supplement of the Current Population Survey. We then produced estimates and simulated new hypothetical data to share here. Our simulated data has 100 fictional people per sex \\(\\times\\) year subgroup. The simulated data are in gender_gap.csv.\nThis code will load the data in R:\nlibrary(tidyverse)\ngender_gap &lt;- read_csv(\"https://soc114.github.io/data/gender_gap.csv\")"
  },
  {
    "objectID": "psets/pset2.html#summarize-the-data",
    "href": "psets/pset2.html#summarize-the-data",
    "title": "Problem Set 2: Visualization",
    "section": "Summarize the data",
    "text": "Summarize the data\nYour first task is to aggregate the data to summary statistics. For help, see R4DS Ch 3.5.\n\nyour input is gender_gap, which has 126,000 rows\nyou will produce an object named summarized, which has\n\n126 rows (one for each sex \\(\\times\\) year subgroup)\n3 columns: sex, year, and estimate\nThe column estimate is the column you will create, which will contain the summary statistic (proportion employed) in each subgroup"
  },
  {
    "objectID": "psets/pset2.html#visualize",
    "href": "psets/pset2.html#visualize",
    "title": "Problem Set 2: Visualization",
    "section": "Visualize",
    "text": "Visualize\nYour second task is to visualize. For help with this part, see R4DS Ch 1. Ideally, the input to this part is the summarized object you created above. In case you did not succeed at the first part, here is another summarized file you could use.\n\n# Note: Only use this code if you did not succeed at the first part.\nsummarized &lt;- read_csv(\"https://soc114.github.io/data/gender_summarized.csv\")\n\nUsing your summarized data, create the graph.\nIn your graph, use labs() to:\n\nmodify x- and y-axis titles * The y-axis title should match its values. If your graph has values between 0 and 1, your summary statistic is a proportion. If your graph has values between 0 and 100, your summary statistic is a percent.\nadd a plot title\nadd a caption\n\nFor this question, none of the above should stay at their default values. Choose readable statements that help to clarify what is in the plot. (You Can Use Title Case) or (You can use sentence case). It should not be all lower case.\nThen choose at least one of the following:\n\nchange the theme of the graph (making sure it retains the elements above)\nchange the colors of the lines\nuse linetype in addition to color to distinguish groups\n\nStore your graph in an object called my_plot. This is so we can grade it. The code below gives an example.\n\nmy_plot &lt;- ggplot(\n  data = gender_gap,\n  mapping = aes()\n)\n\nNote that when you store a graph in an object, R does not print the graph. To see the graph you’ve stored, run this command.\n\nprint(my_plot)\n\nWe recommend first building your plot (without storing it in my_plot) and then adding the storage part only at the end when you are happy. This way you will always be viewing your current plot.\nTest your code before submitting. To make sure your code will run from top to bottom, test it in a clean environment (where you have no packages loaded and no objects existing). One way to do this in RStudio is to click the the down arrow next to the Source button at the top of the script, then click “Source as Background Job.” Then click “Start.” The “Background Jobs” pane will open by your Console, and it will show errors if they appear."
  },
  {
    "objectID": "psets/pset2.html#footnotes",
    "href": "psets/pset2.html#footnotes",
    "title": "Problem Set 2: Visualization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe measure of gender is limited in this study. In social science, sex typically refers to categories assigned at birth (e.g., female, male) while gender is a performed construct with many possible values: man, woman, non-binary, etc. Survey data often contain a variable named “sex” containing the values “male” or “female.” When studying sex/gender disparities over decades of history, this binary categorization is often the only measure available. Researchers often refer to them as “gender” because they are the best available measure that approximates gender. We expect improved measures of gender to become available in the future as quantitative research improves.↩︎"
  },
  {
    "objectID": "psets/pset3_old.html",
    "href": "psets/pset3_old.html",
    "title": "Problem Set 3: Causal Inference",
    "section": "",
    "text": "Due: 5pm on Friday, February 14.\nStudent identifer: [type your anonymous identifier here]\nThis problem set is based on:\nBertrand, M & Mullainathan, S. 2004. “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” American Economic Review 94(4):991–1013.\nRead the first 10 pages of the paper (through the end of section 2). In this paper,"
  },
  {
    "objectID": "psets/pset3_old.html#analyzing-the-experimental-data-25-points",
    "href": "psets/pset3_old.html#analyzing-the-experimental-data-25-points",
    "title": "Problem Set 3: Causal Inference",
    "section": "Analyzing the experimental data (25 points)",
    "text": "Analyzing the experimental data (25 points)\nLoad packages that our code will use.\n\nlibrary(tidyverse)\nlibrary(haven)\n\nDownload the study’s data from OpenICPSR: https://www.openicpsr.org/openicpsr/project/116023/version/V1/view. This will require creating an account and agreeing to terms for using the data ethically. Put the data in the folder on your computer where this .Rmd is located. Read the data into R using read_dta.\n\nd &lt;- read_dta(\"lakisha_aer.dta\")\n\n\nIf you have an error, you might need to set your working directory first. This tells R where to look for data files. At the top of RStudio, click Session -&gt; Set Working Directory -&gt; To Source File Location.\n\nYou will now see d in your Global Environment at the top right of RStudio.\nWe will use two variables:\n\n\n\nName\nRole\nValues\n\n\n\n\ncall\noutcome\n1 if resume submission yielded a callback\n\n\n\n\n0 if not\n\n\nrace\ncategory of treatments\nb if first name signals Black\n\n\n\n\nw if first name signals white\n\n\n\nThe top of Table 1 reports callback rates: 9.65% for white names and 6.45% for Black names. Reproduce those numbers. Write code that reproduces these numbers."
  },
  {
    "objectID": "psets/pset3_old.html#causal-inference-concepts",
    "href": "psets/pset3_old.html#causal-inference-concepts",
    "title": "Problem Set 3: Causal Inference",
    "section": "Causal inference concepts",
    "text": "Causal inference concepts\n2.1. (5 points) Fundamental problem \nOne submitted resume had the name “Emily Baker.” It yielded a callback. The same resume could have had the name “Lakisha Washington.” Explain how the Fundamental Problem of Causal Inference applies to this case (1–2 sentences).\n2.2. (5 points) Potential outcomes. Using math, write the following in potential outcomes notation: resume submission 5 would receive a callback if it signaled the name “Emily Baker”. (There are several ways to write a correct answer.) You can either type math or include a picture of handwritten math. See the bottom of this page for help.\n2.3. (5 points) Exchangeability \nIn a sentence, what is the exchangeability assumption in this study? For concreteness, for this question you may suppose that the only names in the study were “Emily Baker” and “Lakisha Washington.” Be sure to explicitly state the treatment and the potential outcomes.\n2.4. (5 points) Observational study\nSuppose that instead of randomly assigning names to fictitious resumes, the authors had instead analyzed real job applications by people named “Emily Baker” and “Lakisha Washington.” Use mathematical notation with a conditional probability \\(P(\\text{A}\\mid \\text{B})\\) to state the following: the probability of being called back was higher among resumes from Emily Baker than among resumes from Lakisha Washington.\n2.5 (5 points) Exchangeability violated\nThe descriptive estimand in the observational study (2.4) is different than the causal estimand in the experimental study (2.2). Suppose the researchers wanted to use the observational study to learn about the quantity in (2.2). Explain one way the exchangeability assumption would be violated."
  },
  {
    "objectID": "psets/pset3_old.html#how-to-type-math",
    "href": "psets/pset3_old.html#how-to-type-math",
    "title": "Problem Set 3: Causal Inference",
    "section": "How to type math",
    "text": "How to type math\nThere are two ways to type math within your .qmd file, outside of a code chunk.\n\nTo type in-line math surround your math in a single $ at each side. Typing $X = 1$ will produce \\(X = 1\\).\nTo type math that goes on its own equation line, use $$ before and after the math. Typing $$X = 1$$ will produce \\[ X = 1\\]\n\nWhen typing math, there are a few things you will want to know.\n\n_ indicates the start of a subscript: $Y_i$ becomes \\(Y_i\\)\n^ indicates the start of a superscript: $Y^a$ becomes \\(Y^a\\)\n{} will let you use several characters in a subscript or superscript: $Y_{unit}^{treatment}$becomes \\(Y_{unit}^{treatment}\\)\n\\mid is the vertical conditioning bar: $E(Y\\mid X)$ becomes \\(E(Y\\mid X)\\)."
  },
  {
    "objectID": "psets/pset3_old.html#how-to-include-hand-written-math",
    "href": "psets/pset3_old.html#how-to-include-hand-written-math",
    "title": "Problem Set 3: Causal Inference",
    "section": "How to include hand-written math",
    "text": "How to include hand-written math\nYou are also welcome to handwrite math and take a picture of it. Put your picture in the folder where your .qmd document is located. To include the picture in your writeup, type\n![](NameOfYourPictureFile.png)\n(remove the backticks before and after to use this line)"
  },
  {
    "objectID": "psets/pset4_old.html",
    "href": "psets/pset4_old.html",
    "title": "Problem Set 4: DAGs and Statistical Learning",
    "section": "",
    "text": "Due: 5pm on Friday, Feb 28.\nStudent identifier: [type your anonymous identifier here]\nThe format of this problem set is different from the others.\nThe reason for this is that we are all busy with the final project! So you can have time for the project, there will be no peer review on this problem set. So the TAs can focus on helping with the project, some grading will be done automatically via the BruinLearn quiz.\nHere is how to do this problem set:"
  },
  {
    "objectID": "psets/pset4_old.html#points-dags",
    "href": "psets/pset4_old.html#points-dags",
    "title": "Problem Set 4: DAGs and Statistical Learning",
    "section": "1. (30 points) DAGs",
    "text": "1. (30 points) DAGs\nFor 1.1–1.5, answer True or False: \\(X\\) is a sufficient adjustment set to identify the causal effect of \\(A\\) on \\(Y\\). Recall that as you work on these problems, a good strategy is to first list all non-causal paths between \\(A\\) and \\(Y\\) and then cross out any that are blocked when conditioning on \\(X\\).\n\n1.1. [answer here]\n1.2. [answer here]\n1.3. [answer here]\n1.4. [answer here]\n1.5. [answer here]"
  },
  {
    "objectID": "psets/pset4_old.html#causal-inference-with-statistical-modeling",
    "href": "psets/pset4_old.html#causal-inference-with-statistical-modeling",
    "title": "Problem Set 4: DAGs and Statistical Learning",
    "section": "2. Causal inference with statistical modeling",
    "text": "2. Causal inference with statistical modeling\n\nThe paragraphs below introduce this part of the problem set. Your work begins at “Prepare your data.”\n\nHow does parenthood affect labor market outcomes? For an outcome \\(Y\\) such as employment, we can imagine that each person \\(i\\) has a potential outcome as a parent \\(Y_i^1\\) and a potential outcome as a non-parent, \\(Y_i^0\\). Parenthood casually shapes an outcome like employment to the degree that these differ.\nThe effect of parenthood on labor market outcomes has been the subject of extensive social science research which has revealed a consistent finding: parenthood may improve men’s labor market outcomes while harming women’s labor market outcomes (e.g., Waldfogel 1998, Budig & England 2001, Correll et al. 2007). The disparate effects of parenthood for men and women are thus one source of gender disparities in labor market outcomes.\nThis problem set estimates the causal effect of motherhood on mothers’ employment, using data simulated to approximate data that exist in the National Longitudinal Survey of Youth 1997 cohort. The NLSY97 interviews people repeatedly across years. We manipulated these data so that each row contains information from a pre- and a post-observation, separated by 21+ months. In the pre-observation, we measure confounding variables. In the post-observation, we measure the outcome (y, employment). Between the pre- and post-observation, some women experience a first birth (treated == TRUE) and others do not (treated == FALSE).\n\nThe dataset motherhood_simulated.csv contains the following variables.\n\nobservation_id is an index for each observation\nsampling_weight is the weight due to unequal probability sampling\ntreated indicates a first birth (TRUE or FALSE)\n\nThis occurred between the pre- and post-periods.\n\ny is the outcome, coded TRUE if employed or FALSE if not employed.\n\nThis was measured in the post-period.\n\n\nThe data include a set of variables measured in the pre-period. We will consider these to be a sufficient adjustment set. These were measured in the pre-period.\n\nrace is a categorical variable coded Hispanic, Non-Hispanic Black, and Non-Hispanic Non-Black\npre_age is age in years\npre_educ is an ordinal variable for educational attainment, coded Less than high school, High school, 2-year degree, and 4-year degree with those with higher levels of education also coded in this last category\npre_marital is a categorical variable of marital status, coded no_partner, cohabiting, or married\npre_employed is a lag measure of employment in the prior survey wave, coded TRUE and FALSE\npre_fulltime indicates full-time employment in the prior survey wave, coded TRUE and FALSE\npre_tenure is years of experience with a current employer, as of the prior survey wave\npre_experience is total years of full-time work experience, as of the prior survey wave\n\n\nlibrary(tidyverse)\nmotherhood_simulated &lt;- read_csv(\"https://soc114.github.io/data/motherhood_simulated.csv\")\n\n\nPrepare your data\nFilter to create two data objects: one with mothers who have treated == TRUE and one with non-mothers who have treated == FALSE.\n\n# your code here\n\n\n\nEstimate by linear model predictions\nAmong non-mothers, model the probability of employment with a linear model. As predictors, use an additive function of the sufficient adjustment set.\nHints:\n\nUse the lm() function.\nuse this model formula: y ~ race + pre_age + pre_educ + pre_marital + pre_employed + pre_fulltime + pre_tenure + pre_experience\nfor the data argument, use your data containing non-mothers.\nyou will need the argument weights = sampling_weight to specify to weight the model by the sampling_weight variable\n\n\n# your code here\n\n\n\n2.1. (10 points) Report a predicted value\nUsing your model estimated among non-mothers, make predictions of \\(\\hat{Y}^0\\) among mothers. Report the predicted value for the first mother in the data.\nHints:\n\nUse predict() to make predictions.\nWe suggest your store the variables in a new variable in your dataset using mutate().\nTo see the first predicted value in your predicted data, one strategy is to use select() to keep only the variable you’ve created that contains your predicted value.\n\n\n# your code here\n\n\n\n2.2. (10 points) Report an ATT estimate\nAcross mothers, estimate the Average Treatment Effect on the Treated (ATT) by the weighted mean difference between \\(Y\\) (observed) and \\(\\hat{Y}^0\\) (predicted from linear regression), weighted by sampling weights.\n\nFor each mother, take the difference between the observed outcome y and the probability of employment that you predict for her in the absence of motherhood.\nThen take the weighted mean across mothers weighted by the sampling weight.\nReport this weighted mean.\n\n\n# your code here"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#concepts-for-today",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#concepts-for-today",
    "title": "Confidence Intervals",
    "section": "Concepts for today",
    "text": "Concepts for today\nStatistical concepts\n\nSampling distribution\nStandard error\nConfidence interval\nBootstrap\n\nCoding concepts\n\nWriting a custom function\nWriting a for loop"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#example-mean-salary-of-mlb-players",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#example-mean-salary-of-mlb-players",
    "title": "Confidence Intervals",
    "section": "Example: Mean salary of MLB players",
    "text": "Example: Mean salary of MLB players\nLoad data:\n\nbaseball &lt;- read_csv(\"https://soc114.github.io/data/baseball.csv\") |&gt;\n  # Keep only a few variables for simplicity\n  select(player, team, salary)\n\n\n\n# A tibble: 944 × 3\n  player             team      salary\n  &lt;chr&gt;              &lt;chr&gt;      &lt;dbl&gt;\n1 Bumgarner, Madison Arizona 21882892\n2 Marte, Ketel       Arizona 11600000\n3 Ahmed, Nick        Arizona 10375000\n# ℹ 941 more rows"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#example-mean-salary-of-mlb-players-1",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#example-mean-salary-of-mlb-players-1",
    "title": "Confidence Intervals",
    "section": "Example: Mean salary of MLB players",
    "text": "Example: Mean salary of MLB players\nTrue mean in population of all players\n\n\nbaseball |&gt; summarize(population_mean = mean(salary))\n\n# A tibble: 1 × 1\n  population_mean\n            &lt;dbl&gt;\n1        4965481."
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#estimate-from-a-sample",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#estimate-from-a-sample",
    "title": "Confidence Intervals",
    "section": "Estimate from a sample",
    "text": "Estimate from a sample\n\nDraw a sample of 10 players.\n\n\n\nsampled_players &lt;- baseball |&gt; \n  slice_sample(n = 10) |&gt;\n  print(n = 3)\n\n# A tibble: 10 × 3\n  player          team          salary\n  &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;\n1 Matz, Steven    St. Louis   10500000\n2 Barlow, Scott   Kansas City  5300000\n3 Pomeranz, Drew* San Diego   10000000\n# ℹ 7 more rows"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#estimate-from-a-sample-1",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#estimate-from-a-sample-1",
    "title": "Confidence Intervals",
    "section": "Estimate from a sample",
    "text": "Estimate from a sample\nTake the mean among sampled players.\n\n\nsampled_players &lt;- sampled_players |&gt; \n  summarize(sample_estimate = mean(salary)) |&gt;\n  print()\n\n# A tibble: 1 × 1\n  sample_estimate\n            &lt;dbl&gt;\n1         8947500"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#many-times",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#many-times",
    "title": "Confidence Intervals",
    "section": "Many times",
    "text": "Many times\n\nIf you are following, these are in many_samples.csv.\n\nmany_samples &lt;- read_csv(\"https://soc114.github.io/data/many_samples.csv\")\n\n\nBecause each sample produces a different estimate, there is a distribution of different estimates across repeated samples.\n\n\nCan you propose a summary statistic for this distribution?"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#mean-of-the-distribution",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#mean-of-the-distribution",
    "title": "Confidence Intervals",
    "section": "Mean of the distribution",
    "text": "Mean of the distribution\nAlso called the expected value.\n\n\nmany_samples |&gt;\n  summarize(estimator_mean = mean(sample_estimate))\n\n# A tibble: 1 × 1\n  estimator_mean\n           &lt;dbl&gt;\n1       5036657.\n\n\n\n(In practice, the mean of the distribution is unknown)"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#standard-error",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#standard-error",
    "title": "Confidence Intervals",
    "section": "Standard Error",
    "text": "Standard Error\nA measure of dispersion for the distribution of sample mean estimates.\n\n\n\n\nmany_samples |&gt;\n  summarize(standard_error = sd(sample_estimate))\n\n# A tibble: 1 × 1\n  standard_error\n           &lt;dbl&gt;\n1       2210213."
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#as-the-sample-size-grows",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#as-the-sample-size-grows",
    "title": "Confidence Intervals",
    "section": "As the sample size grows",
    "text": "As the sample size grows"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#as-the-sample-size-grows-1",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#as-the-sample-size-grows-1",
    "title": "Confidence Intervals",
    "section": "As the sample size grows",
    "text": "As the sample size grows"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#as-the-sample-size-grows-2",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#as-the-sample-size-grows-2",
    "title": "Confidence Intervals",
    "section": "As the sample size grows",
    "text": "As the sample size grows"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#asymptotic-normality",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#asymptotic-normality",
    "title": "Confidence Intervals",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\n\nAs the sample size gets large (asymptotic)\nThis becomes a Normal distribution"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#middle-95-sampling-interval",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#middle-95-sampling-interval",
    "title": "Confidence Intervals",
    "section": "Middle 95% sampling interval",
    "text": "Middle 95% sampling interval\nWe might want to summarize:\n\nThe mean of the estimator\nA range containing the middle 95% of sample estimates\n\n\nWhy is that hard to do with one actual sample?"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#confidence-interval-via-the-bootstrap",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#confidence-interval-via-the-bootstrap",
    "title": "Confidence Intervals",
    "section": "Confidence interval via the bootstrap",
    "text": "Confidence interval via the bootstrap\nWhat we want:\n\nWe would want many samples: sample_1, sample_2, sample_3,…\nWe estimate with each\nWe summarize the middle 95%"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#confidence-interval-via-the-bootstrap-1",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#confidence-interval-via-the-bootstrap-1",
    "title": "Confidence Intervals",
    "section": "Confidence interval via the bootstrap",
    "text": "Confidence interval via the bootstrap\nWhat we can do:\n\nWe get only one sample\n\nSo we simulate hypothetical sample_sim_1, sample_sim_2,…\n\nWe estimate with each\nWe summarize the middle 95%"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples",
    "title": "Confidence Intervals",
    "section": "How to generate bootstrap samples",
    "text": "How to generate bootstrap samples\nStart with your one sample.\n\nsampled_players &lt;- baseball |&gt;\n  slice_sample(n = 100)\n\n\nResample \\(n\\) players with replacement.\n\nsampled_players_bootstrap &lt;- sampled_players |&gt;\n  slice_sample(prop = 1, replace = TRUE)"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example",
    "title": "Confidence Intervals",
    "section": "How to generate bootstrap samples: Example",
    "text": "How to generate bootstrap samples: Example\nHere is a sample of 3 players:\n\na_small_sample &lt;- baseball |&gt; \n  slice_sample(n = 3) |&gt;\n  print()\n\n# A tibble: 3 × 3\n  player              team       salary\n  &lt;chr&gt;               &lt;chr&gt;       &lt;dbl&gt;\n1 Houck, Tanner       Boston     740000\n2 Gallegos, Giovanny  St. Louis 4750000\n3 Hernandez, Jonathan Texas      995000"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example-1",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example-1",
    "title": "Confidence Intervals",
    "section": "How to generate bootstrap samples: Example",
    "text": "How to generate bootstrap samples: Example\nHere is a bootstrap sample of those 3 players.\n\na_small_sample |&gt; \n  slice_sample(prop = 1, replace = TRUE) |&gt;\n  print()\n\n# A tibble: 3 × 3\n  player             team       salary\n  &lt;chr&gt;              &lt;chr&gt;       &lt;dbl&gt;\n1 Gallegos, Giovanny St. Louis 4750000\n2 Gallegos, Giovanny St. Louis 4750000\n3 Houck, Tanner      Boston     740000"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example-2",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example-2",
    "title": "Confidence Intervals",
    "section": "How to generate bootstrap samples: Example",
    "text": "How to generate bootstrap samples: Example\nHere is a bootstrap sample of those 3 players.\n\na_small_sample |&gt; \n  slice_sample(prop = 1, replace = TRUE) |&gt;\n  print()\n\n# A tibble: 3 × 3\n  player              team       salary\n  &lt;chr&gt;               &lt;chr&gt;       &lt;dbl&gt;\n1 Gallegos, Giovanny  St. Louis 4750000\n2 Hernandez, Jonathan Texas      995000\n3 Gallegos, Giovanny  St. Louis 4750000"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example-3",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example-3",
    "title": "Confidence Intervals",
    "section": "How to generate bootstrap samples: Example",
    "text": "How to generate bootstrap samples: Example\nHere is a bootstrap sample of those 3 players.\n\na_small_sample |&gt; \n  slice_sample(prop = 1, replace = TRUE) |&gt;\n  print()\n\n# A tibble: 3 × 3\n  player              team  salary\n  &lt;chr&gt;               &lt;chr&gt;  &lt;dbl&gt;\n1 Hernandez, Jonathan Texas 995000\n2 Hernandez, Jonathan Texas 995000\n3 Hernandez, Jonathan Texas 995000"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example-4",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-generate-bootstrap-samples-example-4",
    "title": "Confidence Intervals",
    "section": "How to generate bootstrap samples: Example",
    "text": "How to generate bootstrap samples: Example\nHere is a bootstrap sample of those 3 players.\n\na_small_sample |&gt; \n  slice_sample(prop = 1, replace = TRUE) |&gt;\n  print()\n\n# A tibble: 3 × 3\n  player             team       salary\n  &lt;chr&gt;              &lt;chr&gt;       &lt;dbl&gt;\n1 Houck, Tanner      Boston     740000\n2 Gallegos, Giovanny St. Louis 4750000\n3 Gallegos, Giovanny St. Louis 4750000"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#coding-concepts",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#coding-concepts",
    "title": "Confidence Intervals",
    "section": "Coding concepts",
    "text": "Coding concepts\nWe will analyze hundreds of bootstrap samples.\nWe need two coding concepts.\n\nHow to write an estimator function\nHow to write a for loop"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-write-an-estimator-function",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-write-an-estimator-function",
    "title": "Confidence Intervals",
    "section": "How to write an estimator function",
    "text": "How to write an estimator function\nA function (like mean) takes an input and returns an output. You can write your own.\n\nestimator &lt;- function(data) {\n  data |&gt;\n    summarize(estimate = mean(salary)) |&gt;\n    pull(estimate)\n}\n\nThe function takes data and returns an estimate.\n\nestimator(data = sampled_players)\n\n[1] 6254938"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-write-a-for-loop",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#how-to-write-a-for-loop",
    "title": "Confidence Intervals",
    "section": "How to write a for loop",
    "text": "How to write a for loop\nUseful for tasks you will repeat.\n\nFirst, initialize a vector to hold results.\n\nvector_for_results &lt;- rep(NA, 3)\n\nThe rep function repeates the value NA 3 times.\n\n\nSecond, loop through and fill your vector.\n\nfor (index in 1:3) {\n  vector_for_results[index] &lt;- index\n}\n\nSquare brackets [] extract an element of a vector."
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#analyze-500-bootstrap-samples",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#analyze-500-bootstrap-samples",
    "title": "Confidence Intervals",
    "section": "Analyze 500 bootstrap samples",
    "text": "Analyze 500 bootstrap samples\nInitialize a vector to hold the result.\n\nbootstrap_estimates &lt;- rep(NA, times = 500)"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#analyze-500-bootstrap-samples-1",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#analyze-500-bootstrap-samples-1",
    "title": "Confidence Intervals",
    "section": "Analyze 500 bootstrap samples",
    "text": "Analyze 500 bootstrap samples\nWrite a for loop that will repeat 500 times.\n\nfor (index in 1:500) {\n  \n  # Draw a bootstrap sample\n  bootstrap_sample &lt;- sampled_players |&gt;\n    slice_sample(prop = 1, replace = TRUE)\n  \n  # Construct an estimate\n  estimate_this_index &lt;- estimator(bootstrap_sample)\n  \n  # Store that estimate\n  bootstrap_estimates[index] &lt;- estimate_this_index\n}"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#bootstrap-results",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#bootstrap-results",
    "title": "Confidence Intervals",
    "section": "Bootstrap results",
    "text": "Bootstrap results"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#bootstrap-results-summary-statistics",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#bootstrap-results-summary-statistics",
    "title": "Confidence Intervals",
    "section": "Bootstrap results: Summary statistics",
    "text": "Bootstrap results: Summary statistics\nBootstrap estimate of the standard error.\n\nsd(bootstrap_estimates)\n\n[1] 853093.5\n\n\nMiddle 95% of bootstrap estimates\n\nquantile(x = bootstrap_estimates, prob = c(.025, .975))\n\n   2.5%   97.5% \n4691641 8036999"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#confidence-interval",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#confidence-interval",
    "title": "Confidence Intervals",
    "section": "Confidence interval",
    "text": "Confidence interval\nAn interval from \\(\\text{lower}(\\text{sample})\\) to \\(\\text{upper}(\\text{sample})\\) with the property: across repeated samples, 95% of intervals constructed this way would contain the population parameter."
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#confidence-interval-example",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#confidence-interval-example",
    "title": "Confidence Intervals",
    "section": "Confidence interval: Example",
    "text": "Confidence interval: Example\nMiddle 95% of bootstrap estimates is a confidence interval.\n\nThe true population mean salary is $4,965,481\nOur sample mean is $6,254,938\nOur confidence interval is:\n\n\nquantile(x = bootstrap_estimates, prob = c(.025, .975))\n\n   2.5%   97.5% \n4691641 8036999 \n\n\nAcross repeated samples, 95% of intervals constructed this way will contain the population mean salary."
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#recap",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#recap",
    "title": "Confidence Intervals",
    "section": "Recap",
    "text": "Recap\n\nStatistical concepts\nCoding concepts"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#recap-statistical-concepts",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#recap-statistical-concepts",
    "title": "Confidence Intervals",
    "section": "Recap: Statistical concepts",
    "text": "Recap: Statistical concepts\nStatistical concepts\n\nSampling distribution\n\nCannot be directly observed. We have one sample.\n\nStandard error\n\nSpread of the sampling distribution\n\nConfidence interval\n\nCovers truth in 95% of samples\n\nBootstrap\n\nMethod of constructing the CI with one sample"
  },
  {
    "objectID": "slides/lec04_bootstrap/lec04_bootstrap.html#recap-coding-concepts",
    "href": "slides/lec04_bootstrap/lec04_bootstrap.html#recap-coding-concepts",
    "title": "Confidence Intervals",
    "section": "Recap: Coding concepts",
    "text": "Recap: Coding concepts\n\nWriting a custom function (R4DS Ch 25)\nWriting a for loop (R4DS Ch 27.5)"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression-learning-goals",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression-learning-goals",
    "title": "Logistic Regression",
    "section": "Logistic regression: Learning goals",
    "text": "Logistic regression: Learning goals\nSome things you may know\n\nLogistic regression is good for binary outcomes\nCoefficients are hard to interpret\n\nData science ideas\n\nPredicted values make logistic regression easy to use"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression",
    "title": "Logistic Regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nA type of model for a binary outcome\n\n\\(Y\\) taking the values {0,1} or {FALSE,TRUE}\n\nModeled as a function of predictor variables \\(\\vec{X}\\)"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#a-data-example",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#a-data-example",
    "title": "Logistic Regression",
    "section": "A data example",
    "text": "A data example\nbaseball_population.csv\n\npopulation &lt;- read_csv(\"https://soc114.github.io/data/baseball_population.csv\")\n\n\n\n# A tibble: 944 × 6\n  player               salary position team    team_past_record team_past_salary\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;            &lt;dbl&gt;\n1 Bumgarner, Madison 21882892 LHP      Arizona            0.457         2794887.\n2 Marte, Ketel       11600000 2B       Arizona            0.457         2794887.\n3 Ahmed, Nick        10375000 SS       Arizona            0.457         2794887.\n4 Kelly, Merrill      8500000 RHP      Arizona            0.457         2794887.\n5 Walker, Christian   6500000 1B       Arizona            0.457         2794887.\n# ℹ 939 more rows"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#a-data-example-1",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#a-data-example-1",
    "title": "Logistic Regression",
    "section": "A data example",
    "text": "A data example\n\nplayer is the player name\nsalary is the 2023 salary\nposition is the position played (e.g., LHP for left-handed pitcher)\nteam is the team name\nteam_past_record was the team’s win percentage in the previous season\nteam_past_salary was the team’s average salary in the previous season"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#a-binary-outcome",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#a-binary-outcome",
    "title": "Logistic Regression",
    "section": "A binary outcome",
    "text": "A binary outcome\n\nYou see a player’s salary\nAre they a catcher?\n\nposition == \"C\""
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#linear-probability-model",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#linear-probability-model",
    "title": "Logistic Regression",
    "section": "Linear probability model",
    "text": "Linear probability model\nWe can model with lm() for a linear fit.\n\nols_binary_outcome &lt;- lm(\n  position == \"C\" ~ salary,\n  data = population\n)\n\n\n\nCode\npopulation |&gt;\n  mutate(yhat = predict(ols_binary_outcome)) |&gt;\n  ggplot(aes(x = salary, y = yhat)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_x_continuous(\n    name = \"Player Salary\",\n    labels = label_currency(scale = 1e-6, suffix = \"m\")\n  ) +\n  scale_y_continuous(\n    name = \"Predicted Probability\\nof Being a Catcher\"\n  ) +\n  ggtitle(\"Modeling a binary outcome with a line\")"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#goal-avoid-illogical-predictions",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#goal-avoid-illogical-predictions",
    "title": "Logistic Regression",
    "section": "Goal: Avoid illogical predictions",
    "text": "Goal: Avoid illogical predictions\nIn OLS, there is a linear predictor \\[\n\\mu = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 + \\cdots\n\\] that can take any numeric value. Possibly \\(\\mu &lt;0\\) or \\(\\mu &gt; 1\\)."
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#from-mu-to-pi",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#from-mu-to-pi",
    "title": "Logistic Regression",
    "section": "From \\(\\mu\\) to \\(\\pi\\)",
    "text": "From \\(\\mu\\) to \\(\\pi\\)\nLogistic regression passes the linear predictor \\[\\mu = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 + \\cdots\\] through a nonlinear function to force it between 0 and 1.\n\\[\n\\pi = \\text{logit}^{-1}\\left(\\beta_0 + X\\beta_1\\right) = \\frac{e^{\\beta_0 + X\\beta_1}}{1 + e^{\\beta_0 + X\\beta_1}}\n\\]"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#from-mu-to-pi-1",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#from-mu-to-pi-1",
    "title": "Logistic Regression",
    "section": "From \\(\\mu\\) to \\(\\pi\\)",
    "text": "From \\(\\mu\\) to \\(\\pi\\)\n\n\n\nAt linear predictor 0, what is the predicted probability?\nAt linear predictor 2.5, what is the predicted probability?\nAt linear predictor \\(\\infty\\), what is the predicted probability?"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#from-pi-to-mu",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#from-pi-to-mu",
    "title": "Logistic Regression",
    "section": "From \\(\\pi\\) to \\(\\mu\\)",
    "text": "From \\(\\pi\\) to \\(\\mu\\)\nYou can also think from \\(\\pi\\) to \\(\\mu\\).\n\\[\n\\begin{aligned}\n\\text{logit}(\\pi) &= \\mu = \\beta_0 + X\\beta_1 \\\\\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) &= \\mu = \\beta_0 + X\\beta_1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#from-pi-to-mu-1",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#from-pi-to-mu-1",
    "title": "Logistic Regression",
    "section": "From \\(\\pi\\) to \\(\\mu\\)",
    "text": "From \\(\\pi\\) to \\(\\mu\\)"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression-in-r",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\nThe glm() function (for logistic regression) works exactly like the lm() function (for linear regression)"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression-in-r-1",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression-in-r-1",
    "title": "Logistic Regression",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\n\nlogistic_regression &lt;- glm(\n  position == \"C\" ~ salary,\n  data = population,\n  family = \"binomial\"\n)\n\n\nposition == \"C\" is our outcome: the binary indicator that the position variable takes the value \"C\"\nsalary is a predictor variable\nfamily = \"binomial\" specifies logistic regression (since “binomial” is a distribution for binary outcomes)"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#coefficients-a-word-of-warning",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#coefficients-a-word-of-warning",
    "title": "Logistic Regression",
    "section": "Coefficients: A word of warning",
    "text": "Coefficients: A word of warning\nHard to interpret. Not probabilities. Use predicted values instead.\n\nsummary(logistic_regression)\n\n\nCall:\nglm(formula = position == \"C\" ~ salary, family = \"binomial\", \n    data = population)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.268e+00  1.500e-01 -15.126   &lt;2e-16 ***\nsalary      -5.599e-08  2.599e-08  -2.154   0.0312 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 508.94  on 943  degrees of freedom\nResidual deviance: 502.85  on 942  degrees of freedom\nAIC: 506.85\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#predicted-values",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#predicted-values",
    "title": "Logistic Regression",
    "section": "Predicted values",
    "text": "Predicted values\nBe sure to use type = \"response\" predict probabilities (between 0 and 1) instead of log odds\n\npredict(\n  logistic_regression,\n  type = \"response\"\n)"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#predicted-values-1",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#predicted-values-1",
    "title": "Logistic Regression",
    "section": "Predicted values",
    "text": "Predicted values\n\n\nCode\npopulation |&gt;\n  mutate(yhat = predict(logistic_regression, type = \"response\")) |&gt;\n  distinct(salary, yhat) |&gt;\n  ggplot(aes(x = salary, y = yhat)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_x_continuous(\n    name = \"Player Salary\",\n    labels = label_currency(scale = 1e-6, suffix = \"m\")\n  ) +\n  scale_y_continuous(\n    name = \"Predicted Probability\\nof Being a Catcher\"\n  ) +\n  ggtitle(\"Modeling a binary outcome with logistic regression\")"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#predicted-values-with-newdata",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#predicted-values-with-newdata",
    "title": "Logistic Regression",
    "section": "Predicted values with newdata",
    "text": "Predicted values with newdata\n\nNew player: salary is $5 million.\nWhat is the probability that this player is a catcher?\n\n\nto_predict &lt;- tibble(salary = 5e6)\n\n\nMake the predicted value.\n\npredict(\n  logistic_regression,\n  newdata = to_predict,\n  type = \"response\"\n)\n\n         1 \n0.07255671"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#linear-and-logistic-regression",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#linear-and-logistic-regression",
    "title": "Logistic Regression",
    "section": "Linear and logistic regression",
    "text": "Linear and logistic regression\nWhat is the same? What is different?\n\n\nSame\n\nTakes \\(X\\) and predicts \\(Y\\)\nInvolves \\(\\beta_0 + \\beta_1 X\\)\n\nDifferent\n\nLogistic regression predicts a probability \\(0\\leq\\pi\\leq 1\\)\n\n\n\\[\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + X_1\\beta_1\n\\]"
  },
  {
    "objectID": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression-learning-goals-1",
    "href": "slides/lec06_logistic_regression/lec06_logistic_regression.html#logistic-regression-learning-goals-1",
    "title": "Logistic Regression",
    "section": "Logistic regression: Learning goals",
    "text": "Logistic regression: Learning goals\nSome things you may know\n\nLogistic regression is good for binary outcomes\nCoefficients are hard to interpret\n\nData science ideas\n\nPredicted values make logistic regression easy to use"
  },
  {
    "objectID": "slides/m_discussion_sailing.html",
    "href": "slides/m_discussion_sailing.html",
    "title": "Untitled",
    "section": "",
    "text": "You are looking into a sailing class through UCLA Recreation! For each claim below, tell us whether the claim is causal or descriptive.\n2.1 (5 points) \nLast year, there was a survey of students who did and did not take the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher among those who took the class.\nAnswer. Your answer here\n2.2 (5 points) \nLast year, there was a survey of students before and after the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher in the survey taken after the class.\nAnswer. Your answer here\n2.3 (5 points) \nOn average, the students in this class emerged more prepared to sail than they would have been without the class.\nAnswer. Your answer here"
  },
  {
    "objectID": "slides/m_discussion_sailing.html#a-sailing-class",
    "href": "slides/m_discussion_sailing.html#a-sailing-class",
    "title": "Untitled",
    "section": "",
    "text": "You are looking into a sailing class through UCLA Recreation! For each claim below, tell us whether the claim is causal or descriptive.\n2.1 (5 points) \nLast year, there was a survey of students who did and did not take the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher among those who took the class.\nAnswer. Your answer here\n2.2 (5 points) \nLast year, there was a survey of students before and after the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher in the survey taken after the class.\nAnswer. Your answer here\n2.3 (5 points) \nOn average, the students in this class emerged more prepared to sail than they would have been without the class.\nAnswer. Your answer here"
  },
  {
    "objectID": "topics/11_exchangeability.html",
    "href": "topics/11_exchangeability.html",
    "title": "Exchangeability",
    "section": "",
    "text": "Topic for 2/11.\nCausal effects involve both factual and counterfactual outcomes, yet data that we can observe involve only factual outcomes. To learn about causal effects from data that can be observed requires assumptions about the data that are not observed. This page introduces exchangeability, which is an assumption that can identify causal effects.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/11_exchangeability.html#exchangeability-in-simple-random-samples",
    "href": "topics/11_exchangeability.html#exchangeability-in-simple-random-samples",
    "title": "Exchangeability",
    "section": "Exchangeability in simple random samples",
    "text": "Exchangeability in simple random samples\nThe figure below illustrates a population of 6 people. Each person has an outcome \\(Y_i\\), which for example might be that person’s employment at age 40. A researcher draws a random sample without replacement with equal sampling probabilities and records the sampled outcomes. The researcher uses the average of the sampled outcomes as an estimator for the population mean.\n\nWhy do probability samples like this work? They work because selection into the sample (\\(S = 1\\)) is completely randomized and thus independent of the outcome \\(Y\\). In other words, the people who are sampled (\\(S = 1\\)) and the people who are unsampled (\\(S = 0\\)) have the same distribution of outcomes (at least in expectation over samples). We might say that the sampled and the unsampled units are exchangeable in the sense that they follow the same distribution in terms of \\(Y\\). In math, exchangeable sampling can be written as follows.\n\\[\n\\underbrace{Y}_\\text{Outcome}\\quad \\underbrace{\\mathrel{\\unicode{x2AEB}}}_{\\substack{\\text{Is}\\\\\\text{Independent}\\\\\\text{of}}} \\quad \\underbrace{S}_{\\substack{\\text{Sample}\\\\\\text{Inclusion}}}\n\\]\nExchangeability holds in simple random samples because sampling is completely independent of all outcomes by design. In other types of sampling, such as convenience samples that enroll anyone who is interested, exchangeability may hold but is far from guaranteed. Perhaps people who are employed are more likely to answer a survey about employment, so that the employment rate in a convenience sample might far exceed the population mean employment rate. Exchangeability is one condition under which reliable population estimates can be made from samples, and probability samples are good because they make exchangeability hold by design.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/11_exchangeability.html#exchangeability-in-randomized-experiments",
    "href": "topics/11_exchangeability.html#exchangeability-in-randomized-experiments",
    "title": "Exchangeability",
    "section": "Exchangeability in randomized experiments",
    "text": "Exchangeability in randomized experiments\nThe figure below illustrates our population if they all enrolled in a hypothetical randomized experiment. In this experiment, we imagine that each unit is either randomized to attain a four-year college degree (\\(A = 1)\\) or to finish education with a high school diploma (\\(A = 0\\)).\n\nIn this randomization, Maria, Sarah, and Jes'us were randomized to attain a four-year college degree. We observe their outcomes under this treatment condition (\\(Y^1\\)). Because treatment was randomized with equal probabilities, these three units form a simple random sample from the full population of 6 people. We could use the sample mean of \\(Y^1\\) among the treated units (Maria, Sarah, Jes'us) as an estimator of the population mean of \\(Y^1\\) among all 6 units.\nWilliam, Rich, and Alondra were randomized to finish their education with a high school diploma. We see their outcomes under this control condition \\(Y^0\\). Their treatment assignment (\\(A = 0\\)) is analogous to being sampled from the population of \\(Y^0\\) values. We can use their sample mean outcome as an estimator of the population mean of \\(Y^0\\).\nFormally, we can write the exchangeability assumption for treatment assignments as requiring that the set of potential outcomes are independent of treatment assignment.\n\\[\n\\underbrace{\\{Y^1,Y^0\\}}_{\\substack{\\text{Potential}\\\\\\text{Outcomes}}}\\quad\\underbrace{\\mathrel{\\unicode{x2AEB}}}_{\\substack{\\text{Are}\\\\\\text{Independent}\\\\\\text{of}}}\\quad  \\underbrace{A}_\\text{Treatment}\n\\] Exchangeability holds in randomized experiments because treatment is completely independent of all potential outcomes by design. In observational studies, where treatment values are observed but are not assigned randomly by the researcher, exchangeability may hold but is far from guaranteed. In the coming classes, we will talk about generalizations of the exchangeability assumption that one can argue might hold in some observational settings.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/11_exchangeability.html#causal-identification",
    "href": "topics/11_exchangeability.html#causal-identification",
    "title": "Exchangeability",
    "section": "Causal identification",
    "text": "Causal identification\nA population-average causal effect could take many possible values. Using data alone, it is impossible to identify which of these many possible values is the correct one. By pairing data together with causal assumptions, however, one can identify the average causal effect by equating it with a statistical quantity that only involves observable random variables.\n\nCausal identification. A mathematical proof linking a causal estimand (involving potential outcomes) to a statistical quantity involving only factual random variables.\n\nIn a randomized experiment, the average causal effect is identified by the assumptions of consistency and exchangeability. A short proof can yield insight about the goals and how these assumptions are used.\n\\[\n\\begin{aligned}\n&\\overbrace{\\text{E}\\left(Y^1\\right) - \\text{E}\\left(Y^0\\right)}^{\\substack{\\text{Average}\\\\\\text{causal effect}\\\\\\text{(among everyone)}}} \\\\\n&= \\text{E}\\left(Y^1\\mid A = 1\\right) - \\text{E}\\left(Y^0\\mid A = 0\\right) &\\text{by exchangeability}\\\\\n&= \\underbrace{\\text{E}\\left(Y\\mid A = 1\\right)}_{\\substack{\\text{Mean outcome}\\\\\\text{among the treated}}} - \\underbrace{\\text{E}\\left(Y\\mid A = 0\\right)}_{\\substack{\\text{Mean outcome}\\\\\\text{among the untreated}}} &\\text{by consistency}\n\\end{aligned}\n\\]\nThe proof begins with the average causal effect and equates it to a statistical estimand: the mean outcome among the treated minus the mean outcome among the untreated. The first quantity involves potential outcomes (with superscripts), whereas the last quantity involves only factual random variables.\nThe exchangeability assumption allows us to move from the first line to the second line. Under exchangeability, the mean outcome that would be realized under treatment (\\(\\text{E}(Y^1)\\)) equals the mean outcome under treatment among those who were actually treated (\\(\\text{E}(Y^0)\\)). Likewise for outcomes under no treatment. This line is true because the treated (\\(A = 1\\)) and the untreated (\\(A = 0\\)) are both simple random samples from the full population.\nThe consistency assumption allows us to move from the second line to the third. Among the treated, (\\(A = 1\\)), the outcome that is realized is \\(Y = Y^1\\). Among the untreated (\\(A = 0\\)), the outcome that is realized is \\(Y = Y^0\\). Under the assumption that factual outcomes are consistent with the potential outcomes under the assigned treatment, the second line equal the third.\nSomething nice about a causal identification proof is that there is no room for error: it is mathematically true that the premise and the assumptions together yield the result. As long as the assumptions hold, the statistical estimand equals the causal estimand. Causal inference thus boils down to research designs and arguments that can lend credibility to the assumptions that let us draw causal claims from data that are observed.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/11_exchangeability.html#conditional-exchangeability",
    "href": "topics/11_exchangeability.html#conditional-exchangeability",
    "title": "Exchangeability",
    "section": "Conditional exchangeability",
    "text": "Conditional exchangeability\n\nThis topic is covered Feb 4. Here are slides.\n\nConditional exchangeability is an assumption that exchangeability holds within population subgroups. This assumption holds by design in a conditionally randomized experiment (discussed on this page), and may hold under certain causal beliefs in observational settings where the treatment is not randomized (next page).",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/11_exchangeability.html#a-conditionally-randomized-experiment",
    "href": "topics/11_exchangeability.html#a-conditionally-randomized-experiment",
    "title": "Exchangeability",
    "section": "A conditionally randomized experiment",
    "text": "A conditionally randomized experiment\nSuppose we were to carry out an experiment on a simple random sample of U.S. high school students. Among those performing in the top 25% of their high school class, we randomize 80% to attain a four-year college degree. Among those performing in the bottom 75% of their high school class, we randomize 20% to attain a four-year college degree. We are interested in effects on employment at age 40 (\\(Y\\)).\n\n\n\n\n\n\n\n\n\nThis experiment is conditionally randomized because the probability of treatment (four-year degree) is different among the higher- and lower-performing high school students.\n\nConditionally randomized experiment. An experiment in which the probability of treatment assignment depends on the values of pre-treatment covariates. \\(\\text{P}(A = 1\\mid\\vec{X} = \\vec{x})\\) depends on the value \\(\\vec{x}\\).",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/11_exchangeability.html#conditional-exchangeability-1",
    "href": "topics/11_exchangeability.html#conditional-exchangeability-1",
    "title": "Exchangeability",
    "section": "Conditional exchangeability",
    "text": "Conditional exchangeability\nIn a conditionally randomized experiment, exchangeability is not likely to hold. People who are treated (assigned to a four-year degree) are more likely to have come from the top 25% of their high school class. They might be especially hard-working people. The treated and untreated might have had different employment at age 40 even if none of them had been treated.\nEven though exchangeability does not hold marginally (across everyone), in a conditionally randomized experiment exchangeability does hold within subgroups. If we focus on those in the top 25% of the class, the 90% who are assigned to finish college are a simple random sample of the entire higher-performing subgroup. If we focus on those in the bottom 75% of the class, the 10% who are assigned to finish college are a simple random sample of the entire lower-performing subgroup.\nFormally, conditional exchangeability takes the exchangeability assumption (\\(\\{Y^0,Y^1\\}\\unicode{x2AEB}A\\)) and adds a conditioning bar \\(\\mid\\vec{X}\\), meaning that this assumption holds within subgroups defined by one or more pre-treatment variables \\(\\vec{X}\\).\n\nConditional exchangeability. The assumption that potential outcomes \\(\\{Y^0,Y^1\\}\\) are independent of treatment \\(A\\) among subpopulations that are identical along a set of pre-treatment covariates \\(\\vec{X}\\). Formally, \\(\\{Y^0,Y^1\\} \\unicode{x2AEB} A \\mid \\vec{X}\\).\n\nConditional exchangeability holds by design in conditionally randomized experiments: the probability of treatment assignment differs across subgroups, but within each subgroup we have a simple randomized experiment where each unit has an equal probability of being treated.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/11_exchangeability.html#conditional-average-treatment-effects",
    "href": "topics/11_exchangeability.html#conditional-average-treatment-effects",
    "title": "Exchangeability",
    "section": "Conditional average treatment effects",
    "text": "Conditional average treatment effects\nIn our conditionally randomized experiment, we could identify conditional average treatment effects: the average effects of college on employment at age 40 (1) among those in the top 25% of their high school class, and the and (2) among those in the bottom 75% of their high school class.\n\nConditional average treatment effect (CATE). The average causal effect within a population subgroup, \\(\\tau(x) = \\text{E}\\left(Y^1\\mid\\vec{X} = \\vec{x}\\right) - \\text{E}\\left(Y^0\\mid \\vec{X} = \\vec{x}\\right)\\).\n\nOnce we assume conditional exchangeability and consistency, CATEs are causally identified by working within a subgroup defined by \\(\\vec{X} = \\vec{x}\\) and taking the difference in means across subgroups of units assigned to treatment and control.\n\\[\n\\begin{aligned}\n&\\text{E}\\left(Y^1\\mid\\vec{X} = \\vec{x}\\right) -  \\text{E}\\left(Y^0\\mid\\vec{X} = \\vec{x}\\right)\\\\\n&= \\text{E}\\left(Y\\mid\\vec{X} = \\vec{x}, A = 1\\right) - \\text{E}\\left(Y\\mid\\vec{X} = \\vec{x}, A = 0\\right)\n\\end{aligned}\n\\]\nIn our concrete example, this means that we could first focus on the subgroup for whom \\(\\vec{X} = (\\text{Top 25\\% of high school class})\\). Within this subgroup, we can compare employment at age 40 among those randomized to a 4-year college degree to employment at age 40 among those randomized to finish education after high school. This mean difference identifies the CATE: the average causal effect of college among those in the top 25% of their high school class.\nLikewise, our experiment would also identify the CATE among those in the bottom 75% of their high school class.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/11_exchangeability.html#effect-heterogeneity",
    "href": "topics/11_exchangeability.html#effect-heterogeneity",
    "title": "Exchangeability",
    "section": "Effect heterogeneity",
    "text": "Effect heterogeneity\nThere are often good reasons to expect the Conditional Average Treatment Effect (CATE) to differ across subpopulations. In our example, suppose that those from the top 25% of the high school class are very creative and hard-working, and would find ways to be employed at age 40 regardless of whether they finished college. The average causal effect of college on employment in this subgroup might be small. Meanwhile, the average causal effect of college on employment might be quite large among those from the bottom 75% of their high school class. This would be an example of effect heterogeneity,\n\nEffect heterogeneity. Differences in Conditional Average Treatment Effects (CATEs) across subpopulations. \\(\\tau(\\vec{x})\\neq\\tau(\\vec{x}')\\).\n\nAn advantage of analyzing randomized experiments conditionally (within subgroups) is that one can search for effect heterogeneity.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/13_matching.html",
    "href": "topics/13_matching.html",
    "title": "Matching",
    "section": "",
    "text": "Here are slides on matching.\nMatching is a method for causal inference that is analogous to model-based estimation, but is often easier to explain. Suppose we have a set of 5 units, of whom 2 are treated.\nWe would like to infer that treatment causes higher outcomes, but the units also differ along a confounding variable \\(L\\). How can we infer the average treatment effect for units 1 and 2?\nOne way to draw inference is by a model: model \\(Y^0\\) as a function of \\(L\\) among the untreated units, and then use our model to predict for the treated units.\nAnother strategy might involve no model at all. We notice that unit 3 is very similar to unit 1 along \\(L\\). Likewise, unit 2 is very similar to unit 5. We could match these units together and use the matches to infer the unobserved potential outcomes.\nWe then estimate the average effect for units 1 and 2 by the difference of",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Matching"
    ]
  },
  {
    "objectID": "topics/13_matching.html#matching-in-math",
    "href": "topics/13_matching.html#matching-in-math",
    "title": "Matching",
    "section": "Matching in math",
    "text": "Matching in math\nFormally, let \\(\\text{match}(i)\\) denote the index of the match for unit \\(i\\). Our goal is to estimate the average treatment effect on the treated,\n\\[\\tau = \\frac{1}{n_1}\\sum_{i:A_i=1}\\left(Y_i^1 - Y_i^0\\right)\\]\nwhere \\(n_1\\) is the number of treated units and the sum is taken over all units \\(i\\) such that the treatment took the value 1 for those units.\nThe fundamental problem of causal inference is that for these units \\(Y_i^1\\) is observed but \\(Y_i^0\\) is not. But for each treated unit \\(i\\), we find an untreated match \\(j = \\text{match}(i)\\) who is very simialr to \\(i\\) but for whom \\(Y_j^0\\) is observed. We then estimate by the mean difference between the treated units and their matched controls.\n\\[\\hat\\tau_\\text{Matching} = \\frac{1}{n_1}\\sum_{i:A_i=1}\\left(Y_i^1 - Y_{\\text{match}(i)}^0\\right)\\]",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Matching"
    ]
  },
  {
    "objectID": "topics/13_matching.html#matching-vs.-regression",
    "href": "topics/13_matching.html#matching-vs.-regression",
    "title": "Matching",
    "section": "Matching vs. regression",
    "text": "Matching vs. regression\nWhy should we prefer matching or regression?\nWe have already learned a regression solution to this problem: assume a sufficient adjustment set \\(\\vec{X}\\), model the \\(Y_i^0\\) outcomes as a function of \\(\\vec{X}\\) for a set of units who factually were untreated, and use the model to predict what would happen for the treated units if they had been untreated \\((\\hat{Y}_i^0)\\). Then our estimator of the ATT would be:\n\\[\\hat\\tau_\\text{Regression} = \\frac{1}{n_1}\\sum_{i:A_i=1}\\left(Y_i^1 - \\hat{\\text{E}}(Y\\mid\\vec{X} = \\vec{x}_i, A = 0)\\right)\\]\nMatching is actually doing the same thing—we are just using the outcome of unit \\(j\\) as an estimator of \\(\\hat{\\text{E}}(Y\\mid\\vec{X} = \\vec{x}_i, A = 0)\\).\nWhy would we then prefer matching? One reason is explainability. A model is easy to explain to social scientists and statisticians who are familiar with models. It isn’t as good when you are speaking to policymakers and others who are unfamiliar with models.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Matching"
    ]
  },
  {
    "objectID": "topics/13_matching.html#distances-for-multivariate-matching",
    "href": "topics/13_matching.html#distances-for-multivariate-matching",
    "title": "Matching",
    "section": "Distances for multivariate matching",
    "text": "Distances for multivariate matching\nSuppose we have treated and untreated units that differ along two confounding variables, \\(L_1\\) and \\(L_2\\).\n\nWhich control unit should be chosen as the match? With more than one variable, it is not immediately obvious which control points is “closest” to the treated point—we first need to define “closest.” Which one we would choose requires one to choose a distance metric.\n\nDistance metric. A function \\(d()\\) that takes two vectors \\(\\vec{x}\\) and \\(\\vec{x}'\\) and returns a scalar numeric distance \\(d(\\vec{x},\\vec{x}')\\).\n\n\n\nManhattan distance\nImagine that the points are places in Manhattan, where the streets are arranged in a grid. The way to travel from Treated to Untreated 1 is by traveling 4 blocks north and then 3 blocks east, for a total distance of 7 units. This distance metric is called Manhattan distance.\n\nManhattan distance. The distance between two vectors \\(\\vec{x}\\) and \\(\\vec{x}'\\) is the sum of their absolute differences on each element: \\[d_\\text{Manhattan}(\\vec{x},\\vec{x}') = \\sum_p \\lvert x_p - x'_p \\rvert \\]\n\nBy Manhattan distance, we might determine that the unit (Untreated 2) is closest to (Treated) because its Manhattan distance from the treated unit is 6 instead of 7.\n\n\nEuclidean distance\nImagine instead that the points are in a field, and you are a crow. The distance that is relevant to you is the most direct line—the distance as the crow flies! This is Euclidean distance.\n\nEuclidean distance. The distance between two vectors \\(\\vec{x}\\) and \\(\\vec{x}'\\) is the square root of the sum of their squared differences on each element: \\[d_\\text{Euclidean}(\\vec{x},\\vec{x}') = \\sqrt{\\sum_p \\left( x_p - x'_p \\right)^2} \\]\n\nBy Euclidean distance, we would choose a different matched control unit! The unit (Untreated 2) is 6 units away from the (Treated) unit in terms of Euclidean distance, and the unit (Untreated 1) is only 5 units away. By Euclidean distance, the match to choose is (Untreated 1).\nThe comparison between Euclidean and Manhattan distances shows that our choice of distance metric can shape who we choose as matches.\nIn practice, neither Manhattan nor Euclidean distance is commonly used for matching. Instead, researchers often use Mahalanobis distance, which is a generalization of Euclidean distance that takes into account the variance and covariance of \\(\\vec{X}\\). And an even more common distance metric is propensity score distance, which we discuss next.\n\n\nPropensity score distance\nA distance metric requires us to map a pair of vectors \\((\\vec{x},\\vec{x}')\\) into a single-number distance. Thankfully, there is already a way we often map a vector of confounders to a number: predict the probability of treatment.\nSuppose we estimate each unit’s probability of being treated, \\(\\text{P}(A = 1\\mid \\vec{X} = \\vec{x}_i)\\). We might then define the distance between two units as the distance between their predicted probabilities of being treated.\n\nPropensity score distance. The distance between two vectors \\(\\vec{x}\\) and \\(\\vec{x}'\\) is the squared difference in the probability of treatment under these two vectors. \\[d_\\text{PropensityScore}(\\vec{x},\\vec{x}') = \\left(\\text{P}(A = 1\\mid \\vec{X} = \\vec{x}) - \\text{P}(A = 1\\mid \\vec{X} = \\vec{x}')\\right)^2\\]\n\nOften, the propensity score \\(\\text{P}(A = 1\\mid \\vec{X})\\) is estimated by a logistic regression model, but one could estimate by any machine learning strategy or nonparametrically if \\(\\vec{X}\\) is discrete.\nPropensity score matching is especially intuitive. With propensity score matching, the researcher matches each treated unit to a control unit who had a similar probability of being treated.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Matching"
    ]
  },
  {
    "objectID": "topics/13_matching.html#choices-after-the-distance",
    "href": "topics/13_matching.html#choices-after-the-distance",
    "title": "Matching",
    "section": "Choices after the distance",
    "text": "Choices after the distance\nAfter you define the distance, there are many additional choices for how to conduct matching! These choices often involve a tradeoff between bias and variance.\n\nWith and without replacement\nBelow are two treated and two untreated units who differ on one confounding variable \\(L\\). The left-most treated unit is matched to the left-most control unit. The algorithm then moves to the right-most treated unit: which control unit should serve as its match?\n\nOne argument is that the left-most control unit should again serve as the match—it is clearly the closest control unit. But it has already been used! If we want to enforce that each treated unit gets its own unique untreated match, then we should match to the unit at the right.\nThis is the choice between matching with and without replacement.\n\nWith replacement: After an untreated unit is used as a match for one treated unit, it is returned to the pool of untreated units to be considered for future matches.\nWithout replacement: After an untreated unit is used as a match for one treated unit, it is never again used as a match.\n\nMatching with replacement yields the closest possible matches: each treated unit gets paired with the closest control unit that can be found. In this sense, matching with replacement reduces bias.\nBut matching with replacement can also produce a high-variance estimator. Suppose you have 50 treated units who all get matched to a single untreated unit—the random chance that included that particular untreated unit in the sample has huge influence on the resulting estimate!\nThe choice of with and without replacement has no correct answer; whether one or the other is better will depend on the bias and variance in a particular research setting.\n\n\nk:1 matching\nShould each treated unit be matched to only one untreated unit, or should we match to more than one untreated unit and take the average? A k:1 matching algorithm matches each treated unit to \\(k\\) untreated units.\n\nThe advantage of k:1 matching is a reduction in variance: by averaging over a larger number of untreated units, the resulting estimator will vary less from sample to sample. But the cost of k:1 matching bias: the two closest matches are not generally going to be collectively as close to the treated unit as the single closest match.\n\n\nCalipers\nSuppose we have a treated unit, and there seem to be no comparable control units. Is there a point at which we give up the search?\n\nIn our illustration, the treated point at the far right is very far from both untreated units. It is not clear that we should try to match this unit. To formalize that it is too far, we might define the black bars as the farthest distance we are willing to look for a match. The width of these bars is known as a caliper.\n\nCaliper. The maximum distance between a treated and untreated unit such that we will consider them possible matches.\n\nIn caliper matching, we would not match the right-most treated unit to any untreated unit. Instead, we would update the estimand to be the average treatment effect on the treated among those within the caliper distance from the control units.\nCaliper matching can be good because it avoids bad matches. But caliper matching comes with a cost—it changes the causal estimand to the causal effect in a subgroup who can be hard to explain!",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Matching"
    ]
  },
  {
    "objectID": "topics/13_matching.html#regression-after-matching",
    "href": "topics/13_matching.html#regression-after-matching",
    "title": "Matching",
    "section": "Regression after matching",
    "text": "Regression after matching\nAfter matching, there are two estimators we could consider.\n\nMean \\(Y\\) among treated units - mean \\(Y\\) of matched control units\nCoefficient on \\(A\\) in a regression of \\(Y\\) on \\(A\\) and \\(\\vec{X}\\) among matches\n\nEstimator (2) is preferable in the sense that the regression model can correct for imperfections in our matching. Despite our best efforts, the matched controls will not be quite equal to the treated units along \\(\\vec{X}\\)! Regression can fix this. In this sense, we can think of matching as a preprocessing step before regression.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Matching"
    ]
  },
  {
    "objectID": "topics/13_matching.html#matching-in-code",
    "href": "topics/13_matching.html#matching-in-code",
    "title": "Matching",
    "section": "Matching in code",
    "text": "Matching in code\nThe MatchIt package in R carries out all kinds of matching applications. You can do all of the above using MatchIt.\nHere is one example, using our data from the model-based inference page.\n\nlibrary(tidyverse)\nlibrary(MatchIt)\n\n\ndata &lt;- read_csv(\"https://soc114.github.io/data/nlsy97_simulated.csv\")\n\n\nmatched &lt;- matchit(\n  # A formula for treatment given confounders.\n  # Treatment must be a binary or logical variable.\n  formula = (a == \"treated\") ~ sex + race + mom_educ + dad_educ +\n    log_parent_income + log_parent_wealth + test_percentile,\n  # Data containing variables\n  data = data,\n  # Conduct propensity score matching\n  distance = \"glm\",\n  link = \"logit\",\n  estimand = \"ATT\"\n) |&gt; print()\n\nA `matchit` object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 7688 (original), 2958 (matched)\n - target estimand: ATT\n - covariates: sex, race, mom_educ, dad_educ, log_parent_income, log_parent_wealth, test_percentile\n\n\nWe see that this carried out 1:1 nearest neighbor matching without replacement, with distance estimated by the propensity score estimated with logistic regression. We can extract the resulting matches with the match.data() function.\n\nmatches &lt;- match.data(matched)\n\nThen we can estimate by the mean difference across matched treated and control units. In the event of matching with replacement or k:1 matching, it is important to include weights since each unit may be used as a match multiple times.\n\nmatches |&gt;\n  group_by(a) |&gt;\n  summarize(estimate = weighted.mean(y, w = weights))\n\n# A tibble: 2 × 2\n  a         estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 treated      0.518\n2 untreated    0.254\n\n\nAlternatively, we can carry out regression after matching.\n\nlm_after_matching &lt;- lm(\n  y ~ (a == \"treated\") + sex + race + mom_educ + dad_educ +\n    log_parent_income + log_parent_wealth + test_percentile,\n  data = matches,\n  weights = weights\n)\n\nThen our estimate could be the coefficient on the treatment variable.\n\nsummary(lm_after_matching)\n\n\nCall:\nlm(formula = y ~ (a == \"treated\") + sex + race + mom_educ + dad_educ + \n    log_parent_income + log_parent_wealth + test_percentile, \n    data = matches, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7731 -0.3592 -0.1666  0.4412  1.2364 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                -0.1667709  0.0904700  -1.843  0.06537 .  \na == \"treated\"TRUE          0.2350511  0.0169415  13.874  &lt; 2e-16 ***\nsexMale                     0.1119653  0.0170433   6.569 5.95e-11 ***\nraceNon-Hispanic Black     -0.1501000  0.0341049  -4.401 1.12e-05 ***\nraceNon-Hispanic Non-Black -0.0232965  0.0275414  -0.846  0.39769    \nmom_educCollege            -0.0421786  0.0442361  -0.953  0.34042    \nmom_educHigh school        -0.1016445  0.0422265  -2.407  0.01614 *  \nmom_educNo mom             -0.1125961  0.0654962  -1.719  0.08570 .  \nmom_educSome college       -0.0691847  0.0427370  -1.619  0.10559    \ndad_educCollege             0.0872352  0.0451752   1.931  0.05357 .  \ndad_educHigh school         0.0010773  0.0448006   0.024  0.98082    \ndad_educNo dad              0.0713750  0.0452732   1.577  0.11501    \ndad_educSome college        0.0662214  0.0449126   1.474  0.14047    \nlog_parent_income           0.0058689  0.0063126   0.930  0.35260    \nlog_parent_wealth           0.0155895  0.0049802   3.130  0.00176 ** \ntest_percentile             0.0029745  0.0004176   7.122 1.33e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4505 on 2942 degrees of freedom\nMultiple R-squared:  0.1482,    Adjusted R-squared:  0.1438 \nF-statistic: 34.11 on 15 and 2942 DF,  p-value: &lt; 2.2e-16\n\n\nUsing the regression-after-matching strategy, we estimate that going to college leads to a 0.24 increase in the probability of having a college-educated spouse or residential partner.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Matching"
    ]
  },
  {
    "objectID": "topics/13_matching.html#what-to-read",
    "href": "topics/13_matching.html#what-to-read",
    "title": "Matching",
    "section": "What to read",
    "text": "What to read\nTo learn more about matching, a good article is:\n\nStuart, Elizabeth. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.”. Statistical Science 25(1):1-21.\n\nTo see how matching methods have been used in questions of social stratification, see this book and papers cited in it.\n\nBrand, Jennie E. 2023. Overcoming the Odds: The Benefits of Completing College for Unlikely Graduates. Russell Sage Foundation. Here is a link to read online through the UCLA Library.",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Measured Confounding",
      "Matching"
    ]
  },
  {
    "objectID": "topics/15_did.html",
    "href": "topics/15_did.html",
    "title": "Difference in Difference",
    "section": "",
    "text": "Topic for 3/2.\n\nPage to be written.\n\n\n\n Back to top",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Unmeasured Confounding",
      "Difference in Difference"
    ]
  },
  {
    "objectID": "topics/17_iv.html",
    "href": "topics/17_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "To be written.\n\n\n\n Back to top",
    "crumbs": [
      "Syllabus",
      "Causal Inference with Unmeasured Confounding",
      "Instrumental Variables"
    ]
  },
  {
    "objectID": "topics/1b_software.html",
    "href": "topics/1b_software.html",
    "title": "Software Prerequisites",
    "section": "",
    "text": "To succeed in this course, you will need to write statistical programming code. You will need to install two pieces of free, open-source software.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Software Prerequisites"
    ]
  },
  {
    "objectID": "topics/1b_software.html#install-statistical-software-r",
    "href": "topics/1b_software.html#install-statistical-software-r",
    "title": "Software Prerequisites",
    "section": "Install statistical software: R",
    "text": "Install statistical software: R\nWe will write code in the R programming language. R is available as open-source software at https://cran.r-project.org/. The first step to set up your computer is to install R.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Software Prerequisites"
    ]
  },
  {
    "objectID": "topics/1b_software.html#install-the-interface-rstudio",
    "href": "topics/1b_software.html#install-the-interface-rstudio",
    "title": "Software Prerequisites",
    "section": "Install the interface RStudio",
    "text": "Install the interface RStudio\nWe will work with R using an interface called RStudio, which makes it easy to write code and see results all in one place. You should install RStudio Desktop, which is available to download here: https://posit.co/download/rstudio-desktop/",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Software Prerequisites"
    ]
  },
  {
    "objectID": "topics/1b_software.html#install-the-tidyverse-package",
    "href": "topics/1b_software.html#install-the-tidyverse-package",
    "title": "Software Prerequisites",
    "section": "Install the tidyverse package",
    "text": "Install the tidyverse package\nMany R functions are made freely available in open-source packages that contain sets of functions designed to carry out common tasks. One package we will use often is the tidyverse, which contains functions to manipulate and visualize data. To install tidyverse, first open RStudio. Find the Console, which is a place where you can type code to immediately execute.\n\nIn the console type,\n\ninstall.packages(\"tidyverse\")\n\nand press enter or return on your keyboard. This runs a line of code to install a set of software packages.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Software Prerequisites"
    ]
  },
  {
    "objectID": "topics/1b_software.html#optional-install-tinytex-to-produce-pdf-reports",
    "href": "topics/1b_software.html#optional-install-tinytex-to-produce-pdf-reports",
    "title": "Software Prerequisites",
    "section": "(Optional) Install tinytex to produce PDF reports",
    "text": "(Optional) Install tinytex to produce PDF reports\nFor some assignments and the project, you may want to produce a PDF report from RStudio. One way to do that is with LaTeX, which is software that typesets documents and which works well with RStudio. Some versions of LaTeX are large and difficult to install. If you have never used LaTeX on your computer, we recommend that you install as follows: paste the code below into your R console and press enter or return to install a minimal version of the software.\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nStudents often find this step confusing, and computers present various errors. If you have an error, look on Piazza to see if anyone else has encountered your error. If not, then post a screenshot of your error on Piazza so we can help you to resolve the problem. For every assignment, it will also be possible to submit without compiling a PDF this way.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Software Prerequisites"
    ]
  },
  {
    "objectID": "topics/1b_software.html#support-and-guidance",
    "href": "topics/1b_software.html#support-and-guidance",
    "title": "Software Prerequisites",
    "section": "Support and guidance",
    "text": "Support and guidance\nCongratulations on preparing your computing environment!\nThroughout the first part of the course, we will often use the online textbook R for Data Science by Hadley Wickham as a reference. The book will introduce how to work with data using R and RStudio. If you want additional guidance for setting up the software, see the Prerequisites section of R4DS. To learn more about RStudio, visit the RStudio User Guide.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Software Prerequisites"
    ]
  },
  {
    "objectID": "topics/2a_visualization.html",
    "href": "topics/2a_visualization.html",
    "title": "Visualizing a Distribution",
    "section": "",
    "text": "Here are slides in web and pdf format.\nData science questions often involve many units, each of whom may have a unique value of the outcome variable. How do we summarize all of these outcome values? This page focuses on two approaches: visualizing the distribution and producing one or more summary statistics.\nAs an example, below we study household income (an outcome) which is defined for each household (a unit of analysis) among all U.S. households in 2022 (a target population).",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Visualizing a Distribution"
    ]
  },
  {
    "objectID": "topics/2a_visualization.html#visualizing-the-distribution",
    "href": "topics/2a_visualization.html#visualizing-the-distribution",
    "title": "Visualizing a Distribution",
    "section": "Visualizing the distribution",
    "text": "Visualizing the distribution\nNot all households have the same income: there is a distribution of incomes across households. One way to study a distribution is by visualizing it with a histogram.\nTo produce this graph, we first downloaded survey data on annual household income from the 2022 Current Population Survey. The histogram categorizes households into discrete income groups that are each $25,000 wide. The height of each bar corresponds to the number of households falling in that income group. We can see that the most common household income values are below $100,000, but a small number of households have very high incomes that create a long upper tail at the right.\n\n\n\n\n\n\n\n\n\nThe simulated dataset incomeSimulated.csv available on the course website will enable you to produce a similar graph.\nThe code below will produce a basic version of this graph. First, you will need to prepare your environment by loading packages.\n\nlibrary(tidyverse) # package with many functions we use\n\nThen, you can load the data from the course website.\n\nincomeSimulated &lt;- read_csv(\"https://soc114.github.io/data/incomeSimulated.csv\")\n\nThe code below produces the graph.\n\nggplot(\n  data = incomeSimulated,\n  mapping = aes(x = hhincome)\n) +\n  geom_histogram(binwidth = 25e3) +\n  labs(\n    x = \"Household Income\", \n    y = \"Count of Households in Bin\"\n  )\n\n\n\n\n\n\n\n\nLet’s walk through this code in steps.\n\nInitialize with the ggplot() function\n\ndata argument contains the data\nmapping argument maps data to plot elements\naes() is an aesthetics function that helps\n\nAdd a layer with +\ngeom_histogram() layer makes a histogram\n\nOptional argument binwidth sets the width of each bin\n\nlabs() layer modifies axis labels\n\nFor more practice with ggplot, see R4DS Ch 1. A particularly good example you can try is section R4DS 1.2.3.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Visualizing a Distribution"
    ]
  },
  {
    "objectID": "topics/3_sampling.html",
    "href": "topics/3_sampling.html",
    "title": "Population Sampling",
    "section": "",
    "text": "Here are slides\nClaims about inequality are often claims about a population. Our data are typically only a sample! This module addresses the link between samples and populations.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#full-count-enumeration",
    "href": "topics/3_sampling.html#full-count-enumeration",
    "title": "Population Sampling",
    "section": "Full count enumeration",
    "text": "Full count enumeration\nWhat proportion of our class prefers to sit in the front of the room?\nWe answered this question in class using full count enumeration: list the entire target population and ask them the question. Full count enumeration is ideal because it removes all statistical sources of error. But in settings with a larger target population, the high cost of full count enumeration may be prohibitive.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#simple-random-sample",
    "href": "topics/3_sampling.html#simple-random-sample",
    "title": "Population Sampling",
    "section": "Simple random sample",
    "text": "Simple random sample\nWe carried out a simple random sample1 in class.\n\neveryone generated a random number between 0 and 1\nthose with values less than 0.1 were sampled\nour sample estimate was the proportion of those sampled to prefer the front of the room\n\nIn a simple random sample, each person in the population is sampled with equal probabilities. Because the probabilities are known, a simple random sample is a probability sample.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#unequal-probability-sample",
    "href": "topics/3_sampling.html#unequal-probability-sample",
    "title": "Population Sampling",
    "section": "Unequal probability sample",
    "text": "Unequal probability sample\nSuppose we want to make subgroup estimates:\n\nwhat proportion prefer the front, among those sitting in the first 3 rows?\nwhat proportion prefer the front, among those sitting in the back 17 rows?\n\nIn a simple random sample, we might only get a few or even zero people in the first 3 rows! To reduce the chance of this bad sample, we could draw an unequal probability sample:\n\nthose in rows 1–3 are selected with probability 0.5\nthose in rows 4–20 are selected with probability 0.1\n\nOur unequal probability sample will over-represent the first three rows, thus creating a large enough sample in this subgroup to yield precise estimates.\n\nHaving drawn an unequal probability sample, suppose we now want to estimate the class-wide proportion who prefer sitting in the front. We will have a problem: those who prefer the front may be more likely to sit there, and they are also sampled with a higher probability! Sample inclusion is related to the value of our outcome.\nBecause the sampling probabilities are known, we can correct for this by applying sampling weights, which for each person equals the inverse of the known probability of inclusion for that person.\nFor those in rows 1–3,\n\nwe sampled with probability 50%\non average 1 in every 2 people is sampled\neach person in the sample represents 2 people in the population\n\\(w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.5} = 2\\)\n\nFor those in rows 4–20,\n\nwe sampled with probability 10%\non average 1 in every 10 people is sampled\neach person in the sample represents 10 people in the population\n\\(w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.1} = 10\\)\n\nTo estimate the population mean, we can use the weighted sample mean,\n\\[\\frac{\\sum_i y_iw_i}{\\sum_i w_i}\\]",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#stratified-random-sample",
    "href": "topics/3_sampling.html#stratified-random-sample",
    "title": "Population Sampling",
    "section": "Stratified random sample",
    "text": "Stratified random sample\nWe could also draw a stratified random sample by first partitioning the population into subgroups (called strata) and then drawing samples within each subgroup. For instance,\n\nsample 10 of the 20 people in rows 1–3\nsample 10 of the 130 people in rows 4–17\n\nIn simple random or unequal probability sampling, it is always possible that by random chance we sample no one in the front of the room. Stratified random sampling rules this out: we know in advance how our sample will be balanced across the two strata.\n\n\n\n\n\n\nNote\n\n\n\nIn our real-data example at the end of this page, the Current Population Survey is stratified by state so that the Bureau of Labor Statistics knows in advance that they will gather a sufficient sample to estimate unemployment in each state.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#sec-cps",
    "href": "topics/3_sampling.html#sec-cps",
    "title": "Population Sampling",
    "section": "A real case: The Current Population Survey",
    "text": "A real case: The Current Population Survey\nEvery month, the Bureau of Labor Statistics in collaboration with the U.S. Census Bureau collects data on unemployment in the Current Population Survey (CPS). The CPS is a probability sample designed to estimate the unemployment rate in the U.S. and in each state.\nWe will be using the CPS in discussion. This video introduces the CPS and points you toward where you can access the data via IPUMS-CPS.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#sec-baseball",
    "href": "topics/3_sampling.html#sec-baseball",
    "title": "Population Sampling",
    "section": "Example: Baseball players",
    "text": "Example: Baseball players\nAs one example where full-count enumeration is possible, we will examine the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by USA Today and are available in baseball.csv.\n\nbaseball &lt;- read_csv(\"https://soc114.github.io/data/baseball.csv\")\n\n\n\n# A tibble: 944 × 4\n  player            team         position   salary\n  &lt;chr&gt;             &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;\n1 Scherzer, Max     N.Y. Mets    RHP      43333333\n2 Verlander, Justin N.Y. Mets    RHP      43333333\n3 Judge, Aaron      N.Y. Yankees OF       40000000\n4 Rendon, Anthony   L.A. Angels  3B       38571429\n5 Trout, Mike       L.A. Angels  OF       37116667\n# ℹ 939 more rows\n\n\nSalaries are high, and income inequality is also high among baseball players\n\n4% were paid the league minimum of $720,000\n53% were paid less than $2,000,000\nthe highest-paid players—Max Scherzer and Justin Verlander—each earned $43,333,333\nthe highest-paid half of players take home 92% of the total pay\n\n\n\n\n\n\n\n\n\n\nPay also varies widely across teams!\n\n\n\n\n\n\n\n\n\n\nConceptualize the sampling strategy\nSuppose you did not have the whole population. You still want to learn the population mean salary! How could you learn that in a sample of 60 out of the 944 players?\nBefore reading on, think through three questions:\n\nWhat would it mean to use each of these strategies?\n\n\na simple random sample of 60 players\na sample stratified by the 30 MLB teams\na sample clustered by the 30 MLB teams\n\n\nWhich strategies have advantages in terms of\n\n\nbeing least expensive?\nhaving the best statistical properties?\n\n\nGiven that you already have the population, how would you write some R code to carry out the sampling strategies? You might use sample_n() and possibly group_by().\n\n\n\nSampling strategies in code\nIn a simple random sample, we draw 60 players from the entire league. Each player’s probability of sample inclusion is \\(\\frac{60}{n}\\) where \\(n\\) is the number of players in the league (944).\n\nsimple_sample &lt;- function(population) {\n  population |&gt;\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 60 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Sample 60 players\n    sample_n(size = 60)\n}\n\nTo use this function, we give it the baseball data as the population and it returns a tibble containing a sample of 60 players.\n\nsimple_sample(population = baseball)\n\n# A tibble: 60 × 6\n   player            team              position salary p_sampled sampling_weight\n   &lt;chr&gt;             &lt;chr&gt;             &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1 Bird, Jacob       Colorado          RHP      7.22e5    0.0636            15.7\n 2 Vogelbach, Daniel N.Y. Mets         1B       1.5 e6    0.0636            15.7\n 3 Realmuto, JT      Philadelphia      C        2.39e7    0.0636            15.7\n 4 Brown, Seth       Oakland           OF       7.3 e5    0.0636            15.7\n 5 Walker, Christian Arizona           1B       6.5 e6    0.0636            15.7\n 6 Choi, Ji-Man      Pittsburgh        1B       4.65e6    0.0636            15.7\n 7 Buehler, Walker*  L.A. Dodgers      RHP      8.03e6    0.0636            15.7\n 8 Wentz, Joey       Detroit           LHP      7.24e5    0.0636            15.7\n 9 Lodolo, Nick      Cincinnati        LHP      7.3 e5    0.0636            15.7\n10 Crochet, Garrett* Chicago White Sox LHP      7.33e5    0.0636            15.7\n# ℹ 50 more rows\n\n\nIn a stratified random sample by team, we sample 2 players on each of 30 teams. A stratified random sample is often a higher-quality sample, because it eliminates the possibility of an unlucky draw that completely omits a few teams. All teams are equally represented no matter what happens in the randomization. The downside of a stratified random sample is that it is costly.\nEach player’s probability of sample inclusion is \\(\\frac{2}{n}\\) where \\(n\\) is the number on that player’s team (which ranges from 28 to 35).\n\nstratified_sample &lt;- function(population) {\n  population |&gt;\n    # Draw sample within each team\n    group_by(team) |&gt;\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 2 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Within each team, sample 2 players\n    sample_n(size = 2) |&gt;\n    ungroup()\n}\n\nIn a sample clustered by team, we might first sample 3 teams and then sample 20 players on each sampled team. A clustered sample is often less costly, for example because you would only need to call up the front office of 3 teams instead of 30 teams. But this type of sample is lower quality, because there is some chance that one will randomly select a few teams that all have particularly high or low average salaries. A clustered random sample is less expensive but is more susceptible to random error based on the clusters chosen.\nEach player’s probability of sample inclusion is P(Team Chosen) \\(\\times\\) P(Chosen Within Team) = \\(\\frac{3}{30}\\times\\frac{20}{n}\\) where \\(n\\) is the number on that player’s team (which ranges from 28 to 35).\n\nclustered_sample &lt;- function(population) {\n  \n  # First, sample 3 teams\n  sampled_teams &lt;- population |&gt;\n    # Make one row per team\n    distinct(team) |&gt;\n    # Sample 3 teams\n    sample_n(3) |&gt;\n    # Store those 3 team names in a vector\n    pull()\n  \n  # Then load data on those teams and sample 20 per team\n  population |&gt;\n    filter(team %in% sampled_teams) |&gt;\n    # Define sampling probability and weight\n    group_by(team) |&gt;\n    mutate(\n      p_sampled = (3 / 30) * (20 / n()),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Sample 20 players\n    sample_n(20) |&gt;\n    ungroup()\n}\n\n\n\nWeighted mean estimator\nGiven a sample, how do we estimate the population mean? The weighted mean estimator can also be placed in a function\n\nwe hand our sample to the function\nwe get a numeric estimate back\n\n\nestimator &lt;- function(sample) {\n  sample |&gt;\n    summarize(estimate = weighted.mean(\n      x = salary, \n      w = sampling_weight\n    )) |&gt;\n    pull(estimate)\n}\n\nHere is what it looks like to use the estimator.\n\nsample_example &lt;- simple_sample(population = baseball)\nestimator(sample = sample_example)\n\n[1] 5501815\n\n\nTry it for yourself! The true mean salary in the league is $4,965,481. How close do you come when you apply the estimator to a sample drawn by each strategy?\n\n\nEvaluating performance: Many samples\nWe might like to know something about performance across many repeated samples. The replicate function will carry out a set of code many times.\n\nsample_estimates &lt;- replicate(\n  n = 1000,\n  expr = {\n    a_sample &lt;- simple_sample(population = baseball)\n    estimator(sample = a_sample)\n  }\n)\n\nSimulate many samples. Which one is the best? Strategy A, B, or C?\n\n\n\nThe danger of one sample\nIn actual science, we typically have only one sample. Any estimate we produce from that sample involves some signal about the population quantities, and also some noise. Herein is the danger: researchers are very good at telling stories about why their sample evidence tells something about the population, even when it may be random noise. We illustrate this with an example.\nDoes salary differ between left- and right-handed pitchers? To address this question, I create a tibble with only the pitchers(those for whom the position variable takes the value LHP or RHP).\n\npitchers &lt;- baseball |&gt;\n  filter(position == \"LHP\" | position == \"RHP\")\n\nTo illustrate what can happen with a sample, we now draw a sample. Let’s first set our computer’s random number seed so we get the same sample each time.\n\nset.seed(1599)\n\nThen draw a sample of 40 pitchers\n\npitchers_sample &lt;- pitchers |&gt;\n  sample_n(size = 40)\n\nand examine the mean difference in salary.\n\npitchers_sample |&gt;\n  group_by(position) |&gt;\n  summarize(salary_mean = mean(salary))\n\n# A tibble: 2 × 2\n  position salary_mean\n  &lt;chr&gt;          &lt;dbl&gt;\n1 LHP         9677309.\n2 RHP         3182428.\n\n\nThe left-handed pitchers make millions of dollars more per year! You can probably tell many stories why this might be the case. Maybe left-handed pitchers are needed by all teams, and there just aren’t many available because so few people are left-handed!\nWhat happens if we repeat this process many times? The figure below shows many repeated samples of size 40 from the population of pitchers.\n\n\n\n\n\n\n\n\n\nOur original result was really random noise: we happened by chance to draw a sample with some highly-paid left-handed pitchers!\nThis exercise illustrates what is known as the replication crisis: findings that are surprising in one sample may not hold in other repeated samples from the same population, or in the population as a whole. The replication crisis has many sources. One principal source is the one we illustrated above: sample-based estimates involve some randomness, and well-meaning researchers are (unfortunately) very good at telling interesting stories.\nOne solution to the replication crisis is to pay close attention to the statistical uncertainty in our estimates, such as that from random sampling. Another solution is to re-evaluate findings that are of interest on new samples. In any case, both the roots of the problem and the solutions are closely tied to sources of randomness in estimates, such as those generated using samples from a population.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#the-future-of-sample-surveys",
    "href": "topics/3_sampling.html#the-future-of-sample-surveys",
    "title": "Population Sampling",
    "section": "The future of sample surveys",
    "text": "The future of sample surveys\nSample surveys served as a cornerstone of social science research from the 1950s to the present. But there are concerns about their future:\n\nsome sampling frames, such as landline telephones, have become obsolete\nresponse rates have been falling for decades\nsample surveys are slower and more expensive than digital data\n\nWhat is the future for sample surveys? How can they be combined with other data?\nWe will close with a discussion of these questions, which you can also engage with in the Groves 2011 reading that follows this module.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#using-weights",
    "href": "topics/3_sampling.html#using-weights",
    "title": "Population Sampling",
    "section": "Using weights",
    "text": "Using weights\nWhen studying population-level inequality, our goal is to draw inference about all units in the population. We want to know about the people in the U.S., not just the people who answer the Current Population Survey. Drawing inference from a sample to a population is most straightforward for a simple random sample: when people are chosen at random with equal probabilities. For simple random samples, the sample average of any variable is an unbiased and consistent estimator of the population average.\nBut the Current Population Survey is not a simple random sample. Neither are most labor force samples! These samples still begin with a sampling frame, but people are chosen with unequal probabilities. We need sample weights to address this fact.\nIn the CPS, a key goal is to estimate unemployment in each state. Every state needs to have enough sample size—even tiny states like Wyoming. In order to make those estimates, the CPS oversamples people who live in small states.\n\n\n\n\n\n\nAn example: California and Wyoming\n\n\n\nIn 2022, California had 14,822 CPS-ASEC respondents out of a population of 39,029,342. Wyoming had 2,199 CPS-ASEC respondents out of 581,381 residents. The average probability that a CA resident was sampled was about 0.04 percent, whereas the same probability in WY was 0.4 percent. You are 10 times more likely to be sampled for the ASEC if you live in Wyoming.\n\n\nTo draw good population inference, our analysis must incorporate what we know about how the data were collected. If we ignore the weights, our sample will have too many people from Wyoming and too few people from California. Weights correct for this.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#how-survey-designers-create-weights",
    "href": "topics/3_sampling.html#how-survey-designers-create-weights",
    "title": "Population Sampling",
    "section": "How survey designers create weights",
    "text": "How survey designers create weights\nTo calculate sampling weight on person \\(i\\), those who design survey samples take the ratio \\[\\text{weight on unit }i = \\frac{1}{\\text{probability of including person }i\\text{ in the sample}}\\] You can think of the sampling weight as the number of population members a given sample member represents. If there are 100 people with a 1% chance of inclusion, then on average 1 of them will be in the sample. That person represents \\(\\frac{1}{.01}=100\\) people.\n\n\n\n\n\n\nExample redux: California and Wyoming\n\n\n\nSuppose Californians are sampled with probability 0.0004. Then each Californian represents 1 / 0.0004 = 2,500 people. Each Californian should receive a weight of 2,500. Working out the same math for Wyoming, each Wyoming resident should receive a weight of 250. The total weight on these two samples will then be proportional to the sizes of these two populations.\n\n\nIn practice, weighting is more complicated: survey administrators adjust weights for differential nonresponse across population subgroups (a method called post-stratification). How to construct weights is beyond the scope of this course, and could be a whole course in itself!",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#point-estimates",
    "href": "topics/3_sampling.html#point-estimates",
    "title": "Population Sampling",
    "section": "Point estimates",
    "text": "Point estimates\nWhen we download data, we typically download a column of weights. For simplicity, suppose we are given a sample of four people. The weight column tells us how many people in the population each person represents. The employed column tells us whether each person employed.\n\n\n     name weight employed\n1    Luis      4        1\n2 William      1        0\n3   Susan      1        0\n4  Ayesha      4        1\n\n\nIf we take an unweighted mean, we would conclude that only 50% of the population is employed. But with a weighted mean, we would conclude that 80% of the population is employed! This might be the case if the sample was designed to oversample people at a high risk of unemployment.\n\n\n\n\n\n\n\n\n\nEstimator\nMath\nExample\nResult\n\n\n\n\nUnweighted mean\n\\(=\\frac{\\sum_{i=1}^n Y_i}{n}\\)\n\\(=\\frac{1 + 0 + 0 + 1}{4}\\)\n= 50% employed\n\n\nWeighted mean\n\\(=\\frac{\\sum_{i=1}^n w_iY_i}{\\sum_{i=1}^n w_i}\\)\n\\(=\\frac{4*1 + 1*0 + 1*0 + 4*1}{4 + 1 + 1 + 4}\\)\n= 80% employed\n\n\n\nIn R, the weighted.mean(x, w) function will calculate weighted means where x is an argument for the outcome variable and w is an argument for the weight variable.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#standard-errors",
    "href": "topics/3_sampling.html#standard-errors",
    "title": "Population Sampling",
    "section": "Standard errors",
    "text": "Standard errors\nAs you know from statistics, our sample mean is unlikely to equal the population mean. There is random variation in which people were chosen for inclusion in our sample, and this means that across hypothetical repeated samples we would get different sample means! You likely learned formulas to create a standard errors, which quantifies how much a sample estimator would move around across repeated samples.\nUnfortunately, the formula you learned doesn’t work for complex survey samples! Simple random samples (for which those formulas hold) are actually quite rare. When you face a complex survey sample, those who administer the survey might provide\n\na vector of \\(n\\) weights for making a point estimate\na matrix of \\(n\\times k\\) replicate weights for making standard errors\n\nBy providing \\(k\\) different ways to up- and down-weight various observations, the replicate weights enable you to generate \\(k\\) estimates that vary in a way that mimics how the estimator might vary if applied to different samples from the population. For instance, our employment sample might come with 3 replicate weights.\n\n\n     name weight employed repwt1 repwt2 repwt3\n1    Luis      4        1      3      5      3\n2 William      1        0      1      2      2\n3   Susan      1        0      3      1      1\n4  Ayesha      4        1      5      3      4\n\n\nThe procedure to use replicate weights depends on how they are constructed. Often, it is relatively straightforward:\n\nuse weight to create a point estimate \\(\\hat\\tau\\)\nuse repwt* to generate \\(k\\) replicate estimates \\(\\hat\\tau^*_1,\\dots,\\hat\\tau^*_k\\)\ncalculate the standard error of \\(\\hat\\tau\\) using the replicate estimates \\(\\hat\\tau^*\\). The formula will depend on how the replicate weights were constructed, but it will likely involve the standard deviation of the \\(\\hat\\tau^*\\) multiplied by some factor\nconstruct a confidence interval2 by a normal approximation \\[(\\text{point estimate}) \\pm 1.96 * (\\text{standard error estimate})\\]\n\nIn our concrete example, the point estimate is 80% employed. The replicate estimates are 0.67, 0.73, 0.70. Variation across the replicate estimates tells us something about how the estimate would vary across hypothetical repeated samples from the population.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#computational-strategy-for-replicate-weights",
    "href": "topics/3_sampling.html#computational-strategy-for-replicate-weights",
    "title": "Population Sampling",
    "section": "Computational strategy for replicate weights",
    "text": "Computational strategy for replicate weights\nUsing replicate weights can be computationally tricky! It becomes much easier if you write an estimator() function. Your function accepts two arguments\n\ndata is the tibble containing the data\nweight_name is the name of a column containing the weight to be used (e.g., “repwt1”)\n\nExample. If our estimator is the weighted mean of employment,\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    summarize(\n      estimate = weighted.mean(\n        x = employed,\n        # extract the weight column\n        w = sim_rep |&gt; pull(weight_name)\n      )\n    ) |&gt; \n    # extract the scalar estimate\n    pull(estimate)\n}\n\nIn the code above, sim_rep |&gt; pull(weight_name) takes the data frame sim_rep and extracts the weight variable that is named weight_name. There are other ways to do this also.\nWe can now apply our estimator to get a point estimate with the main sampling weight,\n\nestimate &lt;- estimator(data = sim_rep, weight_name = \"weight\")\n\nwhich yields the point estimate 0.80. We can use the same function to produce the replicate estimates,\n\nreplicate_estimates &lt;- c(\n  estimator(data = sim_rep, weight_name = \"repwt1\"),\n  estimator(data = sim_rep, weight_name = \"repwt2\"),\n  estimator(data = sim_rep, weight_name = \"repwt3\")\n)\n\nyielding the three estimates: 0.67, 0.73, 0.70. In real data, you will want to apply this in a loop because there may be dozens of replicate weights.\nThe standard error of the estimator will be some function of the replicate estimates, likely involving the standard deviation of the replicate estimates. Check with the data distributor for a formula for your case. Once you estimate the standard error, a 95% confidence interval can be constructed with a Normal approximation, as discussed above.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#application-in-the-cps",
    "href": "topics/3_sampling.html#application-in-the-cps",
    "title": "Population Sampling",
    "section": "Application in the CPS",
    "text": "Application in the CPS\nStarting in 2005, the CPS-ASEC samples include 160 replicate weights. If you download replicate weights for many years, the file size will be enormous. We illustrate the use of replicate weights with a question that can be explored with only one year of data: among 25-year olds in 2023, how did the proportion holding four-year college degrees differ across those identifying as male and female?\nWe first load some packages, including the foreach package which will be helpful when looping through replicate weights.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(foreach)\n\nTo answer our research question, we download 2023 CPS-ASEC data including the variables sex, educ, age, the weight variable asecwt, and the replicate weights repwtp*.\n\ncps_data &lt;- read_dta(\"../data_raw/cps_00079.dta\")\n\nWe then define an estimator to use with these data. It accepts a tibble data and a character weight_name identifying the name of the weight variable, and it returns a tibble with two columns: sex and estimate for the estimated proportion with a four-year degree.\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    # Define focal_weight to hold the selected weight\n    mutate(focal_weight = data |&gt; pull(weight_name)) |&gt;\n    # Restrict to those age 25+\n    filter(age &gt;= 25) |&gt;\n    # Restrict to valid reports of education\n    filter(educ &gt; 1 & educ &lt; 999) |&gt;\n    # Define a binary outcome: a four-year degree\n    mutate(college = educ &gt;= 110) |&gt;\n    # Estimate weighted means by sex\n    group_by(sex) |&gt;\n    summarize(estimate = weighted.mean(\n      x = college,\n      w = focal_weight\n    ))\n}\n\nWe produce a point estimate by applying that estimator with the asecwt.\n\nestimate &lt;- estimator(data = cps_data, weight_name = \"asecwt\")\n\n\n\n# A tibble: 2 × 2\n  sex        estimate\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;\n1 1 [male]      0.369\n2 2 [female]    0.397\n\n\nUsing the foreach package, we apply the estimator 160 times—once with each replicate weight—and use the argument .combine = \"rbind\" to stitch results together by rows.\n\nlibrary(foreach)\nreplicate_estimates &lt;- foreach(r = 1:160, .combine = \"rbind\") %do% {\n  estimator(data = cps_data, weight_name = paste0(\"repwtp\",r))\n}\n\n\n\n# A tibble: 320 × 2\n   sex        estimate\n   &lt;dbl+lbl&gt;     &lt;dbl&gt;\n 1 1 [male]      0.368\n 2 2 [female]    0.396\n 3 1 [male]      0.371\n 4 2 [female]    0.400\n 5 1 [male]      0.371\n 6 2 [female]    0.397\n 7 1 [male]      0.369\n 8 2 [female]    0.397\n 9 1 [male]      0.370\n10 2 [female]    0.398\n# ℹ 310 more rows\n\n\nWe estimate the standard error of our estimator by a formula \\[\\text{StandardError}(\\hat\\tau) = \\sqrt{\\frac{4}{160}\\sum_{r=1}^{160}\\left(\\hat\\tau^*_r - \\hat\\tau\\right)^2}\\] where the formula comes from the survey documentation. We carry out this procedure within groups defined by sex, since we are producing estimate for each sex.\n\nstandard_error &lt;- replicate_estimates |&gt;\n  # Denote replicate estimates as estimate_star\n  rename(estimate_star = estimate) |&gt;\n  # Merge in the point estimate\n  left_join(estimate,\n            by = join_by(sex)) |&gt;\n  # Carry out within groups defined by sex\n  group_by(sex) |&gt;\n  # Apply the formula from survey documentation\n  summarize(standard_error = sqrt(4 / 160 * sum((estimate_star - estimate) ^ 2)))\n\n\n\n# A tibble: 2 × 2\n  sex        standard_error\n  &lt;dbl+lbl&gt;           &lt;dbl&gt;\n1 1 [male]          0.00280\n2 2 [female]        0.00291\n\n\nFinally, we combine everything and construct a 95% confidence interval by a Normal approximation.\n\nresult &lt;- estimate |&gt;\n  left_join(standard_error, by = \"sex\") |&gt;\n  mutate(ci_min = estimate - 1.96 * standard_error,\n         ci_max = estimate + 1.96 * standard_error)\n\n\n\n# A tibble: 2 × 5\n  sex        estimate standard_error ci_min ci_max\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1 [male]      0.369        0.00280  0.364  0.375\n2 2 [female]    0.397        0.00291  0.391  0.403\n\n\nWe use ggplot() to visualize the result.\n\nresult |&gt;\n  mutate(sex = as_factor(sex)) |&gt;\n  ggplot(aes(\n    x = sex, \n    y = estimate,\n    ymin = ci_min, \n    ymax = ci_max,\n    label = scales::percent(estimate)\n  )) +\n  geom_errorbar(width = .2) +\n  geom_label() +\n  scale_x_discrete(\n    name = \"Sex\", \n    labels = str_to_title\n  ) +\n  scale_y_continuous(name = \"Proportion with 4-Year College Degree\") +\n  ggtitle(\n    \"Sex Disparities in College Completion\",\n    subtitle = \"Estimates from the 2023 CPS-ASEC among those age 25+\"\n  )\n\n\n\n\n\n\n\n\nWe conclude that those identifying as female are more likely to hold a college degree. Because we can see the confidence intervals generated using the replicate weights, we are reasonably confident in the statistical precision of our point estimates.",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/3_sampling.html#footnotes",
    "href": "topics/3_sampling.html#footnotes",
    "title": "Population Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, a simple random sample draws units independently with equal probabilities, and with replacement. Our sample is actually drawn without replacement. In an infinite population, the two are equivalent.↩︎\nIf we hypothetically drew many complex survey samples from the population in this way, an interval generated this way would contain the true population mean 95% of the time.↩︎",
    "crumbs": [
      "Syllabus",
      "Getting Started",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/5_linear_regression.html",
    "href": "topics/5_linear_regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Here are slides in website and pdf format.\nAs an example, we will work with U.S. adult income by sex (male, female), age (30–50), and year (2010–2019). We will focus on the target population of those working 35+ hours per week for 50+ weeks per year. Data are simulated based on the 2010–2019 American Community Survey (ACS).\nThe function below will simulate data\nsimulate &lt;- function(n = 100) {\n  read_csv(\"https://ilundberg.github.io/description/assets/truth.csv\") |&gt;\n    slice_sample(n = n, weight_by = weight, replace = T) |&gt;\n    mutate(income = exp(rnorm(n(), meanlog, sdlog))) |&gt;\n    select(year, age, sex, income)\n}\nBelow you can see this function in action.\nsimulated &lt;- simulate(n = 3e4)\n\nRows: 420 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (5): year, age, meanlog, sdlog, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# A tibble: 30,000 × 4\n   year   age sex    income\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  2011    48 female 93676.\n2  2012    38 female 98805.\n3  2013    38 female 52330.\n# ℹ 29,997 more rows",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Linear Regression"
    ]
  },
  {
    "objectID": "topics/5_linear_regression.html#conditional-expectation",
    "href": "topics/5_linear_regression.html#conditional-expectation",
    "title": "Linear Regression",
    "section": "Conditional expectation",
    "text": "Conditional expectation\nA key goal with linear regression is the conditional expectation: the mean of an outcome within a population subgroup.\n\nexpectation refers to taking a mean\nconditional refers to within a subgroup\n\nExample: Mean income among females age 47 in 2019\nSuppose we want to estimate that conditional mean in our data. One way is to first create the subgroup and take the mean among people in that subgroup in our sample.\nfilter() restricts our data to cases meeting requirements:\n\nthe sex variable equals the value female\nthe age variable equals the value 47\nthe year variable equals the value 2019\n\n\nsubgroup &lt;- simulated |&gt;\n  filter(sex == \"female\") |&gt;\n  filter(age == 47) |&gt;\n  filter(year == 2019)\n\nsummarize() aggregates to the mean\n\nsubgroup |&gt;\n  summarize(conditional_expectation = mean(income))\n\n# A tibble: 1 × 1\n  conditional_expectation\n                    &lt;dbl&gt;\n1                  71530.\n\n\nOften, we want to study many conditional expectations: the mean outcome in many subgroups. In previous homework, we have seen how group_by and summarize can yield the mean in many subgroups.\n\nsimulated |&gt;\n  group_by(sex, age, year) |&gt;\n  summarize(conditional_expectation = mean(income))\n\n`summarise()` has grouped output by 'sex', 'age'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 420 × 4\n# Groups:   sex, age [42]\n  sex      age  year conditional_expectation\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;                   &lt;dbl&gt;\n1 female    30  2010                  45928.\n2 female    30  2011                  43688.\n3 female    30  2012                  42714.\n# ℹ 417 more rows\n\n\nIn math, the conditional expectation function is the subgroup mean of \\(Y\\) within a subgroup with the predictor values \\(\\vec{X} = \\vec{x}\\). We use \\(\\text{E}\\) to denote the expectation operator. For simplicity, we will let \\(f()\\) refer to the conditional expectation function, which has input \\(\\vec{x}\\) and outputs a conditional mean among those with \\(\\vec{X} = \\vec{x}\\).\n\\[\nf(\\vec{x}) = \\text{E}(Y\\mid\\vec{X} = \\vec{x})\n\\]\nTo learn \\(f(\\vec{x})\\) from data is a central task in statistical learning.\n#$ Statistical learning by pooling information\nA common problem of statistical inference is that we want to study a subgroup, but there are few cases within the subgroup. For example, female respondents age 47 in 2019 in our simulated data.\n\nsimulated |&gt;\n  filter(sex == \"female\") |&gt;\n  filter(year == 2019) |&gt;\n  filter(age == 47)\n\n# A tibble: 67 × 4\n   year   age sex    income\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  2019    47 female 15761.\n2  2019    47 female 32995.\n3  2019    47 female 83967.\n# ℹ 64 more rows\n\n\nFew cases in a subgroup leads to statistical uncertainty about the mean in the subgroup. How can we better estimate this conditional mean?\nOne strategy is to pool information using a model. We have many female respondents in 2019. The only problem is that few are age 47.\n\nsimulated |&gt;\n  filter(sex == \"female\") |&gt;\n  filter(year == 2019)\n\n# A tibble: 1,427 × 4\n   year   age sex    income\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  2019    32 female 52130.\n2  2019    46 female 17465.\n3  2019    41 female 66012.\n# ℹ 1,424 more rows\n\n\nWe might think that these other respondents (e.g., those age 46 and 48) are informative about the outcomes of 47-year-olds. There are many ways to pool information. A linear regression model is one strategy that pools information across people of all ages, to estimate a conditional mean at any particular age.\n\n\n\n\n\n\n\n\n\nThe model assumes that all of the conditional means fall along a line. Then, it estimates the intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) of this line to best fit those conditional means. Finally, one can use the model to make a prediction at any particular \\(X\\) value.\n\nPractice question\n\\[\n\\text{E}(Y\\mid X) = \\beta_0 + \\beta_1 X\n\\]\nSuppose \\(\\beta_0 = 5\\) and \\(\\beta_1 = 3\\)\n\nWhat is the conditional mean when \\(X = 0\\)?\nWhat is the conditional mean when \\(X = 1\\)?\nWhat is the conditional mean when \\(X = 2\\)?\nHow much does the conditional mean change for each unit increase in \\(X\\)?",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Linear Regression"
    ]
  },
  {
    "objectID": "topics/5_linear_regression.html#coding-a-linear-model",
    "href": "topics/5_linear_regression.html#coding-a-linear-model",
    "title": "Linear Regression",
    "section": "Coding a linear model",
    "text": "Coding a linear model\nCoding a linear model in R is easy. First, generate some data (get the simulate() function from futher up this page).\n\nsimulated &lt;- simulate(n = 3e4)\n\nRows: 420 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (5): year, age, meanlog, sdlog, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFor our example, restrict to female respondents in 2019.\n\nfemale_2019 &lt;- simulated |&gt;\n  filter(sex == \"female\") |&gt;\n  filter(year == 2019)\n\nThen learn a model from the data with the lm() function.\n\nmodel &lt;- lm(\n  formula = income ~ age, \n  data = female_2019\n)\n\nHere is how that code worked:\n\nmodel is an object of class lm for linear model\nlm() function creates this object\nformula argument is a model formula\n\noutcome ~ predictor is the syntax\n\ndata is a dataset containing outcome and predictor\n\nWe can look at the learned model with the summary() function.\n\nsummary(model)\n\n\nCall:\nlm(formula = income ~ age, data = female_2019)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-52689 -29518 -12682  16013 400507 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  47242.6     7518.3   6.284 4.37e-10 ***\nage            233.7      185.7   1.259    0.208    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 43360 on 1437 degrees of freedom\nMultiple R-squared:  0.001102,  Adjusted R-squared:  0.0004064 \nF-statistic: 1.585 on 1 and 1437 DF,  p-value: 0.2083\n\n\nFinally, we might predict at a new X value. First, define the data at which to make the prediction: a person age 47.\n\nto_predict &lt;- tibble(age = 47)\n\nPredict for that subgroup\n\npredict(model, newdata = to_predict)\n\n       1 \n58228.57 \n\n\nTo review, our model pooled information:\n\nPeople of all ages contributed to model\nThen we predicted at a single age\n\n\nPractice question\nBelow is the line fit to the population data. Suppose we want to learn \\(\\text{E}(\\log(Y)\\mid X = 30)\\).\n\n\nRows: 420 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (5): year, age, meanlog, sdlog, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nWhy might this model make a misleading estimate?\nWhy might the model still be useful?",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Linear Regression"
    ]
  },
  {
    "objectID": "topics/5_linear_regression.html#additive-vs-interactive-models",
    "href": "topics/5_linear_regression.html#additive-vs-interactive-models",
    "title": "Linear Regression",
    "section": "Additive vs interactive models",
    "text": "Additive vs interactive models\nBelow, we visualize two models: one for male and one for female respondents.\n\n\n\n\n\n\n\n\n\nThere are two equivalent ways to describe these two models. The first is by thinking of them as separate linear regressions.\n\\[\n\\begin{aligned}\n\\text{E}(Y\\mid X, \\text{Female}) &= \\beta_0^\\text{Female} + \\beta_1^\\text{Female}\\times \\text{Age} \\\\\n\\text{E}(Y\\mid X, \\text{Male}) &= \\beta_0^\\text{Male} + \\beta_1^\\text{Male}\\times \\text{Age} \\\\\n\\end{aligned}\n\\]\nThe second way is to think of them as one pooled linear regression that interacts age and sex to allow the slope on age to differ by sex.\n\\[\\text{E}(Y \\mid X, \\text{Sex}) = \\gamma_0 + \\gamma_1(\\text{Female}) + \\gamma_2(\\text{Age}) + \\gamma_3 (\\text{Age} \\times \\text{Female})\\]\nNote that both approaches summarize the conditional mean function with 4 parameters. The two approaches are actually equivalent, as you can show with some algebra.\n\\[\\begin{aligned}\n\\gamma_0 &= \\beta_0^\\text{Male}\n&\\gamma_1 &= \\beta_0^\\text{Female} - \\beta_0^\\text{Male} \\\\\n\\gamma_2 &= \\beta_1^\\text{Male}\n&\\gamma_3 &= \\beta_1^\\text{Female} - \\beta_1^\\text{Male}\n\\end{aligned}\\]\nBelow, we practice writing an interaction in code. First generate data in 2019 that vary in both sex and age.\n\nall_2019 &lt;- simulated |&gt;\n  filter(year == 2019)\n\n\n\n# A tibble: 3,204 × 4\n   year   age sex   income\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  2019    41 male  50285.\n2  2019    45 male  31057.\n3  2019    34 male  66166.\n# ℹ 3,201 more rows\n\n\nThe * operator allows slopes to differ across groups\n\nmodel &lt;- lm(\n  formula = income ~ sex * age,\n  data = all_2019\n)",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Linear Regression"
    ]
  },
  {
    "objectID": "topics/5_linear_regression.html#two-models-additive-model-in-r",
    "href": "topics/5_linear_regression.html#two-models-additive-model-in-r",
    "title": "Linear Regression",
    "section": "Two models: Additive model in R",
    "text": "Two models: Additive model in R\nThe + operator assumes slopes are the same across groups\n\nmodel &lt;- lm(\n  formula = income ~ sex + age,\n  data = all_2019\n)\n\n\n\n\n\n\n\n\n\n\nWhen you have many interactions, the model starts to have lots of terms! This can make interpretation hard. But, you can always use the model to predict any conditional mean you want, even if there are many interactions.\n\nmodel &lt;- lm(\n  formula = income ~ sex * age * year,\n  data = simulated\n)\n\n\nsummary(model)\n\n\nCall:\nlm(formula = income ~ sex * age * year, data = simulated)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-81158 -33849 -14946  15839 972817 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)       1.387e+06  2.343e+06   0.592    0.554\nsexmale          -1.943e+06  3.117e+06  -0.623    0.533\nage              -6.273e+04  5.760e+04  -1.089    0.276\nyear             -6.680e+02  1.163e+03  -0.574    0.566\nsexmale:age       6.646e+04  7.675e+04   0.866    0.386\nsexmale:year      9.519e+02  1.547e+03   0.615    0.538\nage:year          3.130e+01  2.859e+01   1.095    0.274\nsexmale:age:year -3.247e+01  3.809e+01  -0.852    0.394\n\nResidual standard error: 57790 on 29992 degrees of freedom\nMultiple R-squared:  0.03332,   Adjusted R-squared:  0.03309 \nF-statistic: 147.7 on 7 and 29992 DF,  p-value: &lt; 2.2e-16\n\n\nThe many terms of the model correspond to many slopes in subgroups, as visualized below.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Linear Regression"
    ]
  },
  {
    "objectID": "topics/5_linear_regression.html#penalized-regression",
    "href": "topics/5_linear_regression.html#penalized-regression",
    "title": "Linear Regression",
    "section": "Penalized Regression",
    "text": "Penalized Regression\nWe close with a more advanced data science topic: penalized regression. We will discuss a method known as ridge regression or L2 penalized regression. This data science approach is a linear model just like OLS, but estimates the coefficients slightly differently.\nBefore defining this type of penalized regression, below we show what happens when we use it. Each line is a regression estimated on a different sample from the population. What similarities and differences do you notice between the penalized and unpenalized regressions?\n\n\n\n\n\n\n\n\n\nWe could use each approach to predict the mean income among 47-year-olds. Below are those predictions, with one dot from each simulated sample. How do the patterns below align with what you noticed in the graph above?\n\n\n\n\n\n\n\n\n\nThe reason one uses penalized regression is to reduce the sampling variance of estimates, at the cost of some bias (estimates are generally drawn in toward the overall sample mean). To understand how, it is useful to see some math.\nOLS chose \\(\\alpha, \\vec\\beta\\) to minimize this function: \\[\n\\begin{aligned}\n\\underbrace{\\sum_i\\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Sum of Squared Error}\n\\end{aligned}\n\\] where \\(\\hat{Y}_i = \\hat\\alpha + \\sum_j X_j \\hat\\beta_j\\)\nPenalized (ridge) regression chose \\(\\alpha, \\vec\\beta\\) to minimize this function: \\[\n\\begin{aligned}\n\\underbrace{\\sum_i\\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda \\sum_{j} \\beta_j^2}_\\text{Penalty Term}\n\\end{aligned}\n\\] where \\(\\hat{Y}_i = \\hat\\alpha + \\sum_j X_j \\hat\\beta_j\\)\nThe only difference between the two is that penalized regression seeks to avoid having a large value of \\(\\sum_j \\beta_j^2\\). Thus, it prefers to estimate models with coefficients near zero. In practice, researchers often mean-center covariates and outcomes for penalized regression so that this pulls all estimates toward the overall mean.\n\nPenalized regression in code\nPenalized regression is available through many R packages. Here we illustrate with the glmnet package. First, simulate a large sample.\n\nsimulated &lt;- simulate(n = 1e5)\n\nLoad the package.\n\nlibrary(glmnet)\n\nCreate a model matrix of predictors\n\nThis converts the predictor data into matrix form\nEach column will correspond to a coefficient\n\n\nX &lt;- model.matrix(~ age * sex * year, data = simulated)\n\nCreate a vector of the outcomes\n\ny &lt;- simulated |&gt; pull(income)\n\nUse the cv.glmnet function to call the package. For now, we will leave as a black box how it chooses the penalty parameter \\(\\lambda\\).\n\npenalized &lt;- cv.glmnet(\n  x = X,    # model matrix we created\n  y = y,    # outcome vector we created\n  alpha = 0 # penalize sum of beta ^ 2\n)\n\nFinally, make predictions from the model at each observed data point.\n\nyhat &lt;- predict(\n  penalized,\n  newx = X\n)\n\n\nsummary(yhat)\n\n   lambda.1se   \n Min.   :60582  \n 1st Qu.:62568  \n Median :65476  \n Mean   :65063  \n 3rd Qu.:67425  \n Max.   :69405  \n\n\nWhen should you use penalized regression? The key reasons are motivated by the original illustration figures above. You should use penalized regression to reduce the variance of your estimates. This may occur in settings where you have many predictors and few observations, for example. However, there is a cost: penalized regression generally yields biased estimates of conditional means, so the model will be wrong on average.\nIn future classes, we will discuss data-driven ways to choose among the many statistical and machine-learning approaches to estimate conditional mean functions.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Linear Regression"
    ]
  },
  {
    "objectID": "topics/7_forests.html",
    "href": "topics/7_forests.html",
    "title": "Forests",
    "section": "",
    "text": "Here are slides.\nRegression models perform well when the the response surface \\(E(Y\\mid\\vec{X})\\) follows a line (or some other assumed shape). In some settings, however, the response surface may be more complex. There may be nonlinearities and interaction terms that the researcher may not know about in advance. In these settings, one might desire an estimator that adaptively learns the functional form from the data. Trees are one such approach.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#things-we-will-use-in-class",
    "href": "topics/7_forests.html#things-we-will-use-in-class",
    "title": "Forests",
    "section": "Things we will use in class",
    "text": "Things we will use in class\n\nThis is a big topic! This page has lots of material. This section provides the code you can copy to follow along with things we do in class. The later sections are more pedagogical if you are reading the material.\n\nWe will use several packages today that you may not have installed. You can install them all with\n\ninstall.packages(c(\"palmerpenguins\",\"rpart\",\"rpart.plot\",\"grf\",\"DiagrammeR\"))\n\n\nlibrary(palmerpenguins)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(grf)\n\nHere is example code we will use to illustrate learning a tree to whether a penguin is found on the island of Biscoe.\n\nrpart.out &lt;- rpart(island == \"Biscoe\" ~ ., data = penguins)\n\nWe will visualize this tree.\n\nrpart.plot(rpart.out)\n\nWe will estimate a regression forest.\n\npenguins_nonmissing &lt;- penguins |&gt; na.omit()\nX &lt;- model.matrix(island == \"Biscoe\" ~ ., data = penguins_nonmissing)\ny &lt;- penguins_nonmissing |&gt; \n  mutate(biscoe = island == \"Biscoe\") |&gt; \n  pull(biscoe)\nforest &lt;- grf.out &lt;- regression_forest(\n  X = X,\n  Y = y,\n  tune.parameters = \"all\"\n)\n\nWe can extract a tree from the forest.\n\nfirst_tree &lt;- get_tree(forest, index = 1)\nplot(first_tree)\n\nWe can extract predicted values.\n\nyhat &lt;- forest$predictions[,1]\n\nWe can visualize those predicted values.\n\npenguins_nonmissing |&gt;\n  mutate(yhat = yhat) |&gt;\n  ggplot(aes(x = bill_length_mm, y = yhat, color = species)) +\n  geom_point() +\n  labs(\n    y = \"Probability of Living on Biscoe\",\n    x = \"Bill Length\",\n    color = \"Species\"\n  )",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#a-simulation-to-illustrate-trees",
    "href": "topics/7_forests.html#a-simulation-to-illustrate-trees",
    "title": "Forests",
    "section": "A simulation to illustrate trees",
    "text": "A simulation to illustrate trees\n\nFrom here on, the material is intended to be read rather than walked through together in class.\n\nAs an example, the figure below presents some hypothetical data with a binary predictor \\(Z\\), a numeric predictor \\(X\\), and a numeric outcome \\(Y\\).\n\n\nCode\ntrue_conditional_mean &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n  bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n  mutate(mu = z * plogis(10 * (x - .5)))\nsimulate &lt;- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n    mutate(mu = z * plogis(10 * (x - .5))) |&gt;\n    slice_sample(n = sample_size, replace = T) |&gt;\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data &lt;- simulate(1000)\np_no_points &lt;- true_conditional_mean |&gt;\n  ggplot(aes(x = x, color = z, y = mu)) +\n  geom_line(linetype = \"dashed\", size = 1.2) +\n  labs(\n    x = \"Numeric Predictor X\",\n    y = \"Numeric Outcome Y\",\n    color = \"Binary Predictor Z\"\n  ) +\n  theme_bw()\np &lt;- p_no_points +\n  geom_point(data = simulated_data, aes(y = y), size = .2, alpha = .3)\np\n\n\n\n\n\n\n\n\n\nIf we tried to approximate these conditional means with an additive linear model, \\[\\hat{E}_\\text{Linear}(Y\\mid X,Z) = \\hat\\alpha + \\hat\\beta X + \\hat\\gamma Z\\] then the model approximation error would be very large.\n\n\nCode\nbest_linear_fit &lt;- lm(mu ~ x + z, data = true_conditional_mean)\np +\n  geom_line(\n    data = true_conditional_mean |&gt;\n      mutate(mu = predict(best_linear_fit))\n  ) +\n  theme_bw() +\n  ggtitle(\"An additive linear model (solid lines) poorly approximates\\nthe true conditional mean function (dashed lines)\")",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#how-a-tree-works",
    "href": "topics/7_forests.html#how-a-tree-works",
    "title": "Forests",
    "section": "How a tree works",
    "text": "How a tree works\nA regression tree begins from a radically different place than regression. Instead of assuming that the response follows some assumed pattern, trees proceed by a much more inductive process: recursive splits.\n\nTrees repeatedly split the data\nWith no model at all, suppose we were to split the sample into two subgroups. For example, we might choose to split on \\(Z\\) and say that all units with z = TRUE are one subgroup while all units with z = FALSE are another subgroup. Or we might split on \\(X\\) and say that all units with x &lt;= .23 are one subgroup and all units with x &gt; .23 are another subgroup. After choosing a way to split the dataset into two subgroups, we would then make a prediction rule: for each unit, predict the mean value of all sampled units who fall in their subgroup. This rule would produce only two predicted values: one prediction per resulting subgroup.\nIf you were designing an algorithm to predict this way, how would you choose to define the split?\nIn regression trees to estimate conditional means, the split is often chosen to minimize the resulting sum of squared prediction errors. Suppose we choose this rule. Suppose we consider splitting on \\(X\\) being above or below each decile of its empirical distribution. Suppose we consider splitting on \\(Z\\) being FALSE or TRUE. The graph below shows the sum of squared prediction error resulting from each rule.\n\n\nCode\nx_split_candidates &lt;- quantile(simulated_data$x, seq(.1,.9,.1))\nz_split_candidates &lt;- .5\nby_z &lt;- simulated_data |&gt;\n  group_by(z) |&gt;\n  mutate(yhat = mean(y)) |&gt;\n  ungroup() |&gt;\n  summarize(sum_squared_error = sum((yhat - y) ^ 2))\nby_x &lt;- foreach(x_split = x_split_candidates, .combine = \"rbind\") %do% {\n  simulated_data |&gt;\n    mutate(left = x &lt;= x_split) |&gt;\n    group_by(left) |&gt;\n    mutate(yhat = mean(y)) |&gt;\n    ungroup() |&gt;\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |&gt;\n    mutate(x_split = x_split)\n}\n\nby_x |&gt;\n  mutate(split = \"If Splitting on X\") |&gt;\n  rename(split_value = x_split) |&gt;\n  bind_rows(\n    by_z |&gt;\n      mutate(split = \"If Splitting on Z\") |&gt;\n      mutate(split_value = .5)\n  ) |&gt;\n  ggplot(aes(x = split_value, y = sum_squared_error)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~split) +\n  labs(\n    x = \"Value on Which to Split into Two Subgroups\",\n    y = \"Resulting Sum of Squared Error\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWith the results above, we would choose to split on \\(Z\\), creating a subpopulation with \\(Z \\leq .5\\) and a subgroup with \\(Z\\geq .5\\). Our prediction function would look like this. Our split very well approximates the true conditional mean function when Z = FALSE, but is still a poor approximator when Z = TRUE.\n\n\nCode\np +\n  geom_line(\n    data = simulated_data |&gt;\n      group_by(z) |&gt;\n      mutate(mu = mean(y))\n  ) +\n  ggtitle(\"Solid lines represent predicted values\\nafter one split on Z\")\n\n\n\n\n\n\n\n\n\nWhat if we make a second split? A regression tree repeats the process and considers making a further split within each subpopulation. The graph below shows the sum of squared error in the each subpopulation of Z when further split at various candidate values of X.\n\n\nCode\n# Split 2: After splitting by Z, only X remains on which to split\nleft_side &lt;- simulated_data |&gt; filter(!z)\nright_side &lt;- simulated_data |&gt; filter(z)\n\nleft_split_candidates &lt;- quantile(left_side$x, seq(.1,.9,.1))\nright_split_candidates &lt;- quantile(right_side$x, seq(.1,.9,.1))\n\nleft_split_results &lt;- foreach(x_split = left_split_candidates, .combine = \"rbind\") %do% {\n  left_side |&gt;\n    mutate(left = x &lt;= x_split) |&gt;\n    group_by(z,left) |&gt;\n    mutate(yhat = mean(y)) |&gt;\n    ungroup() |&gt;\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |&gt;\n    mutate(x_split = x_split)\n} |&gt;\n  mutate(chosen = sum_squared_error == min(sum_squared_error))\n\nright_split_results &lt;- foreach(x_split = right_split_candidates, .combine = \"rbind\") %do% {\n  right_side |&gt;\n    mutate(left = x &lt;= x_split) |&gt;\n    group_by(z,left) |&gt;\n    mutate(yhat = mean(y)) |&gt;\n    ungroup() |&gt;\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |&gt;\n    mutate(x_split = x_split)\n} |&gt;\n  mutate(chosen = sum_squared_error == min(sum_squared_error))\n\nsplit2_results &lt;- left_split_results |&gt; mutate(split1 = \"Among Z = FALSE\") |&gt;\n  bind_rows(right_split_results |&gt; mutate(split1 = \"Among Z = TRUE\"))\n\nsplit2_results |&gt;\n  ggplot(aes(x = x_split, y = sum_squared_error)) +\n  geom_line(color = 'gray') +\n  geom_point(aes(color = chosen)) +\n  scale_color_manual(values = c(\"gray\",\"blue\")) +\n  facet_wrap(~split1) +\n  theme_bw() +\n  labs(\n    x = \"X Value on Which to Split into Two Subgroups\",\n    y = \"Resulting Sum of Squared Error\"\n  )\n\n\n\n\n\n\n\n\n\nThe resulting prediction function is a step function that begins to more closely approximate the truth.\n\n\nCode\nsplit2_for_graph &lt;- split2_results |&gt;\n  filter(chosen) |&gt;\n  mutate(z = as.logical(str_remove(split1,\"Among Z = \"))) |&gt;\n  select(z, x_split) |&gt;\n  right_join(simulated_data, by = join_by(z)) |&gt;\n  mutate(x_left = x &lt;= x_split) |&gt;\n  group_by(z, x_left) |&gt;\n  mutate(yhat = mean(y))\n\np +\n  geom_line(\n    data = split2_for_graph,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines represent predicted values\\nafter two splits on (Z,X)\")\n\n\n\n\n\n\n\n\n\nHaving made one and then two splits, the figure below shows what happens when each subgroup is the created by 4 sequential splits of the data.\n\n\nCode\nlibrary(rpart)\nrpart.out &lt;- rpart(\n  y ~ x + z, data = simulated_data, \n  control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4)\n)\np +\n  geom_step(\n    data = true_conditional_mean |&gt;\n      mutate(mu_hat = predict(rpart.out, newdata = true_conditional_mean)),\n    aes(y = mu_hat)\n  ) +\n  ggtitle(\"Prediction from regression tree grown to depth 4\")\n\n\n\n\n\n\n\n\n\nThis prediction function is called a regression tree because of how it looks when visualized a different way. One begins with a full sample which then “branches” into a left and right part, which further “branch” off in subsequent splits. The terminal nodes of the tree—subgroups defined by all prior splits—are referred to as “leaves.” Below is the prediction function from above, visualized as a tree. This visualization is made possible with the rpart.plot package which we practice further down the page.\n\n\nCode\nlibrary(rpart.plot)\nrpart.plot::rpart.plot(rpart.out)",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#your-turn-fit-a-regression-tree",
    "href": "topics/7_forests.html#your-turn-fit-a-regression-tree",
    "title": "Forests",
    "section": "Your turn: Fit a regression tree",
    "text": "Your turn: Fit a regression tree\nUsing the rpart package, fit a regression tree like the one above. First, load the package.\n\nlibrary(rpart)\n\nThen use this code to simulate data. \n\nsimulate &lt;- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n    mutate(mu = z * plogis(10 * (x - .5))) |&gt;\n    slice_sample(n = sample_size, replace = T) |&gt;\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data &lt;- simulate(1000)\n\nUse the rpart function to grow a tree.\n\nrpart.out &lt;- rpart(y ~ x + z, data = simulated_data)\n\nFinally, we can define a series of predictor values at which to make predictions,\n\nto_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n\nand then make predictions\n\npredicted &lt;- predict(rpart.out, newdata = to_predict)\n\nand visualize in a plot.\n\nto_predict |&gt;\n  mutate(yhat = predicted) |&gt;\n  ggplot(aes(x = x, y = yhat, color = z)) +\n  geom_step()\n\n\n\n\n\n\n\n\nWhen you succeed, there are a few things you can try:\n\nVisualize the tree using the rpart.plot() function applied to your rpart.out object\nAttempt a regression tree using the baseball_population.csv data\nTry different specifications of the tuning parameters. See the control argument of rpart, explained at ?rpart.control. To produce a model with depth 4, we previously used the argument control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4).",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#tuning-parameter-depth",
    "href": "topics/7_forests.html#tuning-parameter-depth",
    "title": "Forests",
    "section": "Tuning parameter: Depth",
    "text": "Tuning parameter: Depth\nHow deep should one make a tree? Recall that the depth of the tree is the number of sequential splits that define a leaf. The figure below shows relatively shallow trees (depth = 2) and relatively deep trees (depth = 4) learned over repeated samples. What do you notice about performance with each choice?\n\n\nCode\nestimator &lt;- function(maxdepth) {\n  foreach(rep = 1:3, .combine = \"rbind\") %do% {\n    this_sample &lt;- simulate(100)\n    rpart.out &lt;- rpart(y ~ x + z, data = this_sample, control = rpart.control(minsplit = 2, cp = 0, maxdepth = maxdepth))\n    true_conditional_mean |&gt;\n      mutate(yhat = predict(rpart.out, newdata = true_conditional_mean),\n             maxdepth = maxdepth,\n             rep = rep)\n  }\n}\nresults &lt;- foreach(maxdepth_value = c(2,5), .combine = \"rbind\") %do% estimator(maxdepth = maxdepth_value)\np_no_points +\n  geom_line(\n    data = results |&gt; mutate(maxdepth = case_when(maxdepth == 2 ~ \"Shallow Trees\\nDepth = 2\", maxdepth == 5 ~ \"Deep Trees\\nDepth = 5\")),\n    aes(group = interaction(z,rep), y = yhat)\n  ) +\n  facet_wrap(\n    ~maxdepth\n  )\n\n\n\n\n\n\n\n\n\nShallow trees yield predictions that tend to be more biased because the terminal nodes are large. At the far right when z = TRUE and x is large, the predictions from the shallow trees are systematically lower than the true conditional mean.\nDeep trees yield predictions that tend to be high variance because the terminal nodes are small. While the flexibility of deep trees yields predictions that are less biased, the high variance can make deep trees poor predictors.\nThe balance between shallow and deep trees can be chosen by various rules of thumb or out-of-sample performance metrics, many of which are built into functions like rpart. Another way out is to move beyond trees to forests, which involve a simple extension that yields substantial improvements in performance.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#what-to-read",
    "href": "topics/7_forests.html#what-to-read",
    "title": "Forests",
    "section": "What to read",
    "text": "What to read\nTo read more on trees, see Ch 8.4 of Efron & Hastie (2016).",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#forests",
    "href": "topics/7_forests.html#forests",
    "title": "Forests",
    "section": "Forests",
    "text": "Forests\nWe saw previously that a deep tree is a highly flexible learner, but one that may have poor predictive performance due to its high sampling variance. Random forests (Breiman 2001) resolve this problem in a simple but powerful way: reduce the variance by averaging the predictions from many trees. The forest is the average of the trees.\nIf one simply estimated a regression tree many times on the same data, every tree would be the same. Instead, each time a random forest grows a tree it proceeds by:\n\nbootstrap a sample \\(n\\) of the \\(n\\) observations chosen with replacement\nrandomly sample some number \\(m\\) of the variables to consider for splitting\n\nThere is an art to selection of the tuning parameter \\(m\\), as well as the parameters of the tree-growing algorithm. But most packages can select these tuning parameters automatically. The more trees you grow, the less the forest-based predictions will be sensitive to the stochastic variability that comes from the random sampling of data for each tree.\n\nIllustration with bagged forest\nTo illustrate, we generate data by the same process as on the trees page.\n\n\nCode\ntrue_conditional_mean &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n  bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n  mutate(mu = z * plogis(10 * (x - .5)))\nsimulate &lt;- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |&gt;\n    mutate(mu = z * plogis(10 * (x - .5))) |&gt;\n    slice_sample(n = sample_size, replace = T) |&gt;\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data &lt;- simulate(1000)\n\n\nFor illustration, we will first consider a simple version of random forest that is a bagging estimator: all predictors are included in every tree and variance is created through bagging, or bootstrap aggregating. The code below builds intuition, and the code later using the regression_forest function from the grf package is one way we would actually recommend learning a forest in practice.\n\ntree_estimates &lt;- foreach(tree_index = 1:100, .combine = \"rbind\") %do% {\n  # Draw a bootstrap sample of the data\n  simulated_data_star &lt;- simulated_data |&gt;\n    slice_sample(prop = 1, replace = T)\n  # Learn the tree\n  rpart.out &lt;- rpart(\n    y ~ x + z, data = simulated_data_star, \n    # Set tuning parameters to grow a deep tree\n    control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4)\n  )\n  # Define data to predict\n  to_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n  # Make predictions\n  predicted &lt;- to_predict |&gt;\n    mutate(\n      yhat = predict(rpart.out, newdata = to_predict),\n      tree_index = tree_index\n    )\n  return(predicted)\n}\n\nWe can then aggregate the tree estimates into a forest prediction by averaging over trees.\n\nforest_estimate &lt;- tree_estimates |&gt;\n  group_by(z,x) |&gt;\n  summarize(yhat = mean(yhat), .groups = \"drop\")\n\nThe forest is very good at approximating the true conditional mean.\n\n\nCode\np_no_points &lt;- true_conditional_mean |&gt;\n  ggplot(aes(x = x, color = z, y = mu)) +\n  geom_line(linetype = \"dashed\", size = 1.2) +\n  labs(\n    x = \"Numeric Predictor X\",\n    y = \"Numeric Outcome Y\",\n    color = \"Binary Predictor Z\"\n  ) +\n  theme_bw()\np_no_points +\n  geom_line(\n    data = forest_estimate,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are forest predictions.\\nDashed lines are the true conditional mean.\")",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#your-turn-a-random-forest-with-grf",
    "href": "topics/7_forests.html#your-turn-a-random-forest-with-grf",
    "title": "Forests",
    "section": "Your turn: A random forest with grf",
    "text": "Your turn: A random forest with grf\nIn practice, it is helpful to work with a function that can choose the tuning parameters of the forest for you. One such function is the regression_forest() function in the grf package.\n\nlibrary(grf)\n\nTo illustrate its use, we first produce a matrix X of predictors and a vector Y of outcome values.\n\nX &lt;- model.matrix(~ x + z, data = simulated_data)\nY &lt;- simulated_data |&gt; pull(y)\n\nWe then estimate the forest with the regression_forest() function, here using the tune.parameters = \"all\" argument to allow automated tuning of all parameters.\n\nforest &lt;- regression_forest(\n  X = X, Y = Y, tune.parameters = \"all\"\n)\n\nWe can extract one tree from the forest with the get_tree() function and then visualize with the plot() function.\n\nfirst_tree &lt;- get_tree(forest, index = 1)\nplot(first_tree)\n\nTo predict in a new dataset requires a new X matrix,\n\nto_predict &lt;- tibble(z = F, x = seq(0,1,.001)) |&gt;\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n\nX_to_predict &lt;- model.matrix(~ x + z, data = to_predict)\n\nwhich can then be used to make predictions.\n\nforest_predicted &lt;- to_predict |&gt;\n  mutate(\n    yhat = predict(forest, newdata = X_to_predict) |&gt; \n      pull(predictions)\n  )\n\nWhen we visualize, we see that the forest from the package is also a good approximator of the conditional mean function. It is possible that the bias of this estimated forest arises from tuning parameters that did not grow sufficiently deep trees.\n\n\nCode\np_no_points +\n  geom_line(\n    data = forest_predicted,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are grf::regression_forest() predictions.\\nDashed lines are the true conditional mean.\")\n\n\n\n\n\n\n\n\n\nOnce you have learned a forest yourself, you might try a regression forest using the baseball_population.csv data or another dataset of your choosing.",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#forests-as-adaptive-nearest-neighbors",
    "href": "topics/7_forests.html#forests-as-adaptive-nearest-neighbors",
    "title": "Forests",
    "section": "Forests as adaptive nearest neighbors",
    "text": "Forests as adaptive nearest neighbors\nA regression tree can be interpreted as an adaptive nearest-neighbor estimator: the prediction at predictor value \\(\\vec{x}\\) is the average outcome of all its neighbors, where neighbors are defined as all sampled data points that fall in the same leaf as \\(\\vec{x}\\). The estimator is adaptive because the definition of the neighborhood around \\(\\vec{x}\\) was learned from the data.\nRandom forests can likewise be interpreted as weighted adaptive nearest-neighbor estimators. For each unit \\(i\\), the predicted value is the average outcome of all other units where each unit \\(j\\) is weighted by the frequency with which it falls in the same leaf as unit \\(i\\). Seeing forest-based predictions as a weighted average of other units’ outcomes is a powerful perspective that has led to new advances in forests for uses that go beyond standard regression (Athey, Tibshirani, & Wager 2019).",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#forests-for-causal-inference",
    "href": "topics/7_forests.html#forests-for-causal-inference",
    "title": "Forests",
    "section": "Forests for causal inference",
    "text": "Forests for causal inference\nTBD",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/7_forests.html#what-to-read-1",
    "href": "topics/7_forests.html#what-to-read-1",
    "title": "Forests",
    "section": "What to read",
    "text": "What to read\nTo read more on trees, see Ch 17 of Efron & Hastie (2016).",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Forests"
    ]
  },
  {
    "objectID": "topics/9_are_complex_better.html",
    "href": "topics/9_are_complex_better.html",
    "title": "Are Complex Models Better?",
    "section": "",
    "text": "Topic for 2/4.\n\nMany data science coursess emphasize complex statistical learners, like forests. But are these models actually much better than classic statistical models such as OLS? They are superior in some settings, but the gains may be smaller in other settings. This course meeting will discuss the relative performance of simple and complex models and the reasons researchers may prefer one over the other. We will use Salganik et al. (2020) as an example illustration that compares simple and complex models in one setting.\n\n\n\n Back to top",
    "crumbs": [
      "Syllabus",
      "Models for Subgroup Summaries",
      "Are Complex Models Better?"
    ]
  }
]