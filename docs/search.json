[
  {
    "objectID": "topics/data_transformation.html",
    "href": "topics/data_transformation.html",
    "title": "Data transformation",
    "section": "",
    "text": "This topic is covered on Jan 16. If you want to download a full .qmd instead of copying codes from the website, you can use data_transformation_exercise.qmd.\nThis exercise examines how income inequality has changed over time in the U.S. We will measure inequality by the 10th, 50th, and 90th percentiles of wage and salary income from 1962 to 2022.1 The goal is to produce a graph like this one.",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#prepare-your-r-environment",
    "href": "topics/data_transformation.html#prepare-your-r-environment",
    "title": "Data transformation",
    "section": "Prepare your R environment",
    "text": "Prepare your R environment\nIn RStudio, create a Quarto document. Save it in your working directory. Run the following code to load two packages we will use. If you do not have these packages, first run install.packages(\"tidyverse\") and install.packages(\"haven\") to install the packages.\n\nlibrary(tidyverse)\nlibrary(haven)",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#data-access",
    "href": "topics/data_transformation.html#data-access",
    "title": "Data transformation",
    "section": "Data access",
    "text": "Data access\nThis exercise uses data from the Current Population Survey, provided via IPUMS. We have two versions of the data:\n\nsimulated data made available to you via the course website\nactual data, for which you will register with the data provider\n\nIn lecture, we will use simulated data. In discussion, your TA will walk through the process to access the actual data. You will ultimately need to access the actual data for future assignments in this class.\n\nAccessing simulated data\nThe simulated data are designed to have the same statistical properties as the actual data. To access the simulated data, copy the following line of code into your R script and run it. This line loads the data and stores it in an object called cps_data.\n\n\ncps_data &lt;- read_dta(\"https://soc114.github.io/data/simulated_cps_data.dta\")\n\n\n\nAccessing actual data\nAccessing the actual data is important for future assignments. You may also use these data in your project. Here are instructions to access the data:\n\nRegister for an account at cps.ipums.org\nLog in\nClick “Get Data”\nAdd the following variables to your cart: incwage, educ, wkswork2, age, asecwt\nAdd the 1962–2023 ASEC samples to your cart. Exclude the basic monthly samples\nCreate a data extract\n\nSelect cases to only download people ages 30–45\nChoose to download in Stata (.dta) format\n\nSubmit your extract and download the data!\n\nStore your data in a working directory: a folder on your computer that will hold the data for this exercise. Load the data using the read_dta function in the haven package.\n\ncps_data &lt;- read_dta(\"your_downloaded_file_name.dta\")\n\n\n\n\n\n\n\nTip\n\n\n\n\nChange the file name to the name of the file you downloaded\nIf R says the file does not exist in your current working directory, you may need to set your working directory by clicking Session -&gt; Set Working Directory -&gt; To Source File Location on a Mac or Tools -&gt; Change Working Directory on Windows.",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#explore-the-data",
    "href": "topics/data_transformation.html#explore-the-data",
    "title": "Data transformation",
    "section": "Explore the data",
    "text": "Explore the data\nType cps_data in the console. Some columns such as educ have a numeric code and a label. The code is how IPUMS has stored the data. The label is what the code means. You can always find more documentation explaining the labels on the IPUMS-CPS website.",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#filter-to-cases-of-interest",
    "href": "topics/data_transformation.html#filter-to-cases-of-interest",
    "title": "Data transformation",
    "section": "filter() to cases of interest",
    "text": "filter() to cases of interest\n\nIn this step, you will use filter() to convert your cps_data object to a new object called filtered.\n\nThe filter() function keeps only rows in our dataset that correspond to those we want to study. The examples on the documentation page are especially helpful. The R4DS section is also helpful.\nHere are two ways to use filter() to restrict to people working 50+ weeks per year. One way is to call the filter() function and hand it two arguments\n\n.data = cps_data is the dataset\nyear == 1962 is a logical condition coded TRUE for observations in 1962\n\n\nfilter(.data = cps_data, year == 1962)\n\nThe result of this call is a tibble with only the observations from 1962. Another way to do the same operation is with the pipe operator |&gt;\n\ncps_data |&gt;\n  filter(year == 1962)\n\nThis approach begins with the data set cps_data. The pipe operator |&gt; hands this data set on as the first argument to the filter() function in the next line. As before, the second argument is the logical condition year == 1962.\nThe piping approach is often preferable because it reads like a sentence: begin with data, then filter to cases with a given condition. The pipe is also useful\nThe pipe operator |&gt; takes what is on the first line and hands it on as the first argument to the function in the next line. This reads in a sentence: begin with the cps_data tibble and then filter() to cases with year == 1962. The pipe can also string together many operations, with comments allowed between them:\n\ncps_data |&gt;\n  # Restrict to 1962\n  filter(year == 1962) |&gt;\n  # Restrict to ages 40-44\n  filter(age &gt;= 40 & age &lt;= 44)\n\nYour turn. Begin with the cps_data dataset. Filter to\n\npeople working 50+ weeks per year (check documentation for wkswork2)\nvalid report of incwage greater than 0 and less than 99999998\n\n\n\nShow the code answer\nfiltered &lt;- cps_data |&gt;\n  # Subset to cases working full year\n  filter(wkswork2 == 6) |&gt;\n  # Subset to cases with valid income\n  filter(incwage &gt; 0 & incwage &lt; 99999998)\n\n\n\n\n\n\n\n\nNote\n\n\n\nFiltering can be a dangerous business! For example, above we dropped people with missing values of income. But what if the lowest-income people refuse to answer the income question? We often have no choice but to filter to those with valid responses, but you should always read the documentation to be sure you understand who you are dropping and why.",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#group_by-and-summarize-for-subpopulation-summaries",
    "href": "topics/data_transformation.html#group_by-and-summarize-for-subpopulation-summaries",
    "title": "Data transformation",
    "section": "group_by() and summarize() for subpopulation summaries",
    "text": "group_by() and summarize() for subpopulation summaries\n\nIn this step, you will use group_by() and summarize() to convert your mutated object to a new object called summarized.\n\nEach row in our dataset is a person. We want a dataset where each row is a year. To get there, we will group our data by year and then summarize each group by a set of summary statistics.\n\nIntroducing summarize() with the sample mean\nTo see how summarize() works, let’s first summarize the sample mean income within each year. The input has one row per person. The result has one row per group. For each year, it records the sample mean income.\n\nfiltered |&gt;\n  group_by(year) |&gt;\n  summarize(mean_income = mean(incwage))\n\n# A tibble: 62 × 2\n    year mean_income\n   &lt;dbl&gt;       &lt;dbl&gt;\n 1  1962       6383.\n 2  1963       5831.\n 3  1964       6688.\n 4  1965       6066.\n 5  1966       6438.\n 6  1967       6745.\n 7  1968       7244.\n 8  1969       8465.\n 9  1970       9198.\n10  1971       8490.\n# ℹ 52 more rows\n\n\n\n\n\nUsing summarize() with weighted quantiles\nInstead of the mean, we plan to use three other summary statistics: the 10th, 50th, and 90th percentiles of income. We also want to incorporate the sampling weights provided with the Current Population Survey, in order to summarize the population instead of the sample.\nWe will use the wtd.quantile function to create weighted quantiles. This function is available in the Hmisc package. If you don’t have that package, install it with install.packages(\"Hmisc\"). Using the Hmisc package is tricky, because it has some functions with the same name as functions that we use in the tidyverse. Instead of loading the whole package, we will only load the functions we are using at the time we use them. Whenever we want to calculate a weighted quantile, we will call it with the code packagename::functionname() which in this case is Hmisc::wtd.quantile().\nThe wtd.quantile function will take three arguments:\n\nx is the variable to be summarized\nweights is the variable containing sampling weights\nprobs is the probability cutoffs for the quantiles. For the 10th, 50th, and 90th percentiles we want 0.1, 0.5, and 0.9.\n\nThe code below produces weighted quantile summaries.\n\nsummarized &lt;- filtered |&gt;\n  group_by(year) |&gt;\n  summarize(\n    p10 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.1),\n    p50 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.5),\n    p90 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.9),\n    .groups = \"drop\"\n  )",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#pivot_longer-to-reshape-data",
    "href": "topics/data_transformation.html#pivot_longer-to-reshape-data",
    "title": "Data transformation",
    "section": "pivot_longer() to reshape data",
    "text": "pivot_longer() to reshape data\n\nIn this step, you will use pivot_longer() to convert your summarized object to a new object called pivoted. We first explain why, then explain the task.\n\nWe ultimately want to make a ggplot() where income values are placed on the y-axis. We want to plot the 10th, 50th, and 90th percentiles along this axis, distinguished by color. We need them all in one colun! But currently, they are in three columns.\nHere is the task. How our data look:\n\n\n# A tibble: 62 × 4\n   year   p10   p50    p90\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  1962 1826. 4460. 11733.\n2  1963 1770. 4484. 11934.\n# ℹ 60 more rows\n\n\nHere we want our data to look:\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10       1826.\n2  1962 p50       4460.\n3  1962 p90      11733.\n4  1963 p10       1770.\n5  1963 p50       4484.\n6  1963 p90      11934.\n# ℹ 180 more rows\n\n\nThis way, we can use year for the x-axis, quantity for color, and value for the y-axis.\nUse pivot_longer() to change the first data frame to the second.\n\nUse the cols argument to tell it which columns will disappear\nUse the names_to argument to tell R that the names of those variables will be moved to a column called quantity\nUse the values_to argument to tell R that the values of those variables will be moved to a column called income\n\nIf you get stuck, see how we did it at the end of this page.\n\n\nShow the code answer\npivoted &lt;- summarized %&gt;%\n  pivot_longer(\n    cols = c(\"p10\",\"p50\",\"p90\"),\n    names_to = \"quantity\",\n    values_to = \"income\"\n  )",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#left_join-an-inflation-adjustment",
    "href": "topics/data_transformation.html#left_join-an-inflation-adjustment",
    "title": "Data transformation",
    "section": "left_join() an inflation adjustment",
    "text": "left_join() an inflation adjustment\n\nIn this step, you will use left_join() to merge in an inflation adjustment\n\nA dollar in 1962 bought a lot more than a dollar in 2022. We will adjust for inflation using the Consumer Price Index, which tracks the cost of a standard basket of market goods. We already took this index to create a file inflation.csv,\n\ninflation &lt;- read_csv(\"https://soc114.github.io/data/inflation.csv\")\n\n\n\n# A tibble: 62 × 2\n   year inflation_factor\n  &lt;dbl&gt;            &lt;dbl&gt;\n1  1962            10.1 \n2  1963             9.95\n3  1964             9.82\n# ℹ 59 more rows\n\n\nThe inflation_factor tells us that $1 in 1962 could buy about as much as $10.10 in 2023. To take a 1962 income and report it in 2023 dollars, we should multiple it by 10.1. We need to join our data\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10       1826.\n2  1962 p50       4460.\n3  1962 p90      11733.\n# ℹ 183 more rows\n\n\ntogether with inflation.csv by the linking variable year. Use left_join() to merge inflation_factor onto the dataset pivoted. Below is a hypothetical example for the structure.\n\n# Hypothetical example\njoined &lt;- data_A |&gt;\n  left_join(\n    data_B,\n    by = join_by(key_variable_in_A_and_B)\n  )\n\n\n\nShow the code answer\njoined &lt;- pivoted |&gt;\n  left_join(\n    inflation,\n    by = join_by(year)\n  )",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#mutate-to-adjust-for-inflation",
    "href": "topics/data_transformation.html#mutate-to-adjust-for-inflation",
    "title": "Data transformation",
    "section": "mutate() to adjust for inflation",
    "text": "mutate() to adjust for inflation\n\nIn this step, you will use mutate() to multiple income by the inflation_factor\n\nThe mutate() function modifies columns. It can overwrite existing columns or create new columns at the right of the data set. The new variable is some transformation of the old variables.\n\n# Hypothetical example\nold_data |&gt;\n  mutate(new_variable = old_variable_1 + old_variable_2)\n\nUse mutate() to modify income so that it takes the values income * inflation_factor.\n\n\nShow the code answer\nmutated &lt;- joined |&gt;\n  mutate(income = income * inflation_factor)",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#ggplot-to-visualize",
    "href": "topics/data_transformation.html#ggplot-to-visualize",
    "title": "Data transformation",
    "section": "ggplot() to visualize",
    "text": "ggplot() to visualize\nNow make a ggplot() where\n\nyear is on the x-axis\nincome is on the y-axis\nquantity is denoted by color\n\nDiscuss. What do you see in this plot?",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#all-together",
    "href": "topics/data_transformation.html#all-together",
    "title": "Data transformation",
    "section": "All together",
    "text": "All together\nPutting it all together, we have a pipeline that goes from data to the plot.\n\ncps_data |&gt;\n  # Subset to cases working full year\n  filter(wkswork2 == 6) |&gt;\n  # Subset to cases with valid income\n  filter(incwage &gt; 0 & incwage &lt; 99999998) |&gt;\n  # Produce summaries\n  group_by(year) |&gt;\n  summarize(\n    p10 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.1),\n    p50 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.5),\n    p90 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.9\n    ),\n    .groups = \"drop\"\n  ) |&gt;\n  pivot_longer(\n    cols = c(\"p10\",\"p50\",\"p90\"),\n    names_to = \"quantity\",\n    values_to = \"income\"\n  ) |&gt;\n  # Join data for inflation adjustment\n  left_join(\n    read_csv(\"https://soc114.github.io/data/inflation.csv\"),\n    by = join_by(year)\n  ) |&gt;\n  # Apply the inflation adjustment\n  mutate(income = income * inflation_factor) |&gt;\n  # Produce a ggplot\n  ggplot(aes(x = year, y = income, color = quantity)) +\n  geom_line() +\n  xlab(\"Year\") +\n  scale_y_continuous(name = \"Annual Wage and Salary Income\\n(2023 dollars)\",\n                     labels = scales::label_dollar()) +\n  scale_color_discrete(name = \"Percentile of\\nDistribution\",\n                       labels = function(x) paste0(gsub(\"p\",\"\",x),\"th\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#want-to-do-more",
    "href": "topics/data_transformation.html#want-to-do-more",
    "title": "Data transformation",
    "section": "Want to do more?",
    "text": "Want to do more?\nIf you have finished and want to do more, you could\n\nincorporate the educ variable in your plot. You might want to group by those who do and do not hold college degrees, perhaps using facet_grid()\ntry geom_histogram() for people’s incomes in a specific year\nexplore IPUMS-CPS for other variables and begin your own visualization",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#footnotes",
    "href": "topics/data_transformation.html#footnotes",
    "title": "Data transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to past TA Abby Sachar for designing the base of this exercise.↩︎",
    "crumbs": [
      " ",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/why_model.html",
    "href": "topics/why_model.html",
    "title": "Why model?",
    "section": "",
    "text": "Covered Feb 11. Here are slides. Lecture for today also includes the next page.\nThis page will help you\nIt motivates the model-based estimation that we will use in the next part of the course.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/why_model.html#a-motivating-example",
    "href": "topics/why_model.html#a-motivating-example",
    "title": "Why model?",
    "section": "A motivating example",
    "text": "A motivating example\nSociologists who study household income inequality often focus on two mechanisms. First, incomes vary across individuals. Second, individuals pool together into households. The tendency of high-income individuals to marry other high-income individuals is thus an important process through which household income inequality arises.1\nThis page asks a question about the pooling of people together into households: To what degree does finishing college increase the probability of having a spouse who finished college?",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/why_model.html#data",
    "href": "topics/why_model.html#data",
    "title": "Why model?",
    "section": "Data",
    "text": "Data\nWe use data from the National Longitudinal Survey of Youth 1997 (NLSY97). The NLSY97 is a probability sample of U.S. non-institutional civilian youths age 12–16 on Dec 31 1996 (\\(n\\) = 8,984). Sampled individuals were surveyed annually 1997–2011, then biennially.\nIf you would like to work with the data, you should first prepare your computer with some files from us:\n\nset your working directory where you will be working\ndownload two supporting files from us\n\nnlsy97.NLSY97 is a tagset file containing the variable names\nprepare_nlsy97.R is an R script to prepare the data\n\n\nNow go to the data distributor\n\nRegister with the survey\nLog in to the NLS Investigator\nChoose the NLSY97 study\nUpload the tagset nlsy97.NLSY97 that you downloaded from us\nIn the Investigator, download the data. Type to change the file name from default to nlsy97\nUnzip the file. Drag nlsy97.dat into the folder you will work in\nIn your R console, run the line of code below\n\nthis will take about 30 seconds to run\nyou will need these R packages: tidyverse and Amelia\n\n\n\nlibrary(tidyverse)\n\n\nsource(\"prepare_nlsy97.R\")\n\nIn the future, you can now load the data with\n\nd &lt;- readRDS(\"d.RDS\")",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/why_model.html#a-dag-with-one-confounder",
    "href": "topics/why_model.html#a-dag-with-one-confounder",
    "title": "Why model?",
    "section": "A DAG with one confounder",
    "text": "A DAG with one confounder\nTo draw a DAG, we first define our treatment and outcome variables\n\nTreatment \\(A\\): Finished BA by age 25\nOutcome \\(Y\\): Spouse or partner at age 30–40 holds a BA\n\n0 if no spouse or partner, or partner with no BA\n1 if spouse or partner holds a BA\n\n\nWith these two variables, we ask whether there are any other variables that are common ancestors of these two. One possibility is the sex of the respondent, \\(L\\).\n\n\n\n\n\n\n\n\n\nIf this were the only confounder, our adjustment procedure would be simple:\n\nEstimate within subgroups defined by {sex}\nAggregate over the subgroups\n\nThe code below estimates within the subgroups\n\nybar_in_subgroups &lt;- d |&gt;\n  # Group by confounders and treatment\n  group_by(sex, a) |&gt;\n  # Summarize mean outcomes and nber of cases\n  summarize(ybar = mean(y),\n            n = n(),\n            .groups = \"drop\") |&gt;\n  pivot_wider(names_from = a,\n              values_from = c(\"ybar\",\"n\")) |&gt;\n  print()\n\n# A tibble: 2 × 5\n  sex    ybar_college ybar_no_college n_college n_no_college\n  &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt;     &lt;int&gt;        &lt;int&gt;\n1 Female        0.467           0.102       896         2953\n2 Male          0.614           0.174       637         3285\n\n\nThen we can aggregate over the subgroups, weighted by size.\n\nybar_in_subgroups |&gt;\n  mutate(\n    conditional_effect = ybar_college - ybar_no_college,\n    n_in_stratum = n_college + n_no_college\n  ) |&gt;\n  select(sex, conditional_effect, n_in_stratum) |&gt;\n  summarize(\n    population_average_effect = weighted.mean(\n      conditional_effect,\n      w = n_in_stratum\n    )\n  ) |&gt;\n  print()\n\n# A tibble: 1 × 1\n  population_average_effect\n                      &lt;dbl&gt;\n1                     0.403\n\n\nThe visualization below seeks to build intuitiomn for what we have just done. We split the population into two subgroups (left and right of the solid line). Within each subgroup, we took the mean outcome among those with and without college degrees (above and below dashed line), taking the difference as the effect of a college degree. Then we took the average across the left and right subgroups.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/why_model.html#with-two-confounders",
    "href": "topics/why_model.html#with-two-confounders",
    "title": "Why model?",
    "section": "With two confounders",
    "text": "With two confounders\nSex may not be the only confounder. What if race also shapes access to college and the probability of having a college-educated spouse at age 35?\n\n\n\n\n\n\n\n\n\nOur procedure would be analogous to the above:\n\nEstimate effects within subgroups defined by {sex, race}\nAggregate over subgroups\n\nThe only difference is that now there are 6 subgroups (3 race categories \\(\\times\\) 2 sex categories). Moving from one to two confounders multiplies the number of subgroups.\n\n\n\n\n\n\n\n\n\nBelow are the unadjusted estimates and the estimates adjusted for sex and race.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/why_model.html#with-three-confounders",
    "href": "topics/why_model.html#with-three-confounders",
    "title": "Why model?",
    "section": "With three confounders",
    "text": "With three confounders\nSex and race are not likely to be a sufficient adjustment set. Our DAG should really include at least three confounders:\n\n\n\n\n\n\n\n\n\nProceeding as above, we can make a nonparametric estimate:\n\nEstimate effects within subgroups defined by {race,sex, mom education}\nAggregate over subgroups\n\nThe number of subgroups is starting to get quite large!",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/why_model.html#with-four-confounders",
    "href": "topics/why_model.html#with-four-confounders",
    "title": "Why model?",
    "section": "With four confounders",
    "text": "With four confounders\nNow we suppose that dad’s education is also a confounder.\n\n\n\n\n\n\n\n\n\nThe procedure still holds:\n\nEstimate effects within subgroups defined by {race,sex, mom education, dad education}\nAggregate over subgroups\n\nbut the number of subgroups is now very many.\nNow we have a problem. Some subgroups (highlighted in red) are unpopulated: there are no sampled units in these subgroups. The problem of unpopulated cells happens because we have many confounders, so that there are a huge number of subgroups. This problem is known as the curse of dimensionality.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/why_model.html#curse-of-dimensionality-unpopulated-cells",
    "href": "topics/why_model.html#curse-of-dimensionality-unpopulated-cells",
    "title": "Why model?",
    "section": "Curse of dimensionality: Unpopulated cells",
    "text": "Curse of dimensionality: Unpopulated cells\nThe curse of dimensionality is that the number of subgroups grows multiplicatively with the number of predictors. Nonparametric estimation (estimation by subgroup means) quickly becomes infeasible as the number of subgroups explodes.\nIn fact, in our example, 4.2% of the sample is in a subgroup where there are either zero people with college degrees or zero people without college degrees.\nTo make conditional exchangeability plausible, social scientists often have to draw a DAG with numerous confounding variables.\n\n\n\n\n\n\n\n\n\nBut when we add tons of variables, we have an ever-increasing problem of empty cells. With the above DAG, 100% of the sample falls in a cell where either everyone is treated or everyone is untreated.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/why_model.html#models-to-the-rescue",
    "href": "topics/why_model.html#models-to-the-rescue",
    "title": "Why model?",
    "section": "Models to the rescue",
    "text": "Models to the rescue\nIn many real research settings, the curse of dimensionality makes it impossible to estimate nonparametrically by subgroup means. With more than a few confounders, there are simply too many subgroups and not enough sampled cases to populate them.\nA model is the solution to this problem. A model is a statistical approach to pool information across units who fall in different subgroups, thereby allowing us to make inference even in subgroups where no units are observed. The next page introduces models through a simple descriptive example, and then we will return to causal examples to see how models can help us to answer causal questions.\nIf you would like to read more, you might see Hernán & Robins Ch 11.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/why_model.html#footnotes",
    "href": "topics/why_model.html#footnotes",
    "title": "Why model?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMare 1991, Schwartz 2013↩︎",
    "crumbs": [
      " ",
      "Inference with Models",
      "Why model?"
    ]
  },
  {
    "objectID": "topics/sampling.html",
    "href": "topics/sampling.html",
    "title": "Population sampling",
    "section": "",
    "text": "This topic is covered on Jan 21 and 23.\nClaims about inequality are often claims about a population. Our data are typically only a sample! This module addresses the link between samples and populations.\nThis page covers two lectures.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Population sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#full-count-enumeration",
    "href": "topics/sampling.html#full-count-enumeration",
    "title": "Population sampling",
    "section": "Full count enumeration",
    "text": "Full count enumeration\nWhat proportion of our class prefers to sit in the front of the room?\nWe answered this question in class using full count enumeration: list the entire target population and ask them the question. Full count enumeration is ideal because it removes all statistical sources of error. But in settings with a larger target population, the high cost of full count enumeration may be prohibitive.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Population sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#simple-random-sample",
    "href": "topics/sampling.html#simple-random-sample",
    "title": "Population sampling",
    "section": "Simple random sample",
    "text": "Simple random sample\nWe carried out a simple random sample1 in class.\n\neveryone generated a random number between 0 and 1\nthose with values less than 0.1 were sampled\nour sample estimate was the proportion of those sampled to prefer the front of the room\n\nIn a simple random sample, each person in the population is sampled with equal probabilities. Because the probabilities are known, a simple random sample is a probability sample.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Population sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#unequal-probability-sample",
    "href": "topics/sampling.html#unequal-probability-sample",
    "title": "Population sampling",
    "section": "Unequal probability sample",
    "text": "Unequal probability sample\nSuppose we want to make subgroup estimates:\n\nwhat proportion prefer the front, among those sitting in the first 3 rows?\nwhat proportion prefer the front, among those sitting in the back 17 rows?\n\nIn a simple random sample, we might only get a few or even zero people in the first 3 rows! To reduce the chance of this bad sample, we could draw an unequal probability sample:\n\nthose in rows 1–3 are selected with probability 0.5\nthose in rows 4–20 are selected with probability 0.1\n\nOur unequal probability sample will over-represent the first three rows, thus creating a large enough sample in this subgroup to yield precise estimates.\n\nHaving drawn an unequal probability sample, suppose we now want to estimate the class-wide proportion who prefer sitting in the front. We will have a problem: those who prefer the front may be more likely to sit there, and they are also sampled with a higher probability! Sample inclusion is related to the value of our outcome.\nBecause the sampling probabilities are known, we can correct for this by applying sampling weights, which for each person equals the inverse of the known probability of inclusion for that person.\nFor those in rows 1–3,\n\nwe sampled with probability 50%\non average 1 in every 2 people is sampled\neach person in the sample represents 2 people in the population\n\\(w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.5} = 2\\)\n\nFor those in rows 4–20,\n\nwe sampled with probability 10%\non average 1 in every 10 people is sampled\neach person in the sample represents 10 people in the population\n\\(w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.1} = 10\\)\n\nTo estimate the population mean, we can use the weighted sample mean,\n\\[\\frac{\\sum_i y_iw_i}{\\sum_i w_i}\\]",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Population sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#stratified-random-sample",
    "href": "topics/sampling.html#stratified-random-sample",
    "title": "Population sampling",
    "section": "Stratified random sample",
    "text": "Stratified random sample\nWe could also draw a stratified random sample by first partitioning the population into subgroups (called strata) and then drawing samples within each subgroup. For instance,\n\nsample 10 of the 20 people in rows 1–3\nsample 10 of the 130 people in rows 4–17\n\nIn simple random or unequal probability sampling, it is always possible that by random chance we sample no one in the front of the room. Stratified random sampling rules this out: we know in advance how our sample will be balanced across the two strata.\n\n\n\n\n\n\nNote\n\n\n\nIn our real-data example at the end of this page, the Current Population Survey is stratified by state so that the Bureau of Labor Statistics knows in advance that they will gather a sufficient sample to estimate unemployment in each state.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Population sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#sec-cps",
    "href": "topics/sampling.html#sec-cps",
    "title": "Population sampling",
    "section": "A real case: The Current Population Survey",
    "text": "A real case: The Current Population Survey\nEvery month, the Bureau of Labor Statistics in collaboration with the U.S. Census Bureau collects data on unemployment in the Current Population Survey (CPS). The CPS is a probability sample designed to estimate the unemployment rate in the U.S. and in each state.\nWe will be using the CPS in discussion. This video introduces the CPS and points you toward where you can access the data via IPUMS-CPS.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Population sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#sec-baseball",
    "href": "topics/sampling.html#sec-baseball",
    "title": "Population sampling",
    "section": "Example: Baseball players",
    "text": "Example: Baseball players\nAs one example where full-count enumeration is possible, we will examine the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by USA Today and are available in baseball.csv.\n\nbaseball &lt;- read_csv(\"https://soc114.github.io/data/baseball.csv\")\n\n\n\n# A tibble: 944 × 4\n  player            team         position   salary\n  &lt;chr&gt;             &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;\n1 Scherzer, Max     N.Y. Mets    RHP      43333333\n2 Verlander, Justin N.Y. Mets    RHP      43333333\n3 Judge, Aaron      N.Y. Yankees OF       40000000\n4 Rendon, Anthony   L.A. Angels  3B       38571429\n5 Trout, Mike       L.A. Angels  OF       37116667\n# ℹ 939 more rows\n\n\nSalaries are high, and income inequality is also high among baseball players\n\n4% were paid the league minimum of $720,000\n53% were paid less than $2,000,000\nthe highest-paid players—Max Scherzer and Justin Verlander—each earned $43,333,333\nthe highest-paid half of players take home 92% of the total pay\n\n\n\n\n\n\n\n\n\n\nPay also varies widely across teams!\n\n\n\n\n\n\n\n\n\n\nConceptualize the sampling strategy\nSuppose you did not have the whole population. You still want to learn the population mean salary! How could you learn that in a sample of 60 out of the 944 players?\nBefore reading on, think through three questions:\n\nWhat would it mean to use each of these strategies?\n\n\na simple random sample of 60 players\na sample stratified by the 30 MLB teams\na sample clustered by the 30 MLB teams\n\n\nWhich strategies have advantages in terms of\n\n\nbeing least expensive?\nhaving the best statistical properties?\n\n\nGiven that you already have the population, how would you write some R code to carry out the sampling strategies? You might use sample_n() and possibly group_by().\n\n\n\nSampling strategies in code\nIn a simple random sample, we draw 60 players from the entire league. Each player’s probability of sample inclusion is \\(\\frac{60}{n}\\) where \\(n\\) is the number of players in the league (944).\n\nsimple_sample &lt;- function(population) {\n  population |&gt;\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 60 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Sample 60 players\n    sample_n(size = 60)\n}\n\nTo use this function, we give it the baseball data as the population and it returns a tibble containing a sample of 60 players.\n\nsimple_sample(population = baseball)\n\n# A tibble: 60 × 6\n   player            team          position   salary p_sampled sampling_weight\n   &lt;chr&gt;             &lt;chr&gt;         &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1 Bader, Harrison*  N.Y. Yankees  OF        5200000    0.0636            15.7\n 2 Murfee, Penn      Seattle       RHP        727000    0.0636            15.7\n 3 Henriquez, Ronny* Minnesota     RHP        720900    0.0636            15.7\n 4 Fried, Max        Atlanta       LHP      13500000    0.0636            15.7\n 5 Cobb, Alex        San Francisco RHP       9000000    0.0636            15.7\n 6 Castro, Rodolfo   Pittsburgh    2B         725000    0.0636            15.7\n 7 Kirby, George     Seattle       RHP        758300    0.0636            15.7\n 8 Guillorme, Luis   N.Y. Mets     SS        1600000    0.0636            15.7\n 9 Kreidler, Ryan    Detroit       SS         721600    0.0636            15.7\n10 Jameson, Drey     Arizona       RHP        723600    0.0636            15.7\n# ℹ 50 more rows\n\n\nIn a stratified random sample by team, we sample 2 players on each of 30 teams. A stratified random sample is often a higher-quality sample, because it eliminates the possibility of an unlucky draw that completely omits a few teams. All teams are equally represented no matter what happens in the randomization. The downside of a stratified random sample is that it is costly.\nEach player’s probability of sample inclusion is \\(\\frac{2}{n}\\) where \\(n\\) is the number on that player’s team (which ranges from 28 to 35).\n\nstratified_sample &lt;- function(population) {\n  population |&gt;\n    # Draw sample within each team\n    group_by(team) |&gt;\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 2 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Within each team, sample 2 players\n    sample_n(size = 2) |&gt;\n    ungroup()\n}\n\nIn a sample clustered by team, we might first sample 3 teams and then sample 20 players on each sampled team. A clustered sample is often less costly, for example because you would only need to call up the front office of 3 teams instead of 30 teams. But this type of sample is lower quality, because there is some chance that one will randomly select a few teams that all have particularly high or low average salaries. A clustered random sample is less expensive but is more susceptible to random error based on the clusters chosen.\nEach player’s probability of sample inclusion is P(Team Chosen) \\(\\times\\) P(Chosen Within Team) = \\(\\frac{3}{30}\\times\\frac{20}{n}\\) where \\(n\\) is the number on that player’s team (which ranges from 28 to 35).\n\nclustered_sample &lt;- function(population) {\n  \n  # First, sample 3 teams\n  sampled_teams &lt;- population |&gt;\n    # Make one row per team\n    distinct(team) |&gt;\n    # Sample 3 teams\n    sample_n(3) |&gt;\n    # Store those 3 team names in a vector\n    pull()\n  \n  # Then load data on those teams and sample 20 per team\n  population |&gt;\n    filter(team %in% sampled_teams) |&gt;\n    # Define sampling probability and weight\n    group_by(team) |&gt;\n    mutate(\n      p_sampled = (3 / 30) * (20 / n()),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Sample 20 players\n    sample_n(20) |&gt;\n    ungroup()\n}\n\n\n\nWeighted mean estimator\nGiven a sample, how do we estimate the population mean? The weighted mean estimator can also be placed in a function\n\nwe hand our sample to the function\nwe get a numeric estimate back\n\n\nestimator &lt;- function(sample) {\n  sample |&gt;\n    summarize(estimate = weighted.mean(\n      x = salary, \n      w = sampling_weight\n    )) |&gt;\n    pull(estimate)\n}\n\nHere is what it looks like to use the estimator.\n\nsample_example &lt;- simple_sample(population = baseball)\nestimator(sample = sample_example)\n\n[1] 4768428\n\n\nTry it for yourself! The true mean salary in the league is $4,965,481. How close do you come when you apply the estimator to a sample drawn by each strategy?\n\n\nEvaluating performance: Many samples\nWe might like to know something about performance across many repeated samples. The replicate function will carry out a set of code many times.\n\nsample_estimates &lt;- replicate(\n  n = 1000,\n  expr = {\n    a_sample &lt;- simple_sample(population = baseball)\n    estimator(sample = a_sample)\n  }\n)\n\nSimulate many samples. Which one is the best? Strategy A, B, or C?\n\n\n\nThe danger of one sample\nIn actual science, we typically have only one sample. Any estimate we produce from that sample involves some signal about the population quantities, and also some noise. Herein is the danger: researchers are very good at telling stories about why their sample evidence tells something about the population, even when it may be random noise. We illustrate this with an example.\nDoes salary differ between left- and right-handed pitchers? To address this question, I create a tibble with only the pitchers(those for whom the position variable takes the value LHP or RHP).\n\npitchers &lt;- baseball |&gt;\n  filter(position == \"LHP\" | position == \"RHP\")\n\nTo illustrate what can happen with a sample, we now draw a sample. Let’s first set our computer’s random number seed so we get the same sample each time.\n\nset.seed(1599)\n\nThen draw a sample of 40 pitchers\n\npitchers_sample &lt;- pitchers |&gt;\n  sample_n(size = 40)\n\nand examine the mean difference in salary.\n\npitchers_sample |&gt;\n  group_by(position) |&gt;\n  summarize(salary_mean = mean(salary))\n\n# A tibble: 2 × 2\n  position salary_mean\n  &lt;chr&gt;          &lt;dbl&gt;\n1 LHP         9677309.\n2 RHP         3182428.\n\n\nThe left-handed pitchers make millions of dollars more per year! You can probably tell many stories why this might be the case. Maybe left-handed pitchers are needed by all teams, and there just aren’t many available because so few people are left-handed!\nWhat happens if we repeat this process many times? The figure below shows many repeated samples of size 40 from the population of pitchers.\n\n\n\n\n\n\n\n\n\nOur original result was really random noise: we happened by chance to draw a sample with some highly-paid left-handed pitchers!\nThis exercise illustrates what is known as the replication crisis: findings that are surprising in one sample may not hold in other repeated samples from the same population, or in the population as a whole. The replication crisis has many sources. One principal source is the one we illustrated above: sample-based estimates involve some randomness, and well-meaning researchers are (unfortunately) very good at telling interesting stories.\nOne solution to the replication crisis is to pay close attention to the statistical uncertainty in our estimates, such as that from random sampling. Another solution is to re-evaluate findings that are of interest on new samples. In any case, both the roots of the problem and the solutions are closely tied to sources of randomness in estimates, such as those generated using samples from a population.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Population sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#the-future-of-sample-surveys",
    "href": "topics/sampling.html#the-future-of-sample-surveys",
    "title": "Population sampling",
    "section": "The future of sample surveys",
    "text": "The future of sample surveys\nSample surveys served as a cornerstone of social science research from the 1950s to the present. But there are concerns about their future:\n\nsome sampling frames, such as landline telephones, have become obsolete\nresponse rates have been falling for decades\nsample surveys are slower and more expensive than digital data\n\nWhat is the future for sample surveys? How can they be combined with other data?\nWe will close with a discussion of these questions, which you can also engage with in the Groves 2011 reading that follows this module.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Population sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#footnotes",
    "href": "topics/sampling.html#footnotes",
    "title": "Population sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, a simple random sample draws units independently with equal probabilities, and with replacement. Our sample is actually drawn without replacement. In an infinite population, the two are equivalent.↩︎",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Population sampling"
    ]
  },
  {
    "objectID": "assignments/pset4_in_progress.html",
    "href": "assignments/pset4_in_progress.html",
    "title": "Problem Set 4: Statistical Learning",
    "section": "",
    "text": "Due: 5pm on Friday, Feb 28.\nIdea: - Income prediction challenge. OLS with two predictor sets. Sample split. Which one is better? - OLS with all variables. G-formula for causal effect of college on earnings, in subgroups of parent college\nThis problem set has not yet been posted.\nclass website page is a possibility for data here. Effect of college on subjective social class.\nknitr::opts_chunk$set(eval = FALSE)\nNOTE: This problem set will be updated to explicitly involve statistical learning with more guidance, as well as some causal inference.\nStudent identifer: [type your anonymous identifier here]\nThis problem set is connected to the PSID Income Prediction Challenge from discussion."
  },
  {
    "objectID": "assignments/pset4_in_progress.html#notes",
    "href": "assignments/pset4_in_progress.html#notes",
    "title": "Problem Set 4: Statistical Learning",
    "section": "NOTES",
    "text": "NOTES\nGoals are\n\nSample split\nEstimate MSE\nParametric g-formula outcome\nthere is no ML algorithm on this problem set nor any IPTW\n\nWould be nice to have a nonlinear confounder and a binary treatment.\n\nlibrary(tidyverse)\n# taken from principal stratification \ndata &lt;- readRDS(\"../data_raw/motherhood.RDS\") |&gt;\n  select(\n    treated, employed, age = age_2, sex, race, employed_baseline, \n    educ, marital, fulltime, tenure, experience,\n    weight = w\n  ) |&gt;\n  na.omit()"
  },
  {
    "objectID": "assignments/pset4_in_progress.html#prediction-challenge",
    "href": "assignments/pset4_in_progress.html#prediction-challenge",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Prediction challenge",
    "text": "Prediction challenge\n\ndata &lt;- read_csv(\"data_raw/income_challenge/for_students/learning.csv\") |&gt;\n  mutate(across(contains(\"educ\"), \\(x) factor(x,levels = c(\"Less than high school\",\"High school\",\"Some college\",\"College\"))))"
  },
  {
    "objectID": "assignments/pset4_in_progress.html#ols-prediction",
    "href": "assignments/pset4_in_progress.html#ols-prediction",
    "title": "Problem Set 4: Statistical Learning",
    "section": "1. OLS prediction",
    "text": "1. OLS prediction\nPredict g3_log_income given all other variables by OLS.\n\nfit &lt;- lm(\n  g3_log_income ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data\n)\n\nEstimate the average causal effect of college vs high school.\n\ndata |&gt;\n  mutate(yhat1 = predict(fit, newdata = data |&gt; mutate(g3_educ = \"College\")),\n         yhat0 = predict(fit, newdata = data |&gt; mutate(g3_educ = \"High school\"))) |&gt;\n  summarize(ate = mean(yhat1 - yhat0))\n\nEstimate within subgroups of parents’ education\n\ndata |&gt;\n  mutate(yhat1 = predict(fit, newdata = data |&gt; mutate(g3_educ = \"College\")),\n         yhat0 = predict(fit, newdata = data |&gt; mutate(g3_educ = \"High school\"))) |&gt;\n  group_by(g2_educ) |&gt;\n  summarize(ate = mean(yhat1 - yhat0))"
  },
  {
    "objectID": "assignments/pset4_in_progress.html#causal-forest-prediction",
    "href": "assignments/pset4_in_progress.html#causal-forest-prediction",
    "title": "Problem Set 4: Statistical Learning",
    "section": "2. Causal forest prediction",
    "text": "2. Causal forest prediction\n\nlibrary(grf)\n\n\nfor_forest &lt;- data |&gt;\n      filter(g3_educ %in% c(\"High school\",\"College\"))\n\nfit &lt;- causal_forest(\n  X = model.matrix(\n    ~ race + sex + g2_log_income + g2_educ,\n    data = for_forest\n  ),\n  Y = for_forest$g3_log_income,\n  W = for_forest$g3_educ == \"College\"\n)\n\nsummary(fit)\naverage_treatment_effect(fit, target.sample = \"control\")\n\n\n\nconditi\n\n# Prepare the predictor matrix\nmodel.matrix(fit)\n\nX &lt;- X0 &lt;- X1 &lt;- model.matrix(\n  ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data\n)\nX1[\"\"]\n\nX_factual &lt;- model.matrix(\n  object = ~g3_log_income ~ race + sex,\n  data = data.frame(data)\n)\nX_treated &lt;- model.matrix(\n  ~g3_log_income ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data |&gt; mutate(g3_educ = \"College\")\n)\nX_untreated &lt;- model.matrix(\n  ~g3_log_income ~ race + sex + \n    g1_log_income + g2_log_income + \n    g1_educ + g2_educ*g3_educ,\n  data = data |&gt; mutate(g3_educ = \"High school\")\n)\n\nfit &lt;- regression_forest(\n  X = \n)\n\n\ndata |&gt;\n  group_by(g3_educ) |&gt;\n  summarize(y = mean(g3_log_income)) |&gt;\n  mutate(\n    g3_educ = factor(g3_educ),\n    g3_educ = fct_relevel(g3_educ, \"Less than high school\",\"High school\",\"Some college\",\"College\")\n  ) |&gt;\n  ggplot(aes(x = g3_educ, y = y)) +\n  geom_point() +\n  scale_y_continuous(labels = function(x) scales::label_dollar()(exp(x)))\n\n\ndata |&gt;\n  mutate(\n    yhat_factual = predict(fit_ols),\n    yhat_counterfactual = predict(fit_ols, newdata = data |&gt; mutate(g3_educ = \"College\"))\n  ) |&gt;\n  select(g2_log_income, starts_with(\"yhat\")) |&gt;\n  pivot_longer(cols = -g2_log_income) |&gt;\n  ggplot(aes(x = g2_log_income, y = value, color = name)) +\n  geom_line()"
  },
  {
    "objectID": "assignments/pset4_in_progress.html#income-prediction-challenge",
    "href": "assignments/pset4_in_progress.html#income-prediction-challenge",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Income Prediction Challenge",
    "text": "Income Prediction Challenge\nCollaboration note. This question is an individual write-up connected to your group work from discussion. We expect that the approach you tell us might be the same as that of your other group members, but your answers to these questions should be in your own words.\n1.1 (5 points) How did you choose the predictor variables you used? Correct answers might be entirely conceptual, entirely data-driven, or a mixture of both.\n1.2 (5 points) What learning algorithms or models did you consider, and how did you choose one? Correct answers might be entirely conceptual, entirely data-driven, or a mixture of both.\n1.3 (20 points) Split the learning data randomly into train and test. Your split can be 50-50 or another ratio. Learn in the train set and make predictions in the test set. What do you estimate for your out-of-sample mean squared error? There is no written answer here; the answer is the code and result."
  },
  {
    "objectID": "assignments/pset4_in_progress.html#grad.-machine-learning-versus-statistics",
    "href": "assignments/pset4_in_progress.html#grad.-machine-learning-versus-statistics",
    "title": "Problem Set 4: Statistical Learning",
    "section": "Grad. Machine learning versus statistics",
    "text": "Grad. Machine learning versus statistics\n\nThis question is required for grad students. It is optional for undergrads, and worth no extra credit.\n\n20 points. This question is about the relative gain in this problem as we move from no model to a statistical model to a machine learning model.\nFirst, use your train set to estimate 3 learners and predict in your test set.\n\nNo model. For every test observation, predict the mean of the train outcomes\nOrdinary Least Squares. Choose a set of predictors \\(\\vec{X}\\). For every test observation, predict using a linear model lm() fit to the train set with the predictors \\(\\vec{X}\\).\nMachine learning. Use the same set of predictors \\(\\vec{X}\\). For every test observation, predict using a machine learning model fit to the train set with the predictors \\(\\vec{X}\\). Your machine learning model could be a Generalized Additive Model (gam()), a decision tree (rpart()), or some other machine learning approach.\n\nReport your out-of-sample mean squared error estimates for each approach. How did mean squared error change from (a) to (b)? From (b) to (c)?\nInterpret what you found. To what degree does machine learning improve predictability, beyond what can be achieved by Ordinary Least Squares?"
  },
  {
    "objectID": "topics/visualization.html",
    "href": "topics/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "This topic is covered on Jan 14.\nVisualizing data is an essential skill for data science. We will write our first code to visualize how countries’ level of economic output is related to their level of inequality. We will use data reported in tabular form in Jencks 2002 Table 1, made available in digital form in jencks_table1.csv.",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#a-motivating-question-economic-output-and-inequality",
    "href": "topics/visualization.html#a-motivating-question-economic-output-and-inequality",
    "title": "Visualization",
    "section": "A motivating question: Economic output and inequality",
    "text": "A motivating question: Economic output and inequality\n\nIs economic output higher in countries with higher levels of income inequality? Some argue that this would be the case, for example if high levels of income inequality provide an incentive for hard work and innovation. But is it true?\n\n\n\n\n\n\n\n\n\nTo begin answering this question, we first have to define measures of economic output (the \\(y\\)-axis) and inequality (the \\(x\\)-axis).\n\nMeasuring economic output\nWe use Gross Domestic Product (GDP) Per Capita as a measure of economic output. This measure captures the total economic production of a country divided by the population of the country. For ease of comparison, our data normalizes GDP per capita by dividing by the U.S. GDP per capita. In our data, Sweden’s GDP per capita is recorded as 0.68, indicating that Sweden’s GDP per capita was 68% as high as the U.S. GDP per capita at the time the data were collected.\n\n\nMeasuring inequality\nTo measure inequality, we consider a measure that asks how many times higher an income at the top of the distribution is compared with an income at the bottom of the distribution. Our measure is called the 90/10 income ratio. We will illustrate this concept using the 2022 U.S. household income distribution, visualized below.\n\n\n\n\n\n\n\n\n\nThis graph is a histogram. The width of each bar in the histogram is $25k. The height of each bar shows the number of households with incomes falling within the range of household incomes (x-axis) that correspond to the width of the bar.\nThere are two summary statistics: the 90th percentile in blue and the 10th percentile in green. The 90th percentile is the income value such that 90% of households have incomes that are lower. 90% of the gray mass is to the left of the blue line. The 10th percentile is the income value such that 10% of households have incomes that are lower. We can think of the 90th percentile as a measure of a high income in the distribution and the 10th percentile as a measure of a low income in the distribution.\nThe 90/10 income ratio is the 90th percentile divided by the 10th percentile. For U.S. household incomes in 2022, this works out as\n\\[\n\\text{90/10 ratio} = \\frac{\\text{90th percentile}}{\\text{10th percentile}} = \\frac{$212,110}{$15,640} = 13.6\n\\]\nA higher value of the 90/10 ratio corresponds to higher inequality. In our hypothetical case, a household at the 90th percentile has an income that is 13.6 times as high as the income of a household at the 10th percentile.",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#prepare-the-environment",
    "href": "topics/visualization.html#prepare-the-environment",
    "title": "Visualization",
    "section": "Prepare the environment",
    "text": "Prepare the environment\n\nOpen a new R Script by clicking the button at the top left of RStudio. Save your R Script in a folder you will use for this exercise by clicking File -&gt; Save from the menu at the very top of your screen.\n Paste the code below into your R Script. Place your cursor within the line and hit CMD + Return or CTRL + Enter to run the code and load the tidyverse package.\n\nlibrary(tidyverse)\n\nYou will see action in the console. You have added some functionality to R for this session!\nThe data can be loaded from the course website with the line below.\n\ntable1 &lt;- read_csv(file = \"https://soc114.github.io/data/jencks_table1.csv\")\n\nWhen you run this code, the object table1 will appear in your environment pane.",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#explore-the-data",
    "href": "topics/visualization.html#explore-the-data",
    "title": "Visualization",
    "section": "Explore the data",
    "text": "Explore the data\nType table1 in your console. You can see the data!\n\nThe data contain four variables (columns):\n\ncountry country name\nratio ratio is the 90/10 income ratio in the country\ngdp is GDP per capita in the country, expressed as a proportion of U.S. GDP\nlife_expectancy life expectancy at birth\n\nThere is one row for each country. For details on the data, see Jencks (2002) Table 1.",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#produce-a-visualization",
    "href": "topics/visualization.html#produce-a-visualization",
    "title": "Visualization",
    "section": "Produce a visualization",
    "text": "Produce a visualization\nTo visualize data, we will use the ggplot() function which you have already loaded into your R session as part of the tidyverse package.\n\nBegin with an empty graph\n\nA function in R takes in arguments and returns an object. The arguments are the inputs that we give to the function. The function then returns something back to us.\nThe ggplot() function takes two arguments:\n\ndata = table1 says that data will come from the object table1\nmapping = aes(x = ratio, y = gdp) maps the data to the aesthetics of the graph. This line says that the ratio variable will be placed on the \\(x\\)-axis and the gdp variable will be on the \\(y\\)-axis.\n\nWhen you run this code, the function returns an object which is the resulting plot. The plot will appear in the Plots pane in RStudio.\n\nggplot(\n  data = table1,\n  mapping = aes(x = ratio, y = gdp)\n)\n\n\n\n\n\n\n\n\n\n\nAdd a layer to the graph\nOnce we have an empty graph, we can add elements to the graph in layers. ggplot() is set up to add layers connected by a + symbol between lines. For example, we can add points to the graph by adding the layer geom_point().\n\nggplot(\n  data = table1,\n  mapping = aes(x = ratio, y = gdp)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nImprove labels\n\nggplot(\n  data = table1,\n  mapping = aes(x = ratio, y = gdp)\n) +\n  geom_point() +\n  labs(\n    x = \"Inequality: 90th percentile / 10th percentile of household income\",\n    y = \"GDP as a Fraction of U.S. GDP\"\n  )\n\n\n\n\n\n\n\n\n\n\nCustomize in many ways\n\nThere are many ways to customize the graph. For example, the code below\n\nloads the ggrepel package in order to add country labels to the points\nuses geom_smooth to add a trend line\nuses scale_y_continuous to convert the \\(y\\)-axis labels from decimals to percentages\n\n\nlibrary(ggrepel)\ntable1 |&gt;\n  ggplot(aes(x = ratio, y = gdp)) +\n  geom_point() +\n  geom_smooth(\n    formula = 'y ~ x',\n    method = \"lm\", \n    se = F, \n    color = \"black\"\n  ) +\n  geom_text_repel(\n    aes(label = country),\n    size = 3\n  ) +\n  scale_y_continuous(\n    labels = scales::label_percent(),\n    name = \"GDP as a Percent of U.S.\"\n  ) +\n  scale_x_continuous(name = \"Inequality: 90th percentile / 10th percentile of household income\") +\n  theme(legend.position = \"none\")",
    "crumbs": [
      " ",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/nonparametric_estimation.html",
    "href": "topics/nonparametric_estimation.html",
    "title": "Nonparametric estimation",
    "section": "",
    "text": "If time allows, this will be covered on Feb 4. Here are slides\nUnder the assumption of conditional exchangeability given a sufficient adjustment set \\(\\vec{X}\\), the average causal effect within subgroups defined by \\(\\vec{X}\\) is identified by the difference in means across the treatment \\(A\\) within this subgroup.\nThis page walks through how to estimate that difference in means and (optinally) re-aggregate the conditional average causal effects to average causal effect estimates. We illustrate in a simple case represented by the DAG below.\nIn our illustration, the treatment a is binary and the outcome y is numeric. The confounder x is a sufficient adjustment set and takes the value 0, 1, or 2.\nYou can simulate a dataset of 1,000 observations by running the code below.\nlibrary(tidyverse)\nsimulated &lt;- tibble(id = 1:1000) |&gt;\n  mutate(\n    x = rbinom(n(), 2, .5),\n    pi = case_when(\n      x == 0 ~ .3,\n      x == 1 ~ .5,\n      x == 2 ~ .8\n    ),\n    a = rbinom(n(), 1, pi),\n    y = rnorm(n(), mean = x*a),\n    # For illustration, we include a sampling weight variable\n    # because in real settings there may be sampling weights\n    sampling_weight = 1\n  )",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Nonparametric estimation"
    ]
  },
  {
    "objectID": "topics/nonparametric_estimation.html#estimate-cates-by-subgroup-means",
    "href": "topics/nonparametric_estimation.html#estimate-cates-by-subgroup-means",
    "title": "Nonparametric estimation",
    "section": "Estimate CATEs by subgroup means",
    "text": "Estimate CATEs by subgroup means\nTo estimate the conditional average effect, we first group the data by the confounder x and treatment a. By conditional exchangeability, the rows with a == TRUE and the rows with a == FALSE are each a simple random sample of all rows within the subgroup defined by x, so we can estimate the mean outcome \\(\\E(Y^a\\mid X = x)\\) by the subgroup sample mean, \\(\\E(Y\\mid A = a, X = x)\\).\n\\[\n\\hat{\\text{E}}\\left(Y^a\\mid X = x\\right) = \\underbrace{\\frac{\\sum_{i:X_i=x,A_i=a}Y_i}{n_{x,a}}}_{\\substack{\\text{Sample mean}\\\\\\text{in subgroup}\\\\X = x, A = a}}\n\\]\n\naverage_potential_outcomes &lt;- simulated |&gt;\n  # Group by confounders and treatment\n  group_by(x,a) |&gt;\n  # Summarize weighted mean of Y, weighted by sampling weights\n  summarize(\n    mean_y = weighted.mean(y, w = sampling_weight), \n    .groups = \"drop\"\n  ) |&gt;\n  print()\n\n# A tibble: 6 × 3\n      x     a  mean_y\n  &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n1     0     0 -0.0384\n2     0     1  0.188 \n3     1     0  0.0994\n4     1     1  0.953 \n5     2     0 -0.0851\n6     2     1  1.95  \n\n\nAs expected, the code above produces estimates for the mean of y within subgroups defined by x and a. The conditional average causal effect is difference in y across values of a, within subgroups defined by x. To take this difference, we pivot wider.\n\ncate &lt;- average_potential_outcomes |&gt;\n  # Pivot wider and difference over A to estimate CATE\n  pivot_wider(\n    names_from = a, \n    names_prefix = \"mean_y\", \n    values_from = \"mean_y\"\n  ) |&gt;\n  mutate(cate = mean_y1 - mean_y0) |&gt;\n  print()\n\n# A tibble: 3 × 4\n      x mean_y0 mean_y1  cate\n  &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     0 -0.0384   0.188 0.226\n2     1  0.0994   0.953 0.854\n3     2 -0.0851   1.95  2.03 \n\n\nAs expected, the code above estimates the Conditional Average Treatment Effect (CATE) within each population subgroup defined by the pre-treatment variable x.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Nonparametric estimation"
    ]
  },
  {
    "objectID": "topics/nonparametric_estimation.html#re-aggregating-to-ate",
    "href": "topics/nonparametric_estimation.html#re-aggregating-to-ate",
    "title": "Nonparametric estimation",
    "section": "Re-aggregating to ATE",
    "text": "Re-aggregating to ATE\nTo determine the overall average treatment effect, we can re-aggregated the CATE estimates weighted by the size of each stratum: how many people have x == 1, x == 2, and x == 3. First, determine the size of each stratum.\n\\[\n\\hat{\\text{P}}\\left(X = x\\right) = \\frac{1}{n}\\sum_i \\mathbb{I}(X_i = x)\n\\]\n\nstratum_sizes &lt;- simulated |&gt;\n  # Count sum of sampling weight in each stratum\n  count(x, wt = sampling_weight) |&gt;\n  # Convert count to a proportion of the population\n  mutate(stratum_size = n / sum(n)) |&gt;\n  print()\n\n# A tibble: 3 × 3\n      x     n stratum_size\n  &lt;int&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1     0   244        0.244\n2     1   479        0.479\n3     2   277        0.277\n\n\nThen merge and take the weighted mean of CATE over the strata.\n\\[\n\\hat{\\text{E}}\\left(Y^1-Y^0\\right) = \\sum_x \\hat{\\text{P}}(X = x)\\left[\\hat{\\text{E}}\\left(Y^1\\mid X = x\\right) - \\hat{\\text{E}}\\left(Y^0\\mid X = x\\right)\\right]\n\\]\n\nate &lt;- cate |&gt;\n  left_join(stratum_sizes, by = join_by(x)) |&gt;\n  summarize(ate = weighted.mean(cate, w = stratum_size)) |&gt;\n  print()\n\n# A tibble: 1 × 1\n    ate\n  &lt;dbl&gt;\n1  1.03",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Nonparametric estimation"
    ]
  },
  {
    "objectID": "topics/nonparametric_estimation.html#estimate-by-treatment-weights",
    "href": "topics/nonparametric_estimation.html#estimate-by-treatment-weights",
    "title": "Nonparametric estimation",
    "section": "Estimate by treatment weights",
    "text": "Estimate by treatment weights\nOne can equivalently take a sampling view of causal inference. When I observe a unit \\(i\\) with outcome \\(Y_i = Y_i^{A_i}\\), the probability of observing this outcome is the product of the probability that the unit was sampled multiplied by the probability that the unit received treatment value \\(A_i\\), conditional on confounders. Just as one generates sampling weights for descriptive population inference by the inverse of the probability of sample inclusion, one can generate inverse probability of treatment weights for causal inference. The full weight for each unit \\(i\\) is then the product of these two weights:\n\\[\nw_i = \\text{SamplingWeight}_i\\times \\frac{1}{\\text{P}(A_i\\mid X_i)}\n\\]\nThe code below constructs these weights.\n\ndata_weighted &lt;- simulated |&gt;\n  # Within confounder subgroups, determine the probability\n  # of the observed treatment\n  group_by(x) |&gt;\n  mutate(probability_of_a = case_when(\n    # For treated units, proportion treated\n    a == 1 ~ mean(a),\n    # For untreated units, proportion untreated\n    a == 0 ~ mean(1 - a)\n  )) |&gt;\n  # Calculate the total weight as the product\n  # of the sampling weight and the inverse probability of treatment\n  mutate(\n    total_weight = sampling_weight * (1 / probability_of_a)\n  ) |&gt;\n  ungroup() |&gt;\n  print()\n\n# A tibble: 1,000 × 8\n      id     x    pi     a      y sampling_weight probability_of_a total_weight\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n 1     1     1   0.5     0  0.969               1            0.547         1.83\n 2     2     2   0.8     1  3.48                1            0.819         1.22\n 3     3     1   0.5     1  1.71                1            0.453         2.21\n 4     4     0   0.3     0 -0.797               1            0.709         1.41\n 5     5     0   0.3     0 -0.603               1            0.709         1.41\n 6     6     1   0.5     0 -1.98                1            0.547         1.83\n 7     7     1   0.5     0  0.216               1            0.547         1.83\n 8     8     2   0.8     1  1.65                1            0.819         1.22\n 9     9     0   0.3     0 -0.416               1            0.709         1.41\n10    10     2   0.8     1  1.51                1            0.819         1.22\n# ℹ 990 more rows\n\n\nOnce we have weights, we can directly estimate the Average Treatment Effect (ATE) by the mean difference in the weighted average outcomes in each treatment group,\n\\[\n\\hat{\\text{E}}(Y^a) = \\frac{\\sum_{i:A_i=a} w_i Y_i}{\\sum_{i:A_i=a}w_i}\n\\] and report the difference in the estimated \\(\\hat{E}(Y^1)\\) and \\(\\hat{E}(Y^0)\\).\n\ndata_weighted |&gt;\n  # Summarize weighted mean outcomes within treatment groups\n  group_by(a) |&gt;\n  summarize(\n    estimate = weighted.mean(\n      y, \n      w = total_weight\n    )\n  ) |&gt;\n  # Difference across treatment groups and estimate the ATE\n  pivot_wider(names_from = a, names_prefix = \"mean_y\", values_from = estimate) |&gt;\n  mutate(ate = mean_y1 - mean_y0) |&gt;\n  print()\n\n# A tibble: 1 × 3\n  mean_y0 mean_y1   ate\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1  0.0147    1.04  1.03",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Nonparametric estimation"
    ]
  },
  {
    "objectID": "topics/nonparametric_estimation.html#comparing-the-two-estimates",
    "href": "topics/nonparametric_estimation.html#comparing-the-two-estimates",
    "title": "Nonparametric estimation",
    "section": "Comparing the two estimates",
    "text": "Comparing the two estimates\nAbove, we illustrated two strategies to estimate the Average Treatment Effect.\n\nEstimate the CATE by subgroup means of \\(Y\\) within \\(X\\) and \\(A\\), then aggregate across strata weighted by size.\nEstimate the probability of treatment \\(A\\) given \\(X\\), then estimate the ATE by a weighted mean.\n\nYou may notice that the estimates by the two approaches are mathematically identical. At least under nonparametric estimation, one can show that these two estimators are two different ways of thinking about the exact same estimation process.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Nonparametric estimation"
    ]
  },
  {
    "objectID": "topics/nonparametric_estimation.html#concluding-thoughts",
    "href": "topics/nonparametric_estimation.html#concluding-thoughts",
    "title": "Nonparametric estimation",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\nNonparametric estimation is worth knowing because of its simplicity: estimate the conditional average causal effect within subgroups by taking the difference in mean outcomes within subgroups. However, nonparametric estimation only works in practice when the confounding variables take only a few discrete values (in this example, x was always 0, 1, or 2). In realistic settings, there are often many confounding variables that take many values each. For this reason, causal effects are most often estimated in practice by the model-based methods that we will learn next.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Nonparametric estimation"
    ]
  },
  {
    "objectID": "topics/learning_exercise.html",
    "href": "topics/learning_exercise.html",
    "title": "Learning Exercise",
    "section": "",
    "text": "Gender inequality in employment is much greater among new parents than among non-parents. This exercise seeks to estimate the proportion employed among married men and women1 with a 1-year-old child at home. Our data include those with at least one child age 0–18."
  },
  {
    "objectID": "topics/learning_exercise.html#synthetic-data",
    "href": "topics/learning_exercise.html#synthetic-data",
    "title": "Learning Exercise",
    "section": "Synthetic data",
    "text": "Synthetic data\nTo speed data access, we downloaded data from the basic monthly Current Population Survey for all months from 2010–2019. We processed these data, grouped by sex and age of the youngest child, and estimated the proportion employed. We then generated synthetic data: we created a new dataset for you to use with simulated people using these known probabilities.\nSynthetic data is good in our setting for two reasons\n\nwe know the answer\nyou can download the synthetic data right from this website\n\nFor transparency, here is the code with which we created the synthetic data. The line below will load the synthetic data.\n\nparents &lt;- read_csv(\"https://info3370.github.io/data/parents.csv\")\n\nYour synthetic data intentionally omits any parents with child age 1! Here is a graph showing the averages in your data, grouped by child age and sex."
  },
  {
    "objectID": "topics/learning_exercise.html#your-task-predict-at-child-age-1",
    "href": "topics/learning_exercise.html#your-task-predict-at-child-age-1",
    "title": "Learning Exercise",
    "section": "Your task: Predict at child age 1",
    "text": "Your task: Predict at child age 1\nYour task is to answer the question: what proportion are employed among female respondents whose youngest child is 1 year old?\n\nyou can use a model with the other ages\nyou can use strategies from the statistical learning page\nyou can use the male respondents if you think they are helpful\n\nNear the end of discussion, we will ask every table to make one estimate. Then we will reveal the truth and see who is closest!"
  },
  {
    "objectID": "topics/learning_exercise.html#one-approach-to-the-task",
    "href": "topics/learning_exercise.html#one-approach-to-the-task",
    "title": "Learning Exercise",
    "section": "One approach to the task",
    "text": "One approach to the task\nOne approach is to estimate a linear regression model with child_age interacted with sex. We would first create a fitted model object,\n\nmodel &lt;- lm(at_work ~ sex * child_age, data = parents)\n\nthen define the target population to predict\n\ntarget_population &lt;- tibble(sex = \"female\", child_age = 0)\n\nand report a predicted value for the employment rate of female respondents with a 1-year-old youngest child.\n\npredict(model, newdata = target_population)\n\n        1 \n0.5468421"
  },
  {
    "objectID": "topics/learning_exercise.html#footnotes",
    "href": "topics/learning_exercise.html#footnotes",
    "title": "Learning Exercise",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEach married pair need not be of different sex. The data include same-sex couples.↩︎"
  },
  {
    "objectID": "topics/questions_data_can_answer.html",
    "href": "topics/questions_data_can_answer.html",
    "title": "Questions data can answer",
    "section": "",
    "text": "Data science should begin with clarity about what questions data can (and cannot) answer. As a motivating example, we will discuss questions about inequality as introduced in p. 49–53 of this paper:\nWe will emphasize two key ideas in this paper:"
  },
  {
    "objectID": "topics/questions_data_can_answer.html#choosing-a-summary-of-a-distribution",
    "href": "topics/questions_data_can_answer.html#choosing-a-summary-of-a-distribution",
    "title": "Questions data can answer",
    "section": "Choosing a summary of a distribution",
    "text": "Choosing a summary of a distribution\nThe histogram below visualizes the distribution of annual household income across U.S. households as estimated from the 2022 Current Population Survey. To produce this graph, we categorized households into discrete income groups that are each $25,000 wide. The height of each bar corresponds to the number of households falling in that income group. We can see that the most common household income values are below $100,000, but a small number of households have very high incomes that create a long upper tail at the right.\n\n\n\n\n\n\n\n\n\nWe often want to take a distribution and convert it to a single-number summary. One way to summarize the distribution is by its median: the value at which 50% of households have higher incomes and 50% of households have lower incomes.\n\n\n\n\n\n\n\n\n\nThe median is a useful measure of central tendency: it gives a sense of the income value in the middle of the distribution. But it may not give us a good sense of inequality, which requires some sense of the spread of the distribution. The graph below presents an alternative summary. The 90th percentile is the household income value such that 90% of households have lower incomes. The 10th percentile is the value such that 10% of households have lower incomes. The 90/10 income ratio (\\(\\frac{\\text{90th percentile}}{\\text{10th percentile}}\\)) is one measure of inequality: how many dollars does a household at the 90th percentile get for every dollar that goes to a household at the 10th percentile? We might say that inequality is high to the degree that the 90/10 income ratio is large.\n\n\n\n\n\n\n\n\n\nThe median and the 90/10 income ratio are only two of many ways to summarize a distribution. At the end of this course, you will produce your own visualization that can have any summary you want. Before analyzing data, it is often useful to ask ourselves: what summary do we want to produce? Ideally, you will produce a summary that is informative and perhaps surprising to your readers."
  },
  {
    "objectID": "topics/questions_data_can_answer.html#normative-and-objective-claims",
    "href": "topics/questions_data_can_answer.html#normative-and-objective-claims",
    "title": "Questions data can answer",
    "section": "Normative and objective claims",
    "text": "Normative and objective claims\nInequality is a topic that brings out normative commitments: beliefs about how the world should be. Many of us study inequality because of our own moral beliefs in fairness, equality, and opportunity. Inequality is also a topic full of objective facts, such as quantitative evidence describing the U.S. household income distribution. When studying inequality, it is important to distinguish between normative claims (what should be) and objective claims (what is).\n\nNormative claims: An illustration with manna\nNormative claims are the focus of moral philosophy. Because this is a course on data science, we will mostly avoid normative claims. To illustrate the difference between the kinds of approaches useful in the two settings, we consider a story from the Hebrew scriptures as retold by Jencks (2002).\nIn the story, the Israelites are wandering in the desert without food. God provides a form of bread (manna) that falls down from heaven every day. People do not work for their manna; it is provided freely by God. Taking this example, Jencks considers a normative question about inequality: how should the manna be distributed among the people?\n\n\n\nThe Israelites Collecting Manna from Heaven. Austria. About 1400–1410. Unknown artist. Getty Open Content Program. getty.edu/art/collection/object/105T6R\n\n\nJencks presents a utilitarian moral argument. Suppose that (1) we want to maximize overall societal welfare. Suppose also that (2) each person gets diminishing welfare returns to additional amounts of manna: the first pound of manna relieves you from starvation, the second pound is a bit less helpful, and the third pound even less than that. Under premises (1) and (2), it would make no sense to give lots of manna to a few people while others go hungry. Instead, the welfare-maximizing distribution of manna would be one in which each person receives an equal share.\nThe story of the manna illustrates the elements of a moral argument. One must argue for premises, such as the notion that we should maximize welfare and the notion that each person receives diminishing returns from manna. A debate could happen by challenging some of these premises, such as by arguing that a child might need less manna than a full-grown adult to achieve the same level of welfare. Applying these arguments to present-day inequality is further complicated by the fact that money is earned through work instead of falling freely from the sky. Moral arguments about inequality are important and should be the subject of more thorough treatments in classes on moral and political philosophy.\n\n\nObjective claims\nWhile we will refrain from explicit normative arguments, we will produce objective claims about inequality that we or our readers might find normatively troubling. For example, it might be concerning to us that a household at the 90th percentile receives $14 for every $1 received by a household at the 10th percentile. People’s normative commitments are often bound up in their objective beliefs about the world. Consider a person who believes that high inequality is desirable in order to promote economic growth. This person might be surprised by an objective fact: the countries with high levels of economic inequality are not necessarily the same countries that have high levels of economic output per capita. We will visualize this objective fact in the pages to come.\nAs a data scientist, your task is to produce statistical summaries that help people better understand the world. As a scholar of inequality, you might hope that your objective evidence will be troubling to people with normative commitments such as beliefs in equality and fairness. But we will focus on making precise objective claims."
  },
  {
    "objectID": "assignments/pset1.html",
    "href": "assignments/pset1.html",
    "title": "Problem Set 1: Visualization",
    "section": "",
    "text": "Due: 5pm on Friday, January 17.\nStudent identifer: [type your anonymous identifier here]\nThis problem set involves both data analysis and reading.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "assignments/pset1.html#visualize-40-points",
    "href": "assignments/pset1.html#visualize-40-points",
    "title": "Problem Set 1: Visualization",
    "section": "1. Visualize (40 points)",
    "text": "1. Visualize (40 points)\n\n\n\n\n\n\nTip\n\n\n\nFunctions named in this problem are links to helper pages that provide documentation.\n\n\nUse ggplot to visualize these data. To denote the different trajectories,\n\nmake your plot using geom_point() or geom_line()\nuse the x-axis for age\nuse the y-axis for income\nuse color for quantity\nuse facet_grid to make a panel of facets where each row is an education value and each column is a cohort value\n\nYou should prepare the graph as though you were going to publish it. Modify the axis titles so that a reader would know what is on the axis. Use appropriate capitalization in all labels. Optionally, try using the label_currency() function from the scales package so that the y-axis uses dollar values.\nYour code should be well-formatted as defined by R4DS. In your produced PDF, no lines of code should run off the page.\nMany different graphs can be equally correct. You will be evaluated by\n\nhaving publication-ready graph aesthetics\ncode that follows style conventions\n\n\n# your code goes here",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "assignments/pset1.html#understand-components-of-research-question",
    "href": "assignments/pset1.html#understand-components-of-research-question",
    "title": "Problem Set 1: Visualization",
    "section": "2. Understand components of research question",
    "text": "2. Understand components of research question\nThe graph above answered a descriptive research question. Here are a few components of that question:\n\nAnnual income in 2022 dollars\nAmerican workers in each subgroup defined by birth cohort, age, and education\n10th, 50th, and 90th percentile\nA person\n\nFor 2.1–2.4, write the letter of the corresponding component of the research question.\n2.1 (2.5 points) What is the unit of analysis?\n2.2 (2.5 points) What is the outcome variable?\n2.3 (2.5 points) What are the target populations?\n2.4 (2.5 points) What are the summary statistics?",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "assignments/pset1.html#recap-and-connections-to-your-project",
    "href": "assignments/pset1.html#recap-and-connections-to-your-project",
    "title": "Problem Set 1: Visualization",
    "section": "Recap and connections to your project",
    "text": "Recap and connections to your project\nThe visualization you produced in this problem set is an example of the kind of visualization you will produce in the final project. As in this problem set, your visualization in the project should include well-written labels to make the graph readable. A difference between the two is that in this problem set we provided a dataset in which each row was already a summary statistic. In the final project, you will manipulate data in which each row corresponds to a unit of analysis that you aggregate to produce a summary statistic. This is a skill you will practice on the next problem set.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "topics/conditional_exchangeability.html",
    "href": "topics/conditional_exchangeability.html",
    "title": "Conditional exchangeability",
    "section": "",
    "text": "This topic is covered Feb 4. Here are slides.\nConditional exchangeability is an assumption that exchangeability holds within population subgroups. This assumption holds by design in a conditionally randomized experiment (discussed on this page), and may hold under certain causal beliefs in observational settings where the treatment is not randomized (next page).",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Conditional exchangeability"
    ]
  },
  {
    "objectID": "topics/conditional_exchangeability.html#a-conditionally-randomized-experiment",
    "href": "topics/conditional_exchangeability.html#a-conditionally-randomized-experiment",
    "title": "Conditional exchangeability",
    "section": "A conditionally randomized experiment",
    "text": "A conditionally randomized experiment\nSuppose we were to carry out an experiment on a simple random sample of U.S. high school students. Among those performing in the top 25% of their high school class, we randomize 80% to attain a four-year college degree. Among those performing in the bottom 75% of their high school class, we randomize 20% to attain a four-year college degree. We are interested in effects on employment at age 40 (\\(Y\\)).\n\n\n\n\n\n\n\n\n\nThis experiment is conditionally randomized because the probability of treatment (four-year degree) is different among the higher- and lower-performing high school students.\n\nConditionally randomized experiment. An experiment in which the probability of treatment assignment depends on the values of pre-treatment covariates. \\(\\text{P}(A = 1\\mid\\vec{X} = \\vec{x})\\) depends on the value \\(\\vec{x}\\).",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Conditional exchangeability"
    ]
  },
  {
    "objectID": "topics/conditional_exchangeability.html#conditional-exchangeability",
    "href": "topics/conditional_exchangeability.html#conditional-exchangeability",
    "title": "Conditional exchangeability",
    "section": "Conditional exchangeability",
    "text": "Conditional exchangeability\nIn a conditionally randomized experiment, exchangeability is not likely to hold. People who are treated (assigned to a four-year degree) are more likely to have come from the top 25% of their high school class. They might be especially hard-working people. The treated and untreated might have had different employment at age 40 even if none of them had been treated.\nEven though exchangeability does not hold marginally (across everyone), in a conditionally randomized experiment exchangeability does hold within subgroups. If we focus on those in the top 25% of the class, the 90% who are assigned to finish college are a simple random sample of the entire higher-performing subgroup. If we focus on those in the bottom 75% of the class, the 10% who are assigned to finish college are a simple random sample of the entire lower-performing subgroup.\nFormally, conditional exchangeability takes the exchangeability assumption (\\(\\{Y^0,Y^1\\}\\unicode{x2AEB}A\\)) and adds a conditioning bar \\(\\mid\\vec{X}\\), meaning that this assumption holds within subgroups defined by one or more pre-treatment variables \\(\\vec{X}\\).\n\nConditional exchangeability. The assumption that potential outcomes \\(\\{Y^0,Y^1\\}\\) are independent of treatment \\(A\\) among subpopulations that are identical along a set of pre-treatment covariates \\(\\vec{X}\\). Formally, \\(\\{Y^0,Y^1\\} \\unicode{x2AEB} A \\mid \\vec{X}\\).\n\nConditional exchangeability holds by design in conditionally randomized experiments: the probability of treatment assignment differs across subgroups, but within each subgroup we have a simple randomized experiment where each unit has an equal probability of being treated.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Conditional exchangeability"
    ]
  },
  {
    "objectID": "topics/conditional_exchangeability.html#conditional-average-treatment-effects",
    "href": "topics/conditional_exchangeability.html#conditional-average-treatment-effects",
    "title": "Conditional exchangeability",
    "section": "Conditional average treatment effects",
    "text": "Conditional average treatment effects\nIn our conditionally randomized experiment, we could identify conditional average treatment effects: the average effects of college on employment at age 40 (1) among those in the top 25% of their high school class, and the and (2) among those in the bottom 75% of their high school class.\n\nConditional average treatment effect (CATE). The average causal effect within a population subgroup, \\(\\tau(x) = \\text{E}\\left(Y^1\\mid\\vec{X} = \\vec{x}\\right) - \\text{E}\\left(Y^0\\mid \\vec{X} = \\vec{x}\\right)\\).\n\nOnce we assume conditional exchangeability and consistency, CATEs are causally identified by working within a subgroup defined by \\(\\vec{X} = \\vec{x}\\) and taking the difference in means across subgroups of units assigned to treatment and control.\n\\[\n\\begin{aligned}\n&\\text{E}\\left(Y^1\\mid\\vec{X} = \\vec{x}\\right) -  \\text{E}\\left(Y^0\\mid\\vec{X} = \\vec{x}\\right)\\\\\n&= \\text{E}\\left(Y\\mid\\vec{X} = \\vec{x}, A = 1\\right) - \\text{E}\\left(Y\\mid\\vec{X} = \\vec{x}, A = 0\\right)\n\\end{aligned}\n\\]\nIn our concrete example, this means that we could first focus on the subgroup for whom \\(\\vec{X} = (\\text{Top 25\\% of high school class})\\). Within this subgroup, we can compare employment at age 40 among those randomized to a 4-year college degree to employment at age 40 among those randomized to finish education after high school. This mean difference identifies the CATE: the average causal effect of college among those in the top 25% of their high school class.\nLikewise, our experiment would also identify the CATE among those in the bottom 75% of their high school class.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Conditional exchangeability"
    ]
  },
  {
    "objectID": "topics/conditional_exchangeability.html#effect-heterogeneity",
    "href": "topics/conditional_exchangeability.html#effect-heterogeneity",
    "title": "Conditional exchangeability",
    "section": "Effect heterogeneity",
    "text": "Effect heterogeneity\nThere are often good reasons to expect the Conditional Average Treatment Effect (CATE) to differ across subpopulations. In our example, suppose that those from the top 25% of the high school class are very creative and hard-working, and would find ways to be employed at age 40 regardless of whether they finished college. The average causal effect of college on employment in this subgroup might be small. Meanwhile, the average causal effect of college on employment might be quite large among those from the bottom 75% of their high school class. This would be an example of effect heterogeneity,\n\nEffect heterogeneity. Differences in Conditional Average Treatment Effects (CATEs) across subpopulations. \\(\\tau(\\vec{x})\\neq\\tau(\\vec{x}')\\).\n\nAn advantage of analyzing randomized experiments conditionally (within subgroups) is that one can search for effect heterogeneity.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Conditional exchangeability"
    ]
  },
  {
    "objectID": "assignments/pset0.html",
    "href": "assignments/pset0.html",
    "title": "Problem Set 0: Computing environment",
    "section": "",
    "text": "Due: 5pm on Friday, January 10.\nThis problem set is a Quarto document, which embeds code, results, and written work. You will edit it and then create a PDF. If you are new to Quarto, see the Quarto tutorial for additional information.\nTo complete the problem set, download pset0.qmd. Add these four lines of code to the top if they do not appear in your download,\n---\ntitle: \"Problem Set 0: Computing environment\"\nformat: pdf\n---\nand render to a PDF. You will then submit two things:",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#help-with-getting-started",
    "href": "assignments/pset0.html#help-with-getting-started",
    "title": "Problem Set 0: Computing environment",
    "section": "Help with getting started",
    "text": "Help with getting started\nIf you are stuck, make sure that you first follow the installation instructions in setting up your computer.\nOpen the downloaded problem set in RStudio. You should then click the “Render” button as shown in the image below.\n This will run the R code and combine the output with the text in the document into a PDF. By default, this PDF will be output in the same directory that you saved pset0.qmd in. You will then submit the PDF on Canvas.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#issues",
    "href": "assignments/pset0.html#issues",
    "title": "Problem Set 0: Computing environment",
    "section": "Issues",
    "text": "Issues\nIf you run into issues while attempting to render the Problem Set, be sure to open a question on Piazza!",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "who_we_are.html",
    "href": "who_we_are.html",
    "title": "Who We Are",
    "section": "",
    "text": "Get to know a little bit about our teaching team! For office hours, see the Syllabus.\n\n\n\n\n\n\n\n\nIan Lundberg\nianlundberg@ucla.edu\n(he / him)\nWorking with data to understand inequality brings me joy and meaning, as I first discovered as a college student years ago. I hope to share that joy with you! Other joys of mine include hiking, surfing, and oatmeal with blueberries.\n\n\n\n\n\n\n\n\nChristina Wilmot\ncwilmot@ucla.edu (she / they)\nI am interested in studying the many ways that technology and society interact. My background is in computer science and software engineering, so I really enjoy bringing those tools to sociologists. Outside of work I enjoy crafting, gaming, and obsessing over my two adorable cats.\n\n\n\n\n\nNathan Hoffmann\nnathanihoff@g.ucla.edu (he / him)\nI’m a PhD candidate in sociology who uses statistics and machine learning to study international migration, education, and sexuality. I also like cooking, biking, and concerts."
  },
  {
    "objectID": "topics/exchangeability.html",
    "href": "topics/exchangeability.html",
    "title": "Exchangeability",
    "section": "",
    "text": "Causal effects involve both factual and counterfactual outcomes, yet data that we can observe involve only factual outcomes. To learn about causal effects from data that can be observed requires assumptions about the data that are not observed. This page introduces exchangeability, which is an assumption that can identify causal effects.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/exchangeability.html#exchangeability-in-simple-random-samples",
    "href": "topics/exchangeability.html#exchangeability-in-simple-random-samples",
    "title": "Exchangeability",
    "section": "Exchangeability in simple random samples",
    "text": "Exchangeability in simple random samples\nThe figure below illustrates a population of 6 people. Each person has an outcome \\(Y_i\\), which for example might be that person’s employment at age 40. A researcher draws a random sample without replacement with equal sampling probabilities and records the sampled outcomes. The researcher uses the average of the sampled outcomes as an estimator for the population mean.\n\nWhy do probability samples like this work? They work because selection into the sample (\\(S = 1\\)) is completely randomized and thus independent of the outcome \\(Y\\). In other words, the people who are sampled (\\(S = 1\\)) and the people who are unsampled (\\(S = 0\\)) have the same distribution of outcomes (at least in expectation over samples). We might say that the sampled and the unsampled units are exchangeable in the sense that they follow the same distribution in terms of \\(Y\\). In math, exchangeable sampling can be written as follows.\n\\[\n\\underbrace{Y}_\\text{Outcome}\\quad \\underbrace{\\mathrel{\\unicode{x2AEB}}}_{\\substack{\\text{Is}\\\\\\text{Independent}\\\\\\text{of}}} \\quad \\underbrace{S}_{\\substack{\\text{Sample}\\\\\\text{Inclusion}}}\n\\]\nExchangeability holds in simple random samples because sampling is completely independent of all outcomes by design. In other types of sampling, such as convenience samples that enroll anyone who is interested, exchangeability may hold but is far from guaranteed. Perhaps people who are employed are more likely to answer a survey about employment, so that the employment rate in a convenience sample might far exceed the population mean employment rate. Exchangeability is one condition under which reliable population estimates can be made from samples, and probability samples are good because they make exchangeability hold by design.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/exchangeability.html#exchangeability-in-randomized-experiments",
    "href": "topics/exchangeability.html#exchangeability-in-randomized-experiments",
    "title": "Exchangeability",
    "section": "Exchangeability in randomized experiments",
    "text": "Exchangeability in randomized experiments\nThe figure below illustrates our population if they all enrolled in a hypothetical randomized experiment. In this experiment, we imagine that each unit is either randomized to attain a four-year college degree (\\(A = 1)\\) or to finish education with a high school diploma (\\(A = 0\\)).\n\nIn this randomization, Maria, Sarah, and Jes'us were randomized to attain a four-year college degree. We observe their outcomes under this treatment condition (\\(Y^1\\)). Because treatment was randomized with equal probabilities, these three units form a simple random sample from the full population of 6 people. We could use the sample mean of \\(Y^1\\) among the treated units (Maria, Sarah, Jes'us) as an estimator of the population mean of \\(Y^1\\) among all 6 units.\nWilliam, Rich, and Alondra were randomized to finish their education with a high school diploma. We see their outcomes under this control condition \\(Y^0\\). Their treatment assignment (\\(A = 0\\)) is analogous to being sampled from the population of \\(Y^0\\) values. We can use their sample mean outcome as an estimator of the population mean of \\(Y^0\\).\nFormally, we can write the exchangeability assumption for treatment assignments as requiring that the set of potential outcomes are independent of treatment assignment.\n\\[\n\\underbrace{\\{Y^1,Y^0\\}}_{\\substack{\\text{Potential}\\\\\\text{Outcomes}}}\\quad\\underbrace{\\mathrel{\\unicode{x2AEB}}}_{\\substack{\\text{Are}\\\\\\text{Independent}\\\\\\text{of}}}\\quad  \\underbrace{A}_\\text{Treatment}\n\\] Exchangeability holds in randomized experiments because treatment is completely independent of all potential outcomes by design. In observational studies, where treatment values are observed but are not assigned randomly by the researcher, exchangeability may hold but is far from guaranteed. In the coming classes, we will talk about generalizations of the exchangeability assumption that one can argue might hold in some observational settings.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/exchangeability.html#causal-identification",
    "href": "topics/exchangeability.html#causal-identification",
    "title": "Exchangeability",
    "section": "Causal identification",
    "text": "Causal identification\nA population-average causal effect could take many possible values. Using data alone, it is impossible to identify which of these many possible values is the correct one. By pairing data together with causal assumptions, however, one can identify the average causal effect by equating it with a statistical quantity that only involves observable random variables.\n\nCausal identification. A mathematical proof linking a causal estimand (involving potential outcomes) to a statistical quantity involving only factual random variables.\n\nIn a randomized experiment, the average causal effect is identified by the assumptions of consistency and exchangeability. A short proof can yield insight about the goals and how these assumptions are used.\n\\[\n\\begin{aligned}\n&\\overbrace{\\text{E}\\left(Y^1\\right) - \\text{E}\\left(Y^0\\right)}^{\\substack{\\text{Average}\\\\\\text{causal effect}\\\\\\text{(among everyone)}}} \\\\\n&= \\text{E}\\left(Y^1\\mid A = 1\\right) - \\text{E}\\left(Y^0\\mid A = 0\\right) &\\text{by exchangeability}\\\\\n&= \\underbrace{\\text{E}\\left(Y\\mid A = 1\\right)}_{\\substack{\\text{Mean outcome}\\\\\\text{among the treated}}} - \\underbrace{\\text{E}\\left(Y\\mid A = 0\\right)}_{\\substack{\\text{Mean outcome}\\\\\\text{among the untreated}}} &\\text{by consistency}\n\\end{aligned}\n\\]\nThe proof begins with the average causal effect and equates it to a statistical estimand: the mean outcome among the treated minus the mean outcome among the untreated. The first quantity involves potential outcomes (with superscripts), whereas the last quantity involves only factual random variables.\nThe exchangeability assumption allows us to move from the first line to the second line. Under exchangeability, the mean outcome that would be realized under treatment (\\(\\text{E}(Y^1)\\)) equals the mean outcome under treatment among those who were actually treated (\\(\\text{E}(Y^0)\\)). Likewise for outcomes under no treatment. This line is true because the treated (\\(A = 1\\)) and the untreated (\\(A = 0\\)) are both simple random samples from the full population.\nThe consistency assumption allows us to move from the second line to the third. Among the treated, (\\(A = 1\\)), the outcome that is realized is \\(Y = Y^1\\). Among the untreated (\\(A = 0\\)), the outcome that is realized is \\(Y = Y^0\\). Under the assumption that factual outcomes are consistent with the potential outcomes under the assigned treatment, the second line equal the third.\nSomething nice about a causal identification proof is that there is no room for error: it is mathematically true that the premise and the assumptions together yield the result. As long as the assumptions hold, the statistical estimand equals the causal estimand. Causal inference thus boils down to research designs and arguments that can lend credibility to the assumptions that let us draw causal claims from data that are observed.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Exchangeability"
    ]
  },
  {
    "objectID": "topics/asking_questions.html",
    "href": "topics/asking_questions.html",
    "title": "Asking questions with data",
    "section": "",
    "text": "slides\nAsking your own research question is fun, and it is also daunting. There is no formula for asking a good question, and much research involves trial and error. In this lecture, we will discuss together what makes a good research question.",
    "crumbs": [
      " ",
      "Working with Data",
      "Asking questions with data"
    ]
  },
  {
    "objectID": "topics/asking_questions.html#some-characteristics-of-a-good-question",
    "href": "topics/asking_questions.html#some-characteristics-of-a-good-question",
    "title": "Asking questions with data",
    "section": "Some characteristics of a good question",
    "text": "Some characteristics of a good question\nWhile good questions come in many forms, good research questions often has a few characteristics.\n\nA unit of analysis\nThe unit of analysis is the bedrock of a good research question. What will a row of your dataset represent? While the answer may seem straightforward, many units of analysis are possible: a person, a school, a neighborhood, a country, etc. In settings with repeated observations, the unit of analysis might be a person observed at a particular age: Jose at age 29 may be one row of your dataset and Jose at age 30 may be another row. The unit of analysis is the unit at which the outcome is defined, so getting it right is important.\n\n\nA clear population\nA good research question addresses a population that is relevant to the author’s argument. A well-defined population could be a set of units, such as American residents in a certain age range. It could be the set of all students at Cornell. It could also be a set of aggregate entities, such as the population of all four-year colleges and universities in the U.S., where each unit in the population is a college or university.\nWhen the population is well-defined, a reader should be able to imagine making a list of all the units in that population. When presented with a new unit and asked whether that unit is part of the population, you should be able to confidently answer “yes” or “no” and not “maybe.”\nIt is rarely satisfying to answer the question “who is being studied?” with the answer “the people in my sample.” While full-count enumerations of interesting populations exist, usually your sample is a subset of a broader population of interest. Good research tells us who that population is.\n\n\nA precise outcome\nFor each unit in the population, there is an outcome. This might be the income of an individual person, or the graduation rate of an individual college. A good research question tells you what the outcome is, and why it matters.\n\n\nThe potential for surprising results\nThis may be the hardest to pin down. Before you work with data, try to tell a story about why the results might go one way or another way. Perhaps you study a particular population of employed people where there is one set of reasons to expect men to earn more and another set of reasons to expect women to earn more. Try to convince yourself that the results could go either way! This builds excitement about the empirical answer, because it shows that the answer is really unknown before data analysis. Write down your arguments because they will also help readers be excited about your questions.\nQuestions where only one answer is plausible are often tedious. Questions that are exciting are often the ones where results could surprise you by going several possible directions.",
    "crumbs": [
      " ",
      "Working with Data",
      "Asking questions with data"
    ]
  },
  {
    "objectID": "slides/m_discussion_poststratification_dag.html",
    "href": "slides/m_discussion_poststratification_dag.html",
    "title": "m_discussion_poststratification_dag",
    "section": "",
    "text": "DAGs are not just useful for causal inference: they can be useful whenever we need to know whether one variable is statistically independent of another. This is true, for example, when drawing inference about a population from a sample.\nA researcher uses an opt-in online web survey to draw inference about support for President Biden. They ask respondents: ``Do you approve of President Biden’s performance in office?’’ with the answer choices Yes/No. The researcher also gathers data on two demographic characteristics: whether the respondent completed college and current employment. They write:\n\nMy sample is not representative. Suppose for every person in the population, \\(S\\) denotes whether they are included in my sample. Then \\(S\\) is related to their approval of President Biden (\\(Y\\)).\nHowever, I believe my sample is representative when I look at a set of people who all take the same value along college completion and employment, such as those who finished college and are currently employed. If these variables are \\(X_1,X_2\\), I believe this independence statement: \\(S\\) is independent of \\(Y\\) given \\(X_1,X_2\\). I will therefore get population estimates by a procedure with several steps: use my sample to estimate the mean outcome \\(E(Y\\mid \\vec{X} = \\vec{x})\\) in each stratum, then use Census data to estimate the size of each stratum \\(P(\\vec{X} = \\vec{x})\\) in the population, then estimate \\(E(Y) = \\sum_{\\vec{x}}E(Y\\mid \\vec{X} = \\vec{x})P(\\vec{X} = \\vec{x})\\).\n\nThis researcher’s reasoning is a common strategy known as post-stratification. This question is about formalizing a set of conditions under which the researcher is right and wrong.\nBefore you begin, we want to emphasize one aspect of the researcher’s assumption that is different from the exchangeability assumption for causal inference.\n\nfor causal claims, we assume conditional exchangeability: \\(A\\) independent of \\(Y^a\\) given \\(\\vec{X}\\)\n\ninvolves the potential outcome \\(Y^a\\)\nholds if the only unblocked paths between \\(A\\) and \\(Y\\) are causal paths\n\nfor sample-to-population inference, we assume conditionally independent sampling \\(S\\) independent of \\(Y\\) given \\(\\vec{X}\\)\n\ninvolves the factual outcome \\(Y\\); there is no intervention here\nholds if there are no unblocked paths between \\(S\\) and \\(Y\\)\n\n\nAlthough the assumption is different, the principles of DAGs are still relevant.\n\n\nDraw a DAG under which the researcher’s claim is valid. Use \\(S,Y,X_1,X_2\\).\n\n\n\nIn a sentence or two, explain your DAG from 3.1 to the researcher. Tell us in words what is meant by each edge in your DAG.\n\n\n\nDraw a DAG showing a counterexample under which the researcher’s claim is invalid.\n\n\n\nIn a sentence or two, explain your DAG from 3.3 to the researcher. Tell us particularly about the path that creates a statistical dependence between \\(S\\) and \\(Y\\)."
  },
  {
    "objectID": "slides/m_discussion_poststratification_dag.html#using-dags-in-a-new-context",
    "href": "slides/m_discussion_poststratification_dag.html#using-dags-in-a-new-context",
    "title": "m_discussion_poststratification_dag",
    "section": "",
    "text": "DAGs are not just useful for causal inference: they can be useful whenever we need to know whether one variable is statistically independent of another. This is true, for example, when drawing inference about a population from a sample.\nA researcher uses an opt-in online web survey to draw inference about support for President Biden. They ask respondents: ``Do you approve of President Biden’s performance in office?’’ with the answer choices Yes/No. The researcher also gathers data on two demographic characteristics: whether the respondent completed college and current employment. They write:\n\nMy sample is not representative. Suppose for every person in the population, \\(S\\) denotes whether they are included in my sample. Then \\(S\\) is related to their approval of President Biden (\\(Y\\)).\nHowever, I believe my sample is representative when I look at a set of people who all take the same value along college completion and employment, such as those who finished college and are currently employed. If these variables are \\(X_1,X_2\\), I believe this independence statement: \\(S\\) is independent of \\(Y\\) given \\(X_1,X_2\\). I will therefore get population estimates by a procedure with several steps: use my sample to estimate the mean outcome \\(E(Y\\mid \\vec{X} = \\vec{x})\\) in each stratum, then use Census data to estimate the size of each stratum \\(P(\\vec{X} = \\vec{x})\\) in the population, then estimate \\(E(Y) = \\sum_{\\vec{x}}E(Y\\mid \\vec{X} = \\vec{x})P(\\vec{X} = \\vec{x})\\).\n\nThis researcher’s reasoning is a common strategy known as post-stratification. This question is about formalizing a set of conditions under which the researcher is right and wrong.\nBefore you begin, we want to emphasize one aspect of the researcher’s assumption that is different from the exchangeability assumption for causal inference.\n\nfor causal claims, we assume conditional exchangeability: \\(A\\) independent of \\(Y^a\\) given \\(\\vec{X}\\)\n\ninvolves the potential outcome \\(Y^a\\)\nholds if the only unblocked paths between \\(A\\) and \\(Y\\) are causal paths\n\nfor sample-to-population inference, we assume conditionally independent sampling \\(S\\) independent of \\(Y\\) given \\(\\vec{X}\\)\n\ninvolves the factual outcome \\(Y\\); there is no intervention here\nholds if there are no unblocked paths between \\(S\\) and \\(Y\\)\n\n\nAlthough the assumption is different, the principles of DAGs are still relevant.\n\n\nDraw a DAG under which the researcher’s claim is valid. Use \\(S,Y,X_1,X_2\\).\n\n\n\nIn a sentence or two, explain your DAG from 3.1 to the researcher. Tell us in words what is meant by each edge in your DAG.\n\n\n\nDraw a DAG showing a counterexample under which the researcher’s claim is invalid.\n\n\n\nIn a sentence or two, explain your DAG from 3.3 to the researcher. Tell us particularly about the path that creates a statistical dependence between \\(S\\) and \\(Y\\)."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The course is divided into three broad sections:\n\nJan 7–16: Working with data\nJan 21–Feb 6: Inference without models\nFeb 11 onward: Inference with models\n\nEach date will be a link to the slides for that lecture.\n\n\n\nDate\nWebsite Page\n\n\n\n\n\nJan 7\nWelcome\n\n\n\n\nAsking questions\n\n\n\n\nSetting up your computer\n\n\n\nJan 9\nVisualization\n\n\n\nJan 14 & Jan 16\nData transformation\n\n\n\nThu, Jan 16\nData transformation\n\n\n\nTue, Jan 21\nPopulation sampling\n\n\n\nThu, Jan 23\nPopulation sampling\n\n\n\nTue, Jan 28\nDefining causal effects\n\n\n\nThu, Jan 30\nExchangeability and experiments\n\n\n\nTue, Feb 4\nConditional exchangeability\n\n\n\n\nNonparametric estimation\n\n\n\nThu, Feb 6\nDirected Acyclic Graphs (DAGs)\n\n\n\nTue, Feb 11 A and B\nWhy model? and What is a model?\n\n\n\nThu, Feb 13\nIntro of outcome models for causal inference\n\n\n\nTue, Feb 18\nWith coding: Outcome models for causal inference\n\n\n\n\nModel-based inverse probability weighting\n\n\n\nThu, Feb 20\nDoubly-robust estimation\n\n\n\nTue, Feb 25\nMatching\n\n\n\nThu, Feb 27\nTrees\n\n\n\nTeu, Mar 4\nForests\n\n\n\nThu, Mar 6\nSample splitting\n\n\n\nTue, Mar 11\nMachine learning vs regression: A case study (FF Challenge\n\n\n\nThur, Mar 13\nCourse recap"
  },
  {
    "objectID": "schedule.html#lecture-topics",
    "href": "schedule.html#lecture-topics",
    "title": "Schedule",
    "section": "",
    "text": "The course is divided into three broad sections:\n\nJan 7–16: Working with data\nJan 21–Feb 6: Inference without models\nFeb 11 onward: Inference with models\n\nEach date will be a link to the slides for that lecture.\n\n\n\nDate\nWebsite Page\n\n\n\n\n\nJan 7\nWelcome\n\n\n\n\nAsking questions\n\n\n\n\nSetting up your computer\n\n\n\nJan 9\nVisualization\n\n\n\nJan 14 & Jan 16\nData transformation\n\n\n\nThu, Jan 16\nData transformation\n\n\n\nTue, Jan 21\nPopulation sampling\n\n\n\nThu, Jan 23\nPopulation sampling\n\n\n\nTue, Jan 28\nDefining causal effects\n\n\n\nThu, Jan 30\nExchangeability and experiments\n\n\n\nTue, Feb 4\nConditional exchangeability\n\n\n\n\nNonparametric estimation\n\n\n\nThu, Feb 6\nDirected Acyclic Graphs (DAGs)\n\n\n\nTue, Feb 11 A and B\nWhy model? and What is a model?\n\n\n\nThu, Feb 13\nIntro of outcome models for causal inference\n\n\n\nTue, Feb 18\nWith coding: Outcome models for causal inference\n\n\n\n\nModel-based inverse probability weighting\n\n\n\nThu, Feb 20\nDoubly-robust estimation\n\n\n\nTue, Feb 25\nMatching\n\n\n\nThu, Feb 27\nTrees\n\n\n\nTeu, Mar 4\nForests\n\n\n\nThu, Mar 6\nSample splitting\n\n\n\nTue, Mar 11\nMachine learning vs regression: A case study (FF Challenge\n\n\n\nThur, Mar 13\nCourse recap"
  },
  {
    "objectID": "schedule.html#assignments",
    "href": "schedule.html#assignments",
    "title": "Schedule",
    "section": "Assignments",
    "text": "Assignments\n\n\n\nDue Date\nAssignment\n\n\n\n\nFri, Jan 10 at 5pm\nProblem Set 0\n\n\nFri, Jan 17 at 5pm\nProblem Set 1\n\n\nFri, Jan 24 at 5pm\nPeer Review 1\n\n\nFri, Jan 31 at 5pm\nProblem Set 2\n\n\nFri, Feb 07 at 5pm\nPeer Review 2\n\n\nFri, Feb 14 at 5pm\nProblem Set 3\n\n\nFri, Feb 21 at 5pm\nPeer Review 3\n\n\nFri, Feb 28 at 5pm\nProblem Set 4\n\n\nFri, Mar 07 at 5pm\nProject\n\n\nFri, Mar 14 at 5pm\nProject Peer Review"
  },
  {
    "objectID": "forms.html",
    "href": "forms.html",
    "title": "Forms",
    "section": "",
    "text": "Assignment extension form. If you are experiencing an exceptional circumstance that may warrant an extension, use this to request one.\nRegrade request form. If you think we have graded something wrong, you can report it on this form. Responses are only accepted for within 72 hours of grade post.\nExcused absence form. To request an excused absence from lecture or discussion.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Final Project",
    "section": "",
    "text": "The culmination of the course is a group research project. You will\nYou can answer any question about inequality (broadly defined) using the ideas we learned in this course.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#what-you-will-submit",
    "href": "assignments/project.html#what-you-will-submit",
    "title": "Final Project",
    "section": "What you will submit",
    "text": "What you will submit\nThere are two submitted items, both due 5pm on Friday, Mar 7.\nWriteup. A .qmd document compiled into a PDF. It should include all code that produces your results. This must be no more than 1,000 words and will contain 1 or more visualizations.\nSlides. A PDF that you will present in discussion. Keep text to a minimum in the slides. You should plan to have all group members speak during the presentation. Your presentation should be 10 minutes or less.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#group-structure",
    "href": "assignments/project.html#group-structure",
    "title": "Final Project",
    "section": "Group structure",
    "text": "Group structure\nWe will form groups of about 5 students each within discussion sections. Groups will be formed during discussion in the middle of the quarter.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#key-components-of-the-project",
    "href": "assignments/project.html#key-components-of-the-project",
    "title": "Final Project",
    "section": "Key components of the project",
    "text": "Key components of the project\nYour goal should be to tell us a story using the data. What do we learn by studying this outcome, aggregated this way, in these subgroups from this population?\nA few points should be part of your writeup:\n\nDefine your unit of analysis\nDefine your target population\n\nMotivate: why is this population interesting to study?\n\nDescribe how your sample was chosen from that population\n\nthis may be a probability sample, such as those available via IPUMS. If so, tell us a little bit about the sampling design\nthis may be a convenience sample. If so, why does it speak to the population and what are the limitations?\nthis may be data on the entire population, as in our baseball example\n\nChoose an outcome variable, which is defined for each unit in the population. Your outcome variable can be a factual variable (\\(Y\\)) for descriptive claims, or a potential outcome (\\(Y^a\\)) or difference in potential outcomes (\\(Y^1-Y^0\\)) for causal claims.\n\nexample: annual wage and salary income\n\nChoose one or more variables on which to create population subgroups\n\nexample: subgroups defined by sex (male and female)\n\nChoose a summary statistic, which aggregates the outcome distribution to one summary per subgroup\n\nexamples: proportion, mean, median, 90th percentile\n\nWrite with clarity on causal assumptions\n\nif your goal is causal with potential outcomes, draw a DAG in which those potential outcomes can be identified by an assumption of conditional exchangeability. If there are unmeasured confounders, discuss how their omission from your DAG may threaten the credibility of your estimates. It is ok if the assumptions are doubtful as long as you write them down clearly.\nif your goal is descriptive with only factual outcomes, then write your results carefully using only descriptive language. As a heuristic, if your goal is descriptive then you should not be using the sentence structure “X [verb] Y”, such as “going to college increases earnings.” This claim suggests a college graduate would have earned less if they had not gone to college—a counterfactual outcome we did not see. For truly descriptive claims, you should phrase more like: “There is a difference in earnings among those who did and did not go to college.” A heuristic to recognize a non-causal claim is that it can be phrased it in an “among” statement: “Among subgroup A, we find ___. Among subgroup B, we find ___.” Or “There is a disparity in Y across subgroups defined by X.”\n\nVisualize your findings in a ggplot",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#considerations-to-bear-in-mind",
    "href": "assignments/project.html#considerations-to-bear-in-mind",
    "title": "Final Project",
    "section": "Considerations to bear in mind",
    "text": "Considerations to bear in mind\n\nWeights. If your sample is drawn from the population with unequal probabilities, you should use sampling weights\nModels. If your question involves many subgroups (e.g., ages) with few observations in each subgroup, you can (but are not required to) use a statistical model to estimate your summary statistic in the subgroup by a predicted value. For example, you could use OLS to predict the proportion mean income at each age. If you do this, you should report the predicted value of the summary statistic, not the coefficients of the model.\nAggregation. Your data must begin with units (e.g., people) who you aggregate into subgroups (e.g., age groups). Your data might come pre-aggregated, such as data where each row contains data for all students in a particular college or university. Then you would need to aggregate further, such as to produce summaries for private versus public universities.\nDropped cases. As you move from raw data to the data that produce your graph, you might drop cases on the way. For example, some cases may have missing values on key predictors. Report how many are dropped, and why. Our goal here is transparent, open science.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#have-fun",
    "href": "assignments/project.html#have-fun",
    "title": "Final Project",
    "section": "Have fun",
    "text": "Have fun\nAs a teaching team, the project is our favorite part of the course. Preparing you to succeed in the project has been (in some sense) the entire goal of all that precedes the project in the course. We hope you will find joy in answering questions with data, as we do.",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/pset4.html",
    "href": "assignments/pset4.html",
    "title": "Problem Set 4: Statistical Learning",
    "section": "",
    "text": "Due: 5pm on Friday, Feb 28.\nThis problem set has not yet been posted.\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office Hours",
    "section": "",
    "text": "Person\nTime\nLocation\n\n\n\n\nNathan\nT 1:30–2:30pm\nHaines A55\n\n\nIan\nTh 1–2pm\nHaines 241C\n\n\nChristina\nTh 2–3pm\nHaines A58-C\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "slides/m_discussion_sailing.html",
    "href": "slides/m_discussion_sailing.html",
    "title": "Untitled",
    "section": "",
    "text": "You are looking into a sailing class through UCLA Recreation! For each claim below, tell us whether the claim is causal or descriptive.\n2.1 (5 points) \nLast year, there was a survey of students who did and did not take the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher among those who took the class.\nAnswer. Your answer here\n2.2 (5 points) \nLast year, there was a survey of students before and after the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher in the survey taken after the class.\nAnswer. Your answer here\n2.3 (5 points) \nOn average, the students in this class emerged more prepared to sail than they would have been without the class.\nAnswer. Your answer here"
  },
  {
    "objectID": "slides/m_discussion_sailing.html#a-sailing-class",
    "href": "slides/m_discussion_sailing.html#a-sailing-class",
    "title": "Untitled",
    "section": "",
    "text": "You are looking into a sailing class through UCLA Recreation! For each claim below, tell us whether the claim is causal or descriptive.\n2.1 (5 points) \nLast year, there was a survey of students who did and did not take the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher among those who took the class.\nAnswer. Your answer here\n2.2 (5 points) \nLast year, there was a survey of students before and after the class. The proportion reporting that they felt prepared to sail in Marina del Rey was higher in the survey taken after the class.\nAnswer. Your answer here\n2.3 (5 points) \nOn average, the students in this class emerged more prepared to sail than they would have been without the class.\nAnswer. Your answer here"
  },
  {
    "objectID": "topics/define_causal.html",
    "href": "topics/define_causal.html",
    "title": "Defining causal effects",
    "section": "",
    "text": "slides\nThe course so far has focused on descriptive claims. For example, we have estimated the average value of an outcome in the population, or among population subgroups. This lecture pivots to causal claims: what would happen if a population were exposed to a hypothetical intervention.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Defining causal effects"
    ]
  },
  {
    "objectID": "topics/define_causal.html#fundamental-problem-of-causal-inference",
    "href": "topics/define_causal.html#fundamental-problem-of-causal-inference",
    "title": "Defining causal effects",
    "section": "Fundamental problem of causal inference",
    "text": "Fundamental problem of causal inference\nHealth professionals often advise people to eat a Mediterranean diet high in healthy fats such as olive oil, whole grains, and fruits. There is descriptive evidence that lifespans are longer among people who eat a Mediterranean diet compared with among people who eat a standard diet. But does eating a Mediterranean diet cause longer lifespan? The figure below visualizes this question in the potential outcomes framework.\n\nIn this hypothetical example, each row corresponds to a person. Person 1 follows a Mediterranean diet and is observed to have a lifespan indicated in blue. Person 2 does not follow a Mediterranean diet and is observed to have a lifespan indicated in green. The descriptive evidence is that lifespans are longer among those eating a Mediterranean diet (blue outcomes on the left) compared with those eating standard diets (green outcomes on the left).\nThe right side of the figure corresponds to the causal claim, which is different. Person 1 has two potential outcomes: a lifespan that would be realized under a Mediterranean diet and a lifespan that would be realized under a standard diet. The causal effect for Person 1 is the difference between the lifespans that would be realized for that person under each of the two diets. But there is a fundamental problem: person 1 ate a Mediterranean diet, and we did not get to observe their outcome under a standard diet. The fundamental problem of causal inference (Holland 1986) is that causal claims involve a contrast between potential outcomes, but for each unit only one of these potential outcomes is realized. The other is counterfactual and cannot be directly observed.\nWe will need additional argument and assumptions to use the factual data (left side of the figure) in order to produce answers about causal effects (right side of the figure). Causal inference is a missing data problem insofar as many of the potential outcomes we need are missing.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Defining causal effects"
    ]
  },
  {
    "objectID": "topics/define_causal.html#mathematical-notation",
    "href": "topics/define_causal.html#mathematical-notation",
    "title": "Defining causal effects",
    "section": "Mathematical notation",
    "text": "Mathematical notation\nBecause each person has more than one potential outcome, we need new mathematical notation to formalize causal claims. We will use subscripts to indicate units (rows of our data). Let \\(Y_i\\) be the outcome for person \\(i\\), such as whether person \\(i\\) survived. Let \\(A_i\\) be the treatment of person \\(i\\), for example taking the value or the value . To refer more abstractly to a value the treatment could take, we use the lower case notation \\(a\\) for a treatment value. Define potential outcomes \\(Y_i^\\text{MediterraneanDiet}\\) and \\(Y_i^\\text{StandardDiet}\\) as the lifespan outcomes that person \\(i\\) would realize under each of the treatment conditions. More generally, let \\(Y_i^a\\) denote the potential outcome for unit \\(i\\) that would be realized if assigned to treatment value \\(a\\).\nThe causal effect is a contrast across potential outcomes. For example, the causal effect on Ian’s lifespan of eating a Mediterranean diet versus a standard diet is \\[Y_\\text{Ian}^\\text{MediterraneanDiet} - Y_\\text{Ian}^\\text{StandardDiet}\\]\nTo connect causal claims to ideas we have already covered from sampling, we will adopt a framework in which potential outcomes are fixed quantities with randomness arising from sampling and/or from random treatment assignment. Each person has a fixed outcome \\(Y_i^\\text{MediterraneanDiet}\\) that would be observed if they were sampled and assigned a Mediterranean diet. This is just like how every baseball player from last week had a salary that would be observed if they were sampled. We will sometimes omit the \\(i\\) subscript to refer to the random variable for the potential outcome of a randomly-sampled person from the population, \\(Y^\\text{MediterraneanDiet}\\).",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Defining causal effects"
    ]
  },
  {
    "objectID": "topics/define_causal.html#the-consistency-assumption",
    "href": "topics/define_causal.html#the-consistency-assumption",
    "title": "Defining causal effects",
    "section": "The consistency assumption",
    "text": "The consistency assumption\nWe want to make causal claims about potential outcomes \\(Y_i^a\\), but what we observe are factually realized outcome \\(Y_i\\). To draw the connection, we need to assume that the factual outcomes are consistent with what would be observed if the person in question were assigned to the treatment condition that factually observed. Using \\(A_i\\) to denote the factual treatment for person \\(i\\), we assume\n\\[\nY_i^{A_i} = Y_i \\qquad\\text{(consistency assumption)}\n\\] This assumption is often obviously true, but we will see later examples where it is violated. One example is when person \\(i\\)’s outcome depends not only on their own treatment but also on the treatment of some neighboring person \\(j\\). In many of our initial examples, we will simply assume the consistency assumption holds.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Defining causal effects"
    ]
  },
  {
    "objectID": "topics/define_causal.html#potential-outcomes-in-math-and-in-words",
    "href": "topics/define_causal.html#potential-outcomes-in-math-and-in-words",
    "title": "Defining causal effects",
    "section": "Potential outcomes in math and in words",
    "text": "Potential outcomes in math and in words\nWe will often use potential outcomes within mathematical statements. For example, we might write about the expected outcome if assigned to a Mediterranean diet, \\(E(Y^\\text{MediterraneanDiet})\\). Recall that the expectation operator \\(E()\\) says to take the population mean of the random variable within the parentheses. We will also use conditional expectations, such as \\(E(Y\\mid A = \\text{MediterraneanDiet})\\) which reads “the expected value of \\(Y\\) given that \\(A\\) took the value .” The vertical bar says that we are taking the expected value of the variable on the left of the bar within the subgroup defined on the right side of the bar.\nIn class, we practiced writing statements in math and in English. For example, the mathematical statement \\[\nE(\\text{Earning} \\mid \\text{CollegeDegree} = \\texttt{TRUE}) &gt; E(\\text{Earning} \\mid \\text{CollegeDegree} = \\texttt{FALSE})\n\\] is a descriptive statement that says the expected value of earnings is higher among the subgroup with college degrees than among those without college degrees. We made these kinds of descriptive claims already in code by using group_by() and summarize().\nThe mathematical statement \\[\nE\\left(\\text{Earning}^{\\text{CollegeDegree} = \\texttt{TRUE}}\\right) &gt; E\\left(\\text{Earning}^{\\text{CollegeDegree} = \\texttt{FALSE}}\\right)\n\\] is a causal claim that the expected value of earnings that a random person would realize if assigned to a college degree is higher than the expected value if a random person were assigned to no college degree. Because there is no vertical bar (\\(\\mid\\)), the causal claim is an average over the entire population on both sides of the inequality. The descriptive claim, by contrast, is an average over two different sets of units. The figure below visualizes the difference between these descriptive and causal claims using a visual analogous to the one used to introduce a Mediterranean diet.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Defining causal effects"
    ]
  },
  {
    "objectID": "topics/define_causal.html#closing-thoughts",
    "href": "topics/define_causal.html#closing-thoughts",
    "title": "Defining causal effects",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nThe fundamental problem of causal inference is a deep challenge; causal claims will always involve outcomes that are missing and which can only be learned through assumptions and argument. The next class will introduce randomized experiments and show how they provide one setting in which the assumptions required for causal inference are especially tenable.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Defining causal effects"
    ]
  },
  {
    "objectID": "topics/define_causal.html#what-to-read",
    "href": "topics/define_causal.html#what-to-read",
    "title": "Defining causal effects",
    "section": "What to read",
    "text": "What to read\nNo reading is required after this class, but if you would like to learn more about causal inference, there are a few places to look. We will repeatedly use college completion as an example of a causal treatment, drawing on examples from the following book.\n\nBrand, Jennie E. 2023. Overcoming the Odds: The Benefits of Completing College for Unlikely Graduates. Russell Sage Foundation. Here is a link to read online through the UCLA Library.\n\nFor a mathematical introduction with examples from epidemiology, see Chapter 1 of this book.\n\nHern'an, Miguel A. and James M. Robins. Causal Inference: What If?. Boca Raton: Chapman & Hall / CRC.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Defining causal effects"
    ]
  },
  {
    "objectID": "topics/working_with_data.html",
    "href": "topics/working_with_data.html",
    "title": "Working with data",
    "section": "",
    "text": "This topic is covered on Jan 7.\n\nData science is a world of abundant opportunities. Over recent decades, the field has been transformed by the rapid growth of both computational power and available data. With the proliferation of new technological advances and data opportunities, a social scientist might reasonably ask: where do I begin?\nA good place to begin is to first\n\nChoose a question data can answer\nWork with data to answer the question\n\nWe will first talk about how to ask a good question that data can answer. Then, we will begin preparing our computers with software and tools to answer those questions. Over the quarter, we will develop the skills to ask our own questions and find data to answer them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Working with Data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Science",
    "section": "",
    "text": "Together, we will use tools from data science to answer social science questions. As an area of application, we will focus on questions about inequality and social stratification.",
    "crumbs": [
      " ",
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Social Data Science",
    "section": "Learning goals",
    "text": "Learning goals\nAs a result of participating in this course, students will be able to\n\nvisualize economic inequality with graphs that summarize survey data\nconnect theories about inequality to quantitative empirical evidence\nevaluate the effects of hypothetical interventions to reduce inequality\nconduct data analysis using the R programming language",
    "crumbs": [
      " ",
      "Home"
    ]
  },
  {
    "objectID": "assignments/pset2.html",
    "href": "assignments/pset2.html",
    "title": "Problem Set 2: Data Transformation",
    "section": "",
    "text": "Due: 5pm on Friday, Jan 31.\nStudent identifer: [type your anonymous identifier here]\nThis problem set draws on the following paper.\nA note about sex and gender. Sex typically refers to categories assigned at birth (e.g., female, male). Gender is a performed construct with many possible values: man, woman, nonbinary, etc. The measure in the CPS-ASEC is “sex,” coded male or female. We will use these data to study sex disparities between those identifying as male and female. The paper at times uses “gender” to refer to this construct.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "assignments/pset2.html#data-analysis-existing-question",
    "href": "assignments/pset2.html#data-analysis-existing-question",
    "title": "Problem Set 2: Data Transformation",
    "section": "1. Data analysis: Existing question",
    "text": "1. Data analysis: Existing question\n25 points. Reproduce Figure 1 from the paper.\nVisit cps.ipums.org to download data. Include these variables in your cart: sex, age, asecwt, empstat. One way to find them is to click “Select Data” and then use the “Search” button under “Select Variables.”\n\n\n\n\n\n\nTip\n\n\n\nLook ahead: you will later study a new outcome of your own choosing. You could add it to your cart now if you want.\n\n\nOnce you select your variables, click “Select Samples.” We will download data from the 1962–2023 March Annual Social and Economic Supplement. When choosing samples, you want all samples in the “ASEC” tab and none of the samples in the “Basic Monthly” tab. To achieve this, under the ASEC tab you can click “Select All Samples.” Under the Basic Monthly tab, you can click and then unclick “Select All Samples” to remove these samples that we will not use.\nOnce you have selected variables and selected samples, you can click “View Cart.” Optionally, reduce the size of your extract by un-checking any variables that have been automatically added which you won’t be using. Note that one automatically-added variable is year, which you should keep because you will be using it. Then click “Create Data Extract.” On the next page, select cases to those ages 25–54. Before submitting your extract, we recommend changing the data format to “Stata (.dta)” so that you get value labels.\nOn your computer, analyze these data.\n\nfilter to asecwt &gt; 0 (see paper footnote on p. 6995 about negative weights)\nmutate to create an employed variable indicating that empstat == 10 | empstat == 12\nmutate to convert sex to a factor variable using as_factor\ngroup by sex and year\nsummarize the proportion employed: use weighted.mean to take the mean of employed using the weight asecwt\n\nYour figure will be close but not identical to the original. Yours will include some years that the original did not. Feel free to change aesthetics of the plot, such as the words used in labels.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(haven)",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "assignments/pset2.html#a-new-outcome",
    "href": "assignments/pset2.html#a-new-outcome",
    "title": "Problem Set 2: Data Transformation",
    "section": "2. A new outcome",
    "text": "2. A new outcome\n25 points. The CPS-ASEC has numerous variables. Pick another variable of your choosing. Add it to your cart in IPUMS, and visualize how that variable has changed over time for those identifying as male and female.\nAs in the previous plot, year should be on the x-axis and color should represent sex. The y-axis is up to you. You can examine something like median income, proportion holding college degrees, or the 90th percentile of usual weekly work hours. You can restrict to some subset if you want, such as those who are employed.\nYour answer should include\n\na written statement of what you estimated: the variable you chose, any sample restrictions you made, and how you summarized that variable\na written interpretation of what you found\ncode following style conventions\nyour publication-quality visualization",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "assignments/pset2.html#recap-and-connections-to-your-project",
    "href": "assignments/pset2.html#recap-and-connections-to-your-project",
    "title": "Problem Set 2: Data Transformation",
    "section": "Recap and connections to your project",
    "text": "Recap and connections to your project\nA few skills relevant to the project were practiced in this problem set. In your project, you should clearly define your unit of analysis and target population. You should use sample weights if appropriate. And you should write readable, well-organized code that carries out your data transformation and visualization.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "topics/r_basics.html",
    "href": "topics/r_basics.html",
    "title": "Setting up your computer",
    "section": "",
    "text": "This topic is covered on Jan 7.\nIn order to begin producing objective evidence about inequality, you will first need to prepare your computer to analyze data.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#install-statistical-software-r",
    "href": "topics/r_basics.html#install-statistical-software-r",
    "title": "Setting up your computer",
    "section": "Install statistical software: R",
    "text": "Install statistical software: R\nWe will write code in the R programming language. R is available as open-source software at https://cran.r-project.org/. The first step to set up your computer is to install R.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#install-the-interface-rstudio",
    "href": "topics/r_basics.html#install-the-interface-rstudio",
    "title": "Setting up your computer",
    "section": "Install the interface RStudio",
    "text": "Install the interface RStudio\nWe will work with R using an interface called RStudio, which makes it easy to write code and see results all in one place. You should install RStudio Desktop, which is available to download here: https://posit.co/download/rstudio-desktop/",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#install-the-tidyverse-package",
    "href": "topics/r_basics.html#install-the-tidyverse-package",
    "title": "Setting up your computer",
    "section": "Install the tidyverse package",
    "text": "Install the tidyverse package\nMany R functions are made freely available in open-source packages that contain sets of functions designed to carry out common tasks. One package we will use often is the tidyverse, which contains functions to manipulate and visualize data. To install tidyverse, first open RStudio. Find the Console, which is a place where you can type code to immediately execute.\n\nIn the console type,\n\ninstall.packages(\"tidyverse\")\n\nand press enter or return on your keyboard. This runs a line of code to install a set of software packages.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#install-tinytex-to-produce-pdf-reports",
    "href": "topics/r_basics.html#install-tinytex-to-produce-pdf-reports",
    "title": "Setting up your computer",
    "section": "Install tinytex to produce PDF reports",
    "text": "Install tinytex to produce PDF reports\nHomework assignments will be submitted in PDF form. We will learn how to use RStudio to produce a PDF documents that embed your code, results, and written responses. In order to do so, your computer needs to have some version of LaTeX, which is software that typesets documents. Some versions of LaTeX are large and difficult to install. If you have never used LaTeX on your computer, we recommend that you install as follows: paste the code below into your R console and press enter or return to install a minimal version of the software.\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nStudents often find this step confusing, and computers present various errors. If you have an error, look on Piazza to see if anyone else has encountered your error. If not, then post a screenshot of your error on Piazza so we can help you to resolve the problem.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "topics/r_basics.html#support-and-guidance",
    "href": "topics/r_basics.html#support-and-guidance",
    "title": "Setting up your computer",
    "section": "Support and guidance",
    "text": "Support and guidance\nCongratulations on preparing your computing environment!\nThroughout the first part of the course, we will often use the online textbook R for Data Science by Hadley Wickham as a reference. The book will introduce how to work with data using R and RStudio. If you want additional guidance for setting up the software, see the Prerequisites section of R4DS. To learn more about RStudio, visit the RStudio User Guide.\nProblem sets will be completed in Quarto documents, which enable you to embed code and results within a written PDF document. You will learn about this in the problem sets. See the Quarto tutorial for additional information.",
    "crumbs": [
      " ",
      "Working with Data",
      "Setting up your computer"
    ]
  },
  {
    "objectID": "assignments/pset3.html",
    "href": "assignments/pset3.html",
    "title": "Problem Set 3: Causal Inference",
    "section": "",
    "text": "Due: 5pm on Friday, February 14.\nStudent identifer: [type your anonymous identifier here]\nThis problem set is based on:\nBertrand, M & Mullainathan, S. 2004. “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” American Economic Review 94(4):991–1013.\nRead the first 10 pages of the paper (through the end of section 2). In this paper,",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "assignments/pset3.html#analyzing-the-experimental-data-25-points",
    "href": "assignments/pset3.html#analyzing-the-experimental-data-25-points",
    "title": "Problem Set 3: Causal Inference",
    "section": "Analyzing the experimental data (25 points)",
    "text": "Analyzing the experimental data (25 points)\nLoad packages that our code will use.\n\nlibrary(tidyverse)\nlibrary(haven)\n\nDownload the study’s data from OpenICPSR: https://www.openicpsr.org/openicpsr/project/116023/version/V1/view. This will require creating an account and agreeing to terms for using the data ethically. Put the data in the folder on your computer where this .Rmd is located. Read the data into R using read_dta.\n\nd &lt;- read_dta(\"lakisha_aer.dta\")\n\n\nIf you have an error, you might need to set your working directory first. This tells R where to look for data files. At the top of RStudio, click Session -&gt; Set Working Directory -&gt; To Source File Location.\n\nYou will now see d in your Global Environment at the top right of RStudio.\nWe will use two variables:\n\n\n\nName\nRole\nValues\n\n\n\n\ncall\noutcome\n1 if resume submission yielded a callback\n\n\n\n\n0 if not\n\n\nrace\ncategory of treatments\nb if first name signals Black\n\n\n\n\nw if first name signals white\n\n\n\nThe top of Table 1 reports callback rates: 9.65% for white names and 6.45% for Black names. Reproduce those numbers. Write code that reproduces these numbers.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "assignments/pset3.html#causal-inference-concepts",
    "href": "assignments/pset3.html#causal-inference-concepts",
    "title": "Problem Set 3: Causal Inference",
    "section": "Causal inference concepts",
    "text": "Causal inference concepts\n2.1. (5 points) Fundamental problem \nOne submitted resume had the name “Emily Baker.” It yielded a callback. The same resume could have had the name “Lakisha Washington.” Explain how the Fundamental Problem of Causal Inference applies to this case (1–2 sentences).\n2.2. (5 points) Potential outcomes. Using math, write the following in potential outcomes notation: resume submission 5 would receive a callback if it signaled the name “Emily Baker”. (There are several ways to write a correct answer.) You can either type math or include a picture of handwritten math. See the bottom of this page for help.\n2.3. (5 points) Exchangeability \nIn a sentence, what is the exchangeability assumption in this study? For concreteness, for this question you may suppose that the only names in the study were “Emily Baker” and “Lakisha Washington.” Be sure to explicitly state the treatment and the potential outcomes.\n2.4. (5 points) Observational study\nSuppose that instead of randomly assigning names to fictitious resumes, the authors had instead analyzed real job applications by people named “Emily Baker” and “Lakisha Washington.” Use mathematical notation with a conditional probability \\(P(\\text{A}\\mid \\text{B})\\) to state the following: the probability of being called back was higher among resumes from Emily Baker than among resumes from Lakisha Washington.\n2.5 (5 points) Exchangeability violated\nThe descriptive estimand in the observational study (2.4) is different than the causal estimand in the experimental study (2.2). Suppose the researchers wanted to use the observational study to learn about the quantity in (2.2). Explain one way the exchangeability assumption would be violated.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "assignments/pset3.html#how-to-type-math",
    "href": "assignments/pset3.html#how-to-type-math",
    "title": "Problem Set 3: Causal Inference",
    "section": "How to type math",
    "text": "How to type math\nThere are two ways to type math within your .qmd file, outside of a code chunk.\n\nTo type in-line math surround your math in a single $ at each side. Typing $X = 1$ will produce \\(X = 1\\).\nTo type math that goes on its own equation line, use $$ before and after the math. Typing $$X = 1$$ will produce \\[ X = 1\\]\n\nWhen typing math, there are a few things you will want to know.\n\n_ indicates the start of a subscript: $Y_i$ becomes \\(Y_i\\)\n^ indicates the start of a superscript: $Y^a$ becomes \\(Y^a\\)\n{} will let you use several characters in a subscript or superscript: $Y_{unit}^{treatment}$becomes \\(Y_{unit}^{treatment}\\)\n\\mid is the vertical conditioning bar: $E(Y\\mid X)$ becomes \\(E(Y\\mid X)\\).",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "assignments/pset3.html#how-to-include-hand-written-math",
    "href": "assignments/pset3.html#how-to-include-hand-written-math",
    "title": "Problem Set 3: Causal Inference",
    "section": "How to include hand-written math",
    "text": "How to include hand-written math\nYou are also welcome to handwrite math and take a picture of it. Put your picture in the folder where your .qmd document is located. To include the picture in your writeup, type\n![](NameOfYourPictureFile.png)\n(remove the backticks before and after to use this line)",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "topics/DAGs.html",
    "href": "topics/DAGs.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "This topic is covered Feb 6. Here are slides.\nDirected Acyclic Graphs (DAGs) formalize causal assumptions mathematically in graphs. One way DAGs are useful in observational studies is by helping us to identify a sufficient adjustment set (\\(\\vec{X}\\)) such that conditional exchangeability holds.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/DAGs.html#nodes-edges-and-paths",
    "href": "topics/DAGs.html#nodes-edges-and-paths",
    "title": "Directed Acyclic Graphs",
    "section": "Nodes, edges, and paths",
    "text": "Nodes, edges, and paths\nThe previous page introduced a conditionally randomized experiment in which the researcher assigned participants to the treatment condition of (four-year college degree) vs (high school degree) with probabilities that depended on high school class rank. In this experiment, being in the top 25% of one’s high school class caused a higher chance of receiving the treatment. We will also assume that both high school performance and college completion may causally shape employment at age 40.\nWe can formalize these ideas in a graph where each node (a letter) is a variable and each edge (\\(\\rightarrow\\)) is a causal relationship.\n\n\n\n\n\n\n\n\n\nBetween each pair of nodes, you can enumerate every path or sequence of edges connecting the nodes.\n\nPath. A path between nodes \\(A\\) and \\(B\\) is any set of edges that starts at \\(A\\) and ends at \\(B\\). Paths can involve arrows in either direction.\n\nIn our DAG above, there are two paths between \\(A\\) and \\(Y\\).\n\n\\(A\\rightarrow Y\\)\n\\(A\\leftarrow X \\rightarrow Y\\)\n\nWe use paths to determine the reasons why \\(A\\) and \\(Y\\) might be statistically dependent or independent.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/DAGs.html#causal-paths",
    "href": "topics/DAGs.html#causal-paths",
    "title": "Directed Acyclic Graphs",
    "section": "Causal paths",
    "text": "Causal paths\nThe first type of path that we consider is a causal path.\n\nCausal path. A path in which all arrows point in the same direction. For example, \\(A\\) and \\(D\\) could be connected by a causal path \\(A\\rightarrow B \\rightarrow C \\rightarrow D\\).\n\nA causal path creates statistical depends between the nodes because the first node causes the last node, possibly through a sequence of other nodes.\nIn our example, there is a causal path \\(A\\rightarrow Y\\): being assigned to a four-year college degree affects employment at age 40. Because of this causal path, people who are assigned to a four-year degree have different rates of employment at age 40 than those who are not.\nA causal path can go through several variables. For example, if we listed the paths between \\(X\\) and \\(Y\\) we would include the path \\(X\\rightarrow A \\rightarrow Y\\). This is a causal path because being in the top 25% of one’s high school class increases the probability of assignment to a four-year degree in our experiment, which in turn increases the probability of employment at age 40.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/DAGs.html#fork-structures",
    "href": "topics/DAGs.html#fork-structures",
    "title": "Directed Acyclic Graphs",
    "section": "Fork structures",
    "text": "Fork structures\nTo reason about non-causal paths, we have to think about several structures that can exist along these paths. The first such structure is a fork structure.\n\nFork structure. A fork structure is a sequence of edges in which two variables (\\(A\\) and \\(B\\)) are both caused a third variable (\\(C\\)): \\(A\\leftarrow C \\rightarrow B\\).\n\nIn our example, the path \\(A\\leftarrow X \\rightarrow Y\\) involves a fork structure. being in the top 25% of one’s high school class causally affects both the treatment (college degree) and the outcome (employment at age 40).\nA fork structure creates statistical dependence between \\(A\\) and \\(Y\\) that does not correspond to a causal effect of \\(A\\) on \\(Y\\). In our example, people who are assigned to the treatment value (college degree) are more likely to have been in the top 25% of their high school class, since this high class rank affected treatment assignment in our experiment. A high class rank also affected employment at age 40. Thus, in our experiment the treatment would be associated with the outcome even if finishing college had no causal effect on employment.\nFork structures can be blocked by conditioning on the common cause. In our example, suppose we filter our data to only include those in the top 25% of their high school class. We sometimes use a box to denote conditioning on a variable, (\\(A\\leftarrow\\boxed{X}\\rightarrow Y\\)). Conditioning on \\(X\\) blocks the path because within this subgroup \\(X\\) does not vary, so it cannot cause the values of \\(A\\) and \\(Y\\) within the subgroup. In our example, if we looked among those in the top 25% of their high school classes the only reason college enrollment would be related to employment at age 40 would be the causal effect \\(A\\rightarrow Y\\).\nTo emphasize ideas, it is also helpful to consider a fork structure in an example where the variables have no causal relationship.\nSuppose a beach records for each day the number of ice cream cones sold and the number of rescues by lifeguards. There is no causal effect between these two variables; eating ice cream does not cause more lifeguard rescues and vice versa. But the two are correlated because they share a common cause: warm temperatures cause high ice cream sales and also high lifeguard rescues. A fork structure formalizes this notion: \\((\\text{ice cream sales}) \\leftarrow (\\text{warm temperature}) \\rightarrow (\\text{lifeguard rescues})\\).\nIn the population of days as a whole, this fork structure means that ice cream sales are related to lifeguard rescues. But if we condition on having a warm temperature by filtering to days when the temperature took a particular value, ice cream sales would be unrelated to lifeguard rescues across those days. This is the sense in which conditioning on the common cause variable blocks the statistical associations that would otherwise arise from a fork structure.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/DAGs.html#collider-structures",
    "href": "topics/DAGs.html#collider-structures",
    "title": "Directed Acyclic Graphs",
    "section": "Collider structures",
    "text": "Collider structures\nIn contrast to a fork structure where one variable affects two others (\\(\\bullet\\leftarrow\\bullet\\rightarrow\\bullet\\)), a collider structure is a structure where one variable is affected by two others.\n\nCollider structure. A collider structure is a sequence of edges in which two variables (\\(A\\) and \\(B\\)) both cause a third variable (\\(C\\)). We say that \\(C\\) is a collider on the path \\(A\\rightarrow C \\leftarrow B\\).\n\nFork and collider structures have very different properties, as we will illustrate through an example.\nSuppose that every day I observe whether the grass on my lawn is wet. I have sprinklers that turn on with a timer at the same time every day, regardless of the weather. It also sometimes rains. When the grass is wet, it is wet because either the sprinklers have been on or it has been raining.\n\\[\n(\\text{sprinklers on}) \\rightarrow (\\text{grass wet}) \\leftarrow (\\text{raining})\n\\]\nIf I look across all days, the variable (sprinklers on) is unrelated to the variable (raining). After all, the sprinklers are just on a timer! Formally, we say that even though (sprinklers on) and (raining) are connected by the path above, this path is blocked by the collider structure. A path does not create dependence between two variables when it contains a collider structure.\nIf I look only at the days when the grass is wet, a different pattern emerges. If the grass is wet and the sprinklers have not been on, then it must have been raining: the grass had to get wet somehow. If the grass is wet and it has not been raining, then the sprinklers must have been on. Once I look at days when the grass is wet (or condition on the grass being wet), the two input variables become statistically associated.\nA collider blocks a path when that collider is left unadjusted, but conditioning on the collider variable opens the path containing the collider.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/DAGs.html#open-and-blocked-paths",
    "href": "topics/DAGs.html#open-and-blocked-paths",
    "title": "Directed Acyclic Graphs",
    "section": "Open and blocked paths",
    "text": "Open and blocked paths\nA central purpose of a DAG is to connect causal assumptions to implications about associations that should be present (or absent) in data under those causal assumptions. To make this connection, we need a final concept of open and blocked paths.\n\nA path is blocked if it contains an unconditioned collider or a conditioned non-collider. Otherwise, the path is open. An open path creates statistical dependence between its terminal nodes whereas a blocked path does not.\n\nAs examples for a path with no colliders,\n\n\\(A\\leftarrow B \\rightarrow C \\rightarrow D\\) is an open path because no variables are conditioned and it contains no colliders.\n\\(A\\leftarrow \\boxed{B}\\rightarrow C \\rightarrow D\\) is a blocked path because we have conditioned on the non-collider \\(B\\).\n\\(A\\leftarrow  B\\rightarrow \\boxed{C} \\rightarrow D\\) is a blocked path because we have conditioned on the non-collider \\(C\\).",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/DAGs.html#determining-statistical-dependence",
    "href": "topics/DAGs.html#determining-statistical-dependence",
    "title": "Directed Acyclic Graphs",
    "section": "Determining statistical dependence",
    "text": "Determining statistical dependence\nWe are now ready to use DAGs to determine if \\(A\\) and \\(B\\) are statistically dependent. The process involves three steps.\n\nList all paths between \\(A\\) and \\(B\\).\nCross out any paths that are blocked.\nThe causal assumptions imply that \\(A\\) and \\(B\\) may be statistically dependent only if any open paths remain.\n\nAs an example, below we consider a hypothetical DAG.\n\n\n\n\n\n\n\n\n\n1. Marginal dependence\nMarginally without any adjustment, are \\(A\\) and \\(D\\) statistically dependent? We first write out all paths connecting \\(A\\) and \\(D\\).\n\n\\(A\\rightarrow B \\leftarrow C\\rightarrow D\\)\n\\(A\\leftarrow X\\rightarrow D\\)\n\nWe then cross out the paths that are blocked\n\n\\(\\cancel{A\\rightarrow B \\leftarrow C\\rightarrow D}\\) (blocked by unconditioned collider \\(B\\))\n\\(A\\leftarrow X\\rightarrow D\\)\n\nBecause an open path remains, \\(A\\) and \\(D\\) are statistically dependent.\n2. Dependence conditional on \\(X\\)\nIf we condition on \\(X\\), are \\(A\\) and \\(D\\) statistically dependent? We first write out all paths connecting \\(A\\) and \\(D\\).\n\n\\(A\\rightarrow B \\leftarrow C\\rightarrow D\\)\n\\(A\\leftarrow \\boxed{X}\\rightarrow D\\)\n\nWe then cross out the paths that are blocked\n\n\\(\\cancel{A\\rightarrow B \\leftarrow C\\rightarrow D}\\) (blocked by unconditioned collider \\(B\\))\n\\(\\cancel{A\\leftarrow \\boxed{X}\\rightarrow D}\\) (blocked by conditioned non-collider \\(X\\))\n\nBecause no open path remains, \\(A\\) and \\(D\\) are statistically independent.\n3. Dependence conditional on \\(\\{X,B\\}\\)\nIf we condition on \\(X\\) and \\(B\\), are \\(A\\) and \\(D\\) statistically dependent? We first write out all paths connecting \\(A\\) and \\(D\\).\n\n\\(A\\rightarrow B \\leftarrow C\\rightarrow D\\)\n\\(A\\leftarrow \\boxed{X}\\rightarrow D\\)\n\nWe then cross out the paths that are blocked\n\n\\(A\\rightarrow \\boxed{B} \\leftarrow C\\rightarrow D\\) (open since collider \\(B\\) is conditioned)\n\\(\\cancel{A\\leftarrow \\boxed{X}\\rightarrow D}\\) (blocked by conditioned non-collider \\(X\\))\n\nBecause an open path remains, \\(A\\) and \\(D\\) are statistically dependent.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/DAGs.html#causal-identification-with-dags",
    "href": "topics/DAGs.html#causal-identification-with-dags",
    "title": "Directed Acyclic Graphs",
    "section": "Causal identification with DAGs",
    "text": "Causal identification with DAGs\nWhen our aim is to identify the average causal effect of \\(A\\) on \\(Y\\), we want to choose a set of variables for adjustment so that all remaining paths are causal paths. We call this a sufficient adjustment set.\n\nA sufficient adjustment set for the causal effect of \\(A\\) on \\(Y\\) is a set of nodes that, when conditioned, block all non-causal paths between \\(A\\) and \\(Y\\).\n\nIn our example from the top of this page, there were two paths between \\(A\\) and \\(Y\\):\n\n\\((A\\text{: college degree})\\rightarrow (Y\\text{: employed at age 40})\\)\n\\((A\\text{: college degree})\\leftarrow (X\\text{: top 25\\% of high school class})\\rightarrow (Y\\text{: employed at age 40})\\)\n\nIn this example, \\(X\\) is a sufficient adjustment set. Once we condition on \\(X\\) by e.g. filtering to those in the top 25% of their high school class, the only remaining path between \\(A\\) and \\(Y\\) is the causal path \\(A\\rightarrow Y\\). Thus, the difference in means in \\(Y\\) across \\(A\\) within subgroups defined by \\(X\\) identifies the conditional average causal effect of \\(A\\) on \\(Y\\).\nA more difficult example.\nSufficient adjustment sets can be much more complicated. As an example, consider the DAG below.\n\n\n\n\n\n\n\n\n\nWe first list all paths between \\(A\\) and \\(Y\\).\n\n\\(A\\rightarrow Y\\)\n\\(A\\rightarrow M\\rightarrow Y\\)\n\\(A\\leftarrow X_1\\rightarrow X_3 \\rightarrow Y\\)\n\\(A\\leftarrow X_1\\rightarrow X_3 \\leftarrow X_2\\rightarrow Y\\)\n\\(A\\leftarrow X_3 \\rightarrow Y\\)\n\\(A\\leftarrow X_3\\leftarrow X_2 \\rightarrow Y\\)\n\nThe first two paths are causal, and the others are non-causal. We want to find a sufficient adjustment set to block all the non-causal paths.\nIn order to block paths (3), (5), and (6) we might condiiton on \\(X_3\\). But doing so opens path (2), which was otherwise blocked by the collider \\(X_3\\). In order to also block path (2), we might additionally condition on \\(X_1\\). In this case, our sufficient adjustment set is \\(\\{X_1,X_3\\}\\).\n\n\\(A\\rightarrow Y\\)\n\\(A\\rightarrow M\\rightarrow Y\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_1}\\rightarrow \\boxed{X_3} \\rightarrow Y}\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_1}\\rightarrow \\boxed{X_3} \\leftarrow X_2\\rightarrow Y}\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_3} \\rightarrow Y}\\)\n\\(\\cancel{A\\leftarrow \\boxed{X_3}\\leftarrow X_2 \\rightarrow Y}\\)\n\nThen the only open paths are paths (1) and (2), both of which are causal paths from \\(A\\) to \\(Y\\).\nSometimes there are several sufficient adjustment sets. In this example, sufficient adjustment sets include:\n\n\\(\\{X_1,X_3\\}\\)\n\\(\\{X_2,X_3\\}\\)\n\\(\\{X_1,X_2,X_3\\}\\)\n\nWe sometimes call the first two minimal sufficient adjustment sets because they are the smallest.\n\nA minimal sufficient adjustment set is an adjustment set that achieves causal identification by conditioning on the fewest number of variables possible.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/DAGs.html#how-to-draw-a-dag",
    "href": "topics/DAGs.html#how-to-draw-a-dag",
    "title": "Directed Acyclic Graphs",
    "section": "How to draw a DAG",
    "text": "How to draw a DAG\nSo far, we have focused on causal identification with a DAG that has been given. But how do you draw one for yourself?\nWhen drawing a DAG, there is an important rule: any node that would have edges pointing into any two nodes already represented in the DAG must be included. This is because what you omit from the DAG is far more important than what you include in the DAG. Many of your primary assumptions are about the nodes and edges that you leave out.\nFor example, suppose we are estimating the causal effect of a college degree on employment at age 40. After beginning our DAG with only these variables, we have to think about any other variables that might affect these two. High school performance is one example. Then we have to include any nodes that affect any two of {high school performance, college degree, employment at age 40}. Perhaps a person’s parents’ education affects that person’s high school performance and college degree attainment. Then parents’ education should be included as an additional node. The cycle continues, so that in observational causal inference settings you are likely to have a DAG with many nodes.\nIn practice, you may not have data on all the nodes that comprise the sufficient adjustment set in your graph. In this case, we recommend that you first draw a graph under which you can form a sufficient adjustment set with the measured variables. This allows you to state one set of causal beliefs under which your analysis can answer your causal question. Then, also draw a second DAG that includes the other variables you think are relevant. This will enable you to reason about the sense in which your results could be misleading because of omitting important variables.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "topics/DAGs.html#closing-thoughts",
    "href": "topics/DAGs.html#closing-thoughts",
    "title": "Directed Acyclic Graphs",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nDAGs are a powerful tool for causal inference, because they are both visually intuitive and mathematically precise. They translate our theories about the world into a formal language with implications for causal identification.\nIf you’d like to learn more about DAGs, here are a few good resources:\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. New York: Basic Books.\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. New York: Wiley.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "Discussion Meetings",
    "section": "",
    "text": "This page contains a general plan for the topics of the 10 discussion section meetings that will occur this quarter.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#jan-79",
    "href": "discussion.html#jan-79",
    "title": "Discussion Meetings",
    "section": "Jan 7–9",
    "text": "Jan 7–9\nIn this discussion, we will introduce ourselves and ensure our computers are prepared for the class. This will involve checking that we have R, RStudio, tinytex, and tidyverse installed. See R basics.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#jan-1416",
    "href": "discussion.html#jan-1416",
    "title": "Discussion Meetings",
    "section": "Jan 14–16",
    "text": "Jan 14–16\nDue to the fires, this week’s discussions became Zoom office hours.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#jan-2123",
    "href": "discussion.html#jan-2123",
    "title": "Discussion Meetings",
    "section": "Jan 21–23",
    "text": "Jan 21–23\nIn this discussion, you will register for an account to access the Current Population Survey data. Click here for detailed instructions.\nYour TA may walk through how to look at the survey documentation for one or more of the variables. If time allows, we will break into small groups to look for other variables that might be interesting in a project, and then come back together to tell the class about them.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#jan-2830",
    "href": "discussion.html#jan-2830",
    "title": "Discussion Meetings",
    "section": "Jan 28–30",
    "text": "Jan 28–30\nYou should read the following paper before this discussion:\n\nEngland, Paula, Andrew Levine, and Emma Mishel. 2020. Progress toward gender equality in the United States has slowed or stalled, PNAS 117(13):6990–6997.\n\nThe problem set reproduces findings from this paper. The discussion will focus on conceptual questions from the reading. The questions below are only guidelines, and your TA might have other topics to discuss.\n1. The authors write that “change in the gender system has been deeply asymmetric.” Explain this in a sentence or two to someone who hasn’t read the article.\n2. The authors discuss cultural changes that could lead to greater equality. Propose a question that could (hypothetically) be included in the CPS-ASEC questionnaire to help answer questions about cultural changes.\n\n\n\n\n\n\nTip\n\n\n\nIf you are not sure how to word a survey question, here are some examples from the American Time Use Survey, Current Population Survey, and General Social Survey.\n\n\n3. The authors discuss institutional changes that could lead to greater equality. Propose a question that could (hypothetically) be included in the CPS-ASEC questionnaire to help answer questions about institutional changes.\n4. What was one fact presented in this paper that most surprised you?\n5. What about the graphs in this paper is compelling? What would you improve or change if you were producing these graphs?",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#feb-46",
    "href": "discussion.html#feb-46",
    "title": "Discussion Meetings",
    "section": "Feb 4–6",
    "text": "Feb 4–6\nYour TA will introduce the final project, which will be carried out in groups of about 5 students within your discussion section. Your TA will help you to form groups with common interest areas.\nPossible topics include (but are not limited to) inequality by race or gender, trends in poverty or inequality over time, and the causal effect of education or other life experiences on subsequent outcomes. Another possibility is to form groups centered on a particular dataset, with the topic to be decided later.\nAll subsequent discussions are devoted to group work on the final project.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#feb-1113",
    "href": "discussion.html#feb-1113",
    "title": "Discussion Meetings",
    "section": "Feb 11–13",
    "text": "Feb 11–13\nBy the end of this discussion, your group should select a dataset for the final project. If you aren’t sure where to start, we suggest the options at ipums.org.\nIf you finish early, you are always welcome to move on to the tasks below.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#feb-1820",
    "href": "discussion.html#feb-1820",
    "title": "Discussion Meetings",
    "section": "Feb 18–20",
    "text": "Feb 18–20\nBy the end of this discussion, your group should\n\nselect an outcome variable\ndefine the unit of analysis (e.g., a person, a school, a state) for which that outcome variable is defined\ndetermine how you plan to aggregate the outcome variable across units (e.g., by a mean, median, proportion)\n\nIf you finish early, you are always welcome to move on to the tasks below.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#feb-2527",
    "href": "discussion.html#feb-2527",
    "title": "Discussion Meetings",
    "section": "Feb 25–27",
    "text": "Feb 25–27\nBy the end of this discussion, your group should discuss the target population and whether you will need to use sampling weights in your analysis.\nIf you finish early, you are always welcome to move on to the tasks below.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#mar-46",
    "href": "discussion.html#mar-46",
    "title": "Discussion Meetings",
    "section": "Mar 4–6",
    "text": "Mar 4–6\nThis discussion is open for your group to work on any remaining tasks to finish up your final project. Recall that PDF slides and writeup are due by 5pm on Mar 7, with one submission per group.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#mar-1113",
    "href": "discussion.html#mar-1113",
    "title": "Discussion Meetings",
    "section": "Mar 11–13",
    "text": "Mar 11–13\nYour group will present your final project in this discussion. During each presentation by a group that is not your own, you will be asked to note something you appreciated about their presentation.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "discussion.html#other-discussion-possibilities",
    "href": "discussion.html#other-discussion-possibilities",
    "title": "Discussion Meetings",
    "section": "Other discussion possibilities",
    "text": "Other discussion possibilities\nThis part of the page has other possibilities for discussion topics. We may use these in one of the discussions depending on how the course material is progressing over the quarter.\n\nExplain a ggplot function\nLast week’s lecture introduced visualization with the ggplot() function in the tidyverse package. This discussion explores more of what ggplot() can do.\n\nForm groups of about 4 students\nIntroduce yourself to your group members\nChoose a graph that your group likes in Ch 1 of R For Data Science.\n\nLook at the code for that graph\nDiscuss what each element of the code is doing\nChoose someone to present to the class\n\nAt the end of discussion, we will regroup. Each group will present the code for their chosen graph to the class\n\n\n\nExplain a tidyverse function\nThis discussion explores more of what the tidyverse can do.\n\nForm groups of about 4 students\nIntroduce yourself to your group members\nChoose a function in Ch 3 of R For Data Science that we did not cover explicitly in class.\n\nPossibilities include: arrange(), distinct(), rename(), relocate(), the slice_ functions, ungroup()\nFind an example of the function in the chapter.\nDiscuss how the code works.\nPrepare to explain to the class.\n\nAt the end of discussion, we will regroup. Each group will present the code for their chosen graph to the class\n\n\n\nWrite a function\nNow that we have worked with functions in R, it is time to understand them on a deeper level. In this exercise, we will write our own functions.\n\nA basic function\nYou can store a function as an object in your environment, just like any other object. The function below accepts a numeric variable x and returns twice the value of x.\n\ndouble_x &lt;- function(x) {\n  doubled &lt;- 2 * x\n  return(doubled)\n}\n\nThere are a few pieces to the code above\n\nWe created a function that has a name: double_x\nOur function that takes one argument named x\nThe body of the function is the lines within the {}. These lines take the argument, do some things, and then return() a result. The object within return() is what R sends back after running the function.\n\nOnce you create that function, you can run it just like any other function.\n\ndouble_x(x = 2)\n\n[1] 4\n\n\n\n\nA function with two arguments\nA function can also take multiple arguments, such as one that adds x and y.\n\nadd_x_y &lt;- function(x, y) {\n  added &lt;- x + y\n  return(added)\n}\n\nwhich works as follows.\n\nadd_x_y(x = 2, y = 3)\n\n[1] 5\n\n\n\n\nChallenge: Write your own function\nA function does not have to just take numbers as an argument. It can also take a dataset as an argument. Sometimes, we might want an estimator(data) function that takes data as an argument and applies an estimator() to that data, to return an estimate of something. You will create one such function here.\nAs an example, suppose three surveys ask people if they prefer chocolate ice cream (prefers_chocolate = TRUE) or vanilla ice cream (prefers_chocolate = FALSE). The survey also records whether the respondent is a child age = \"child\" or an adult age = \"adult\".\n\nlibrary(tidyverse)\nsample_a &lt;- tibble(\n  age = c(\"child\",\"child\",\"child\",\"adult\",\"adult\"),\n  prefers_chocolate = c(T,T,F,F,F)\n)\nsample_b &lt;- tibble(\n  age = c(\"child\",\"child\",\"adult\",\"adult\"),\n  prefers_chocolate = c(T,F,T,F)\n)\nsample_c &lt;- tibble(\n  age = c(\"child\",\"child\",\"child\",\"adult\",\"adult\",\"adult\"),\n  prefers_chocolate = c(T,T,F,F,F,T)\n)\n\nWe want to know the proportion preferring chocolate among children and adults in each sample. To estimate in sample_a, we would write\n\nestimate &lt;- sample_a |&gt;\n  group_by(age) |&gt;\n  summarize(prefers_chocolate = mean(prefers_chocolate))\n\nNow write that within a function.\n\nestimator &lt;- function(data) {\n  # do some things to data\n  # return() your estimate\n}\n\nand apply the estimator to sample_a, sample_b, and sample_c.",
    "crumbs": [
      " ",
      "Discussion Meetings"
    ]
  },
  {
    "objectID": "topics/matching.html",
    "href": "topics/matching.html",
    "title": "Matching",
    "section": "",
    "text": "Here are slides on matching.\nMatching is a method for causal inference that is analogous to model-based estimation, but is often easier to explain. Suppose we have a set of 5 units, of whom 2 are treated.\nWe would like to infer that treatment causes higher outcomes, but the units also differ along a confounding variable \\(L\\). How can we infer the average treatment effect for units 1 and 2?\nOne way to draw inference is by a model: model \\(Y^0\\) as a function of \\(L\\) among the untreated units, and then use our model to predict for the treated units.\nAnother strategy might involve no model at all. We notice that unit 3 is very similar to unit 1 along \\(L\\). Likewise, unit 2 is very similar to unit 5. We could match these units together and use the matches to infer the unobserved potential outcomes.\nWe then estimate the average effect for units 1 and 2 by the difference of",
    "crumbs": [
      " ",
      "Inference with Models",
      "Matching"
    ]
  },
  {
    "objectID": "topics/matching.html#matching-in-math",
    "href": "topics/matching.html#matching-in-math",
    "title": "Matching",
    "section": "Matching in math",
    "text": "Matching in math\nFormally, let \\(\\text{match}(i)\\) denote the index of the match for unit \\(i\\). Our goal is to estimate the average treatment effect on the treated,\n\\[\\tau = \\frac{1}{n_1}\\sum_{i:A_i=1}\\left(Y_i^1 - Y_i^0\\right)\\]\nwhere \\(n_1\\) is the number of treated units and the sum is taken over all units \\(i\\) such that the treatment took the value 1 for those units.\nThe fundamental problem of causal inference is that for these units \\(Y_i^1\\) is observed but \\(Y_i^0\\) is not. But for each treated unit \\(i\\), we find an untreated match \\(j = \\text{match}(i)\\) who is very simialr to \\(i\\) but for whom \\(Y_j^0\\) is observed. We then estimate by the mean difference between the treated units and their matched controls.\n\\[\\hat\\tau_\\text{Matching} = \\frac{1}{n_1}\\sum_{i:A_i=1}\\left(Y_i^1 - Y_{\\text{match}(i)}^0\\right)\\]",
    "crumbs": [
      " ",
      "Inference with Models",
      "Matching"
    ]
  },
  {
    "objectID": "topics/matching.html#matching-vs.-regression",
    "href": "topics/matching.html#matching-vs.-regression",
    "title": "Matching",
    "section": "Matching vs. regression",
    "text": "Matching vs. regression\nWhy should we prefer matching or regression?\nWe have already learned a regression solution to this problem: assume a sufficient adjustment set \\(\\vec{X}\\), model the \\(Y_i^0\\) outcomes as a function of \\(\\vec{X}\\) for a set of units who factually were untreated, and use the model to predict what would happen for the treated units if they had been untreated \\((\\hat{Y}_i^0)\\). Then our estimator of the ATT would be:\n\\[\\hat\\tau_\\text{Regression} = \\frac{1}{n_1}\\sum_{i:A_i=1}\\left(Y_i^1 - \\hat{\\text{E}}(Y\\mid\\vec{X} = \\vec{x}_i, A = 0)\\right)\\]\nMatching is actually doing the same thing—we are just using the outcome of unit \\(j\\) as an estimator of \\(\\hat{\\text{E}}(Y\\mid\\vec{X} = \\vec{x}_i, A = 0)\\).\nWhy would we then prefer matching? One reason is explainability. A model is easy to explain to social scientists and statisticians who are familiar with models. It isn’t as good when you are speaking to policymakers and others who are unfamiliar with models.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Matching"
    ]
  },
  {
    "objectID": "topics/matching.html#distances-for-multivariate-matching",
    "href": "topics/matching.html#distances-for-multivariate-matching",
    "title": "Matching",
    "section": "Distances for multivariate matching",
    "text": "Distances for multivariate matching\nSuppose we have treated and untreated units that differ along two confounding variables, \\(L_1\\) and \\(L_2\\).\n\nWhich control unit should be chosen as the match? With more than one variable, it is not immediately obvious which control points is “closest” to the treated point—we first need to define “closest.” Which one we would choose requires one to choose a distance metric.\n\nDistance metric. A function \\(d()\\) that takes two vectors \\(\\vec{x}\\) and \\(\\vec{x}'\\) and returns a scalar numeric distance \\(d(\\vec{x},\\vec{x}')\\).\n\n\n\nManhattan distance\nImagine that the points are places in Manhattan, where the streets are arranged in a grid. The way to travel from Treated to Untreated 1 is by traveling 4 blocks north and then 3 blocks east, for a total distance of 7 units. This distance metric is called Manhattan distance.\n\nManhattan distance. The distance between two vectors \\(\\vec{x}\\) and \\(\\vec{x}'\\) is the sum of their absolute differences on each element: \\[d_\\text{Manhattan}(\\vec{x},\\vec{x}') = \\sum_p \\lvert x_p - x'_p \\rvert \\]\n\nBy Manhattan distance, we might determine that the unit (Untreated 2) is closest to (Treated) because its Manhattan distance from the treated unit is 6 instead of 7.\n\n\nEuclidean distance\nImagine instead that the points are in a field, and you are a crow. The distance that is relevant to you is the most direct line—the distance as the crow flies! This is Euclidean distance.\n\nEuclidean distance. The distance between two vectors \\(\\vec{x}\\) and \\(\\vec{x}'\\) is the square root of the sum of their squared differences on each element: \\[d_\\text{Euclidean}(\\vec{x},\\vec{x}') = \\sqrt{\\sum_p \\left( x_p - x'_p \\right)^2} \\]\n\nBy Euclidean distance, we would choose a different matched control unit! The unit (Untreated 2) is 6 units away from the (Treated) unit in terms of Euclidean distance, and the unit (Untreated 1) is only 5 units away. By Euclidean distance, the match to choose is (Untreated 1).\nThe comparison between Euclidean and Manhattan distances shows that our choice of distance metric can shape who we choose as matches.\nIn practice, neither Manhattan nor Euclidean distance is commonly used for matching. Instead, researchers often use Mahalanobis distance, which is a generalization of Euclidean distance that takes into account the variance and covariance of \\(\\vec{X}\\). And an even more common distance metric is propensity score distance, which we discuss next.\n\n\nPropensity score distance\nA distance metric requires us to map a pair of vectors \\((\\vec{x},\\vec{x}')\\) into a single-number distance. Thankfully, there is already a way we often map a vector of confounders to a number: predict the probability of treatment.\nSuppose we estimate each unit’s probability of being treated, \\(\\text{P}(A = 1\\mid \\vec{X} = \\vec{x}_i)\\). We might then define the distance between two units as the distance between their predicted probabilities of being treated.\n\nPropensity score distance. The distance between two vectors \\(\\vec{x}\\) and \\(\\vec{x}'\\) is the squared difference in the probability of treatment under these two vectors. \\[d_\\text{PropensityScore}(\\vec{x},\\vec{x}') = \\left(\\text{P}(A = 1\\mid \\vec{X} = \\vec{x}) - \\text{P}(A = 1\\mid \\vec{X} = \\vec{x}')\\right)^2\\]\n\nOften, the propensity score \\(\\text{P}(A = 1\\mid \\vec{X})\\) is estimated by a logistic regression model, but one could estimate by any machine learning strategy or nonparametrically if \\(\\vec{X}\\) is discrete.\nPropensity score matching is especially intuitive. With propensity score matching, the researcher matches each treated unit to a control unit who had a similar probability of being treated.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Matching"
    ]
  },
  {
    "objectID": "topics/matching.html#choices-after-the-distance",
    "href": "topics/matching.html#choices-after-the-distance",
    "title": "Matching",
    "section": "Choices after the distance",
    "text": "Choices after the distance\nAfter you define the distance, there are many additional choices for how to conduct matching! These choices often involve a tradeoff between bias and variance.\n\nWith and without replacement\nBelow are two treated and two untreated units who differ on one confounding variable \\(L\\). The left-most treated unit is matched to the left-most control unit. The algorithm then moves to the right-most treated unit: which control unit should serve as its match?\n\nOne argument is that the left-most control unit should again serve as the match—it is clearly the closest control unit. But it has already been used! If we want to enforce that each treated unit gets its own unique untreated match, then we should match to the unit at the right.\nThis is the choice between matching with and without replacement.\n\nWith replacement: After an untreated unit is used as a match for one treated unit, it is returned to the pool of untreated units to be considered for future matches.\nWithout replacement: After an untreated unit is used as a match for one treated unit, it is never again used as a match.\n\nMatching with replacement yields the closest possible matches: each treated unit gets paired with the closest control unit that can be found. In this sense, matching with replacement reduces bias.\nBut matching with replacement can also produce a high-variance estimator. Suppose you have 50 treated units who all get matched to a single untreated unit—the random chance that included that particular untreated unit in the sample has huge influence on the resulting estimate!\nThe choice of with and without replacement has no correct answer; whether one or the other is better will depend on the bias and variance in a particular research setting.\n\n\nk:1 matching\nShould each treated unit be matched to only one untreated unit, or should we match to more than one untreated unit and take the average? A k:1 matching algorithm matches each treated unit to \\(k\\) untreated units.\n\nThe advantage of k:1 matching is a reduction in variance: by averaging over a larger number of untreated units, the resulting estimator will vary less from sample to sample. But the cost of k:1 matching bias: the two closest matches are not generally going to be collectively as close to the treated unit as the single closest match.\n\n\nCalipers\nSuppose we have a treated unit, and there seem to be no comparable control units. Is there a point at which we give up the search?\n\nIn our illustration, the treated point at the far right is very far from both untreated units. It is not clear that we should try to match this unit. To formalize that it is too far, we might define the black bars as the farthest distance we are willing to look for a match. The width of these bars is known as a caliper.\n\nCaliper. The maximum distance between a treated and untreated unit such that we will consider them possible matches.\n\nIn caliper matching, we would not match the right-most treated unit to any untreated unit. Instead, we would update the estimand to be the average treatment effect on the treated among those within the caliper distance from the control units.\nCaliper matching can be good because it avoids bad matches. But caliper matching comes with a cost—it changes the causal estimand to the causal effect in a subgroup who can be hard to explain!",
    "crumbs": [
      " ",
      "Inference with Models",
      "Matching"
    ]
  },
  {
    "objectID": "topics/matching.html#regression-after-matching",
    "href": "topics/matching.html#regression-after-matching",
    "title": "Matching",
    "section": "Regression after matching",
    "text": "Regression after matching\nAfter matching, there are two estimators we could consider.\n\nMean \\(Y\\) among treated units - mean \\(Y\\) of matched control units\nCoefficient on \\(A\\) in a regression of \\(Y\\) on \\(A\\) and \\(\\vec{X}\\) among matches\n\nEstimator (2) is preferable in the sense that the regression model can correct for imperfections in our matching. Despite our best efforts, the matched controls will not be quite equal to the treated units along \\(\\vec{X}\\)! Regression can fix this. In this sense, we can think of matching as a preprocessing step before regression.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Matching"
    ]
  },
  {
    "objectID": "topics/matching.html#matching-in-code",
    "href": "topics/matching.html#matching-in-code",
    "title": "Matching",
    "section": "Matching in code",
    "text": "Matching in code\nThe MatchIt package in R carries out all kinds of matching applications. You can do all of the above using MatchIt.\nHere is one example, using our data from the model-based inference page.\n\nlibrary(tidyverse)\nlibrary(MatchIt)\n\n\ndata &lt;- read_csv(\"https://soc114.github.io/data/nlsy97_simulated.csv\")\n\n\nmatched &lt;- matchit(\n  # A formula for treatment given confounders.\n  # Treatment must be a binary or logical variable.\n  formula = (a == \"treated\") ~ sex + race + mom_educ + dad_educ +\n    log_parent_income + log_parent_wealth + test_percentile,\n  # Data containing variables\n  data = data,\n  # Conduct propensity score matching\n  distance = \"glm\",\n  link = \"logit\",\n  estimand = \"ATT\"\n) |&gt; print()\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 7771 (original), 3048 (matched)\n - target estimand: ATT\n - covariates: sex, race, mom_educ, dad_educ, log_parent_income, log_parent_wealth, test_percentile\n\n\nWe see that this carried out 1:1 nearest neighbor matching without replacement, with distance estimated by the propensity score estimated with logistic regression. We can extract the resulting matches with the match.data() function.\n\nmatches &lt;- match.data(matched)\n\nThen we can estimate by the mean difference across matched treated and control units. In the event of matching with replacement or k:1 matching, it is important to include weights since each unit may be used as a match multiple times.\n\nmatches |&gt;\n  group_by(a) |&gt;\n  summarize(estimate = weighted.mean(y, w = weights))\n\n# A tibble: 2 × 2\n  a         estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 treated      0.518\n2 untreated    0.233\n\n\nAlternatively, we can carry out regression after matching.\n\nlm_after_matching &lt;- lm(\n  y ~ (a == \"treated\") + sex + race + mom_educ + dad_educ +\n    log_parent_income + log_parent_wealth + test_percentile,\n  data = matches,\n  weights = weights\n)\n\nThen our estimate could be the coefficient on the treatment variable.\n\nsummary(lm_after_matching)\n\n\nCall:\nlm(formula = y ~ (a == \"treated\") + sex + race + mom_educ + dad_educ + \n    log_parent_income + log_parent_wealth + test_percentile, \n    data = matches, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8542 -0.3488 -0.1638  0.4430  0.9574 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                -0.0491272  0.0480226  -1.023 0.306389    \na == \"treated\"TRUE          0.2586579  0.0165599  15.620  &lt; 2e-16 ***\nsexMale                     0.0943613  0.0167787   5.624 2.04e-08 ***\nraceNon-Hispanic Black     -0.0222555  0.0325394  -0.684 0.494056    \nraceNon-Hispanic Non-Black  0.0546578  0.0261997   2.086 0.037044 *  \nmom_educCollege             0.0700367  0.0391409   1.789 0.073658 .  \nmom_educHigh school         0.0249783  0.0390663   0.639 0.522624    \nmom_educNo mom              0.0342920  0.0581012   0.590 0.555093    \nmom_educSome college        0.0553309  0.0382264   1.447 0.147873    \ndad_educCollege             0.1044855  0.0426691   2.449 0.014392 *  \ndad_educHigh school        -0.0199892  0.0438341  -0.456 0.648408    \ndad_educNo dad              0.0035126  0.0438258   0.080 0.936124    \ndad_educSome college        0.0452056  0.0426203   1.061 0.288930    \nlog_parent_income           0.0015685  0.0003327   4.714 2.53e-06 ***\nlog_parent_wealth           0.0011310  0.0003276   3.452 0.000563 ***\ntest_percentile             0.0011585  0.0003330   3.479 0.000511 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4493 on 3032 degrees of freedom\nMultiple R-squared:  0.1433,    Adjusted R-squared:  0.1391 \nF-statistic: 33.82 on 15 and 3032 DF,  p-value: &lt; 2.2e-16\n\n\nUsing the regression-after-matching strategy, we estimate that going to college leads to a 0.26 increase in the probability of having a college-educated spouse or residential partner.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Matching"
    ]
  },
  {
    "objectID": "topics/matching.html#what-to-read",
    "href": "topics/matching.html#what-to-read",
    "title": "Matching",
    "section": "What to read",
    "text": "What to read\nTo learn more about matching, a good article is:\n\nStuart, Elizabeth. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.”. Statistical Science 25(1):1-21.\n\nTo see how matching methods have been used in questions of social stratification, see this book and papers cited in it.\n\nBrand, Jennie E. 2023. Overcoming the Odds: The Benefits of Completing College for Unlikely Graduates. Russell Sage Foundation. Here is a link to read online through the UCLA Library.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Matching"
    ]
  },
  {
    "objectID": "topics/sample_splitting.html",
    "href": "topics/sample_splitting.html",
    "title": "Sample Splitting",
    "section": "",
    "text": "A predictive model is an input-output function:\nThe performance of a model can be measured by how well the output \\(\\hat{y}\\) corresponds to the true outcome \\(y\\). This page considers three ways to assess the performance of a model.\nOften, the purpose a predictive model is to accomplish out-of-sample prediction (2). But when learning the model, often only one sample is available. Therefore, evaluation by split-sample prediction (3) is often desirable because it most closely mimics this task."
  },
  {
    "objectID": "topics/sample_splitting.html#simulated-setting",
    "href": "topics/sample_splitting.html#simulated-setting",
    "title": "Sample Splitting",
    "section": "Simulated setting",
    "text": "Simulated setting\nWhen a model is evaluated by in-sample prediction, there is a danger: even if the features have no predictive value in the population, a model might discover patterns that exist in the training sample due to random variation.\nTo illustrate this, we generate a simulation with 100 features x1,…,x100 and one outcome y, all of which are independent normal variables. We know from the beginning that x* should be useless predictors: they contain no information about y.\nWe first load tidyverse\nand write a function to generate the data\n\ngenerate_data &lt;- function(sample_size = 1000, num_features = 100) {\n  # Predictors are independent Normal\n  X &lt;- replicate(num_features, rnorm(sample_size))\n  colnames(X) &lt;- paste0(\"x\",1:num_features)\n  \n  as_tibble(X) |&gt;\n    # Outcome is an independent Normal\n    mutate(y = rnorm(n()))\n}\n\nbefore applying that function to generate one sample.\n\ndata &lt;- generate_data(sample_size = 1000, num_features = 100)"
  },
  {
    "objectID": "topics/sample_splitting.html#in-sample-prediction",
    "href": "topics/sample_splitting.html#in-sample-prediction",
    "title": "Sample Splitting",
    "section": "In-sample prediction",
    "text": "In-sample prediction\nWe then estimate a linear regression model, where . includes all the x1,…,x100 features other than y as predictors.\n\nmodel &lt;- lm(y ~ ., data = data)\n\nand a benchmark of no model, which only includes an intercept.\n\nno_model &lt;- lm(y ~ 1, data = data)\n\nWe know from the simulation that the model is useless: the x-variables contain no information about y. But if we make predictions in-sample, we will see that the mean squared error of the model is surprisingly lower (better) than no model.\n\ndata |&gt;\n  mutate(\n    predicted_model = predict(model),\n    predicted_no_model = predict(no_model),\n    squared_error_model = (y - predicted_model) ^ 2,\n    squared_error_no_model = (y - predicted_no_model) ^ 2,\n  ) |&gt;\n  select(starts_with(\"squared\")) |&gt;\n  summarize_all(.funs = mean)\n\n# A tibble: 1 × 2\n  squared_error_model squared_error_no_model\n                &lt;dbl&gt;                  &lt;dbl&gt;\n1               0.902                   1.00"
  },
  {
    "objectID": "topics/sample_splitting.html#out-of-sample-prediction",
    "href": "topics/sample_splitting.html#out-of-sample-prediction",
    "title": "Sample Splitting",
    "section": "Out-of-sample prediction",
    "text": "Out-of-sample prediction\nThe problem is that the model fit to the noise in the data. We can see this with the out-of-sample performance assessment. First, we generate a new dataset of out-of-sample data.\n\nout_of_sample &lt;- generate_data()\n\nThen, we use the model learned in data and predict in out_of_sample. By this evaluation, predictions are now worse (higher mean squared error) than no model.\n\nout_of_sample |&gt;\n  mutate(\n    predicted_model = predict(model, newdata = out_of_sample),\n    predicted_no_model = predict(no_model, newdata = out_of_sample),\n    squared_error_model = (y - predicted_model) ^ 2,\n    squared_error_no_model = (y - predicted_no_model) ^ 2,\n  ) |&gt;\n  select(starts_with(\"squared\")) |&gt;\n  summarize_all(.funs = mean)\n\n# A tibble: 1 × 2\n  squared_error_model squared_error_no_model\n                &lt;dbl&gt;                  &lt;dbl&gt;\n1                1.05                  0.981"
  },
  {
    "objectID": "topics/sample_splitting.html#split-sample-prediction",
    "href": "topics/sample_splitting.html#split-sample-prediction",
    "title": "Sample Splitting",
    "section": "Split-sample prediction",
    "text": "Split-sample prediction\nIn practice, we often do not have a second sample. We can therefore mimic the out-of-sample task by a sample split, which we can create using the initial_split function in the rsample package,\n\nlibrary(rsample)\nsplit &lt;- initial_split(data, prop = .5)\n\nwhich randomly assigns the data into training and testing sets of equal size. We can extract those data by typing training(split) and testing(split).\nThe strategy is to learn on training(split)\n\nmodel &lt;- lm(y ~ ., data = training(split))\nno_model &lt;- lm(y ~ 1, data = training(split))\n\nand evaluate performance on testing(split).\n\ntesting(split) |&gt;\n  mutate(\n    predicted_model = predict(model, newdata = testing(split)),\n    predicted_no_model = predict(no_model, newdata = testing(split)),\n    squared_error_model = (y - predicted_model) ^ 2,\n    squared_error_no_model = (y - predicted_no_model) ^ 2,\n  ) |&gt;\n  select(starts_with(\"squared\")) |&gt;\n  summarize_all(.funs = mean)\n\n# A tibble: 1 × 2\n  squared_error_model squared_error_no_model\n                &lt;dbl&gt;                  &lt;dbl&gt;\n1                1.29                   1.01\n\n\nJust like the out-of-sample prediction, this shows that the model is worse than no model at all. By sample splitting, we can learn this even when we have only one sample.\nAn important caveat is that sample splitting has a cost: the number of cases available for training is smaller once we split the sample. This can mean that the sample-split predictions will have worse performance than predictions trained on the full sample and evaluated out-of-sample."
  },
  {
    "objectID": "topics/sample_splitting.html#repeating-this-many-times",
    "href": "topics/sample_splitting.html#repeating-this-many-times",
    "title": "Sample Splitting",
    "section": "Repeating this many times",
    "text": "Repeating this many times\nIf we repeat the above many times, we can see the distribution of these performance evaluation strategies across repeated samples.\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\n\n\n\n\n\n\n\nBy the gold standard of out-of-sample prediction, no model is better than a model. In-sample prediction yields the misleading appearance that a model is better than no model. Split-sample prediction successfully mimics the out-of-sample behavior when only one sample is available."
  },
  {
    "objectID": "topics/sample_splitting.html#closing-thoughts",
    "href": "topics/sample_splitting.html#closing-thoughts",
    "title": "Sample Splitting",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nSample splitting is an art as much as a science. In particular applications, the gain from sample splitting is not always clear and must be balanced against the reduction in cases available for training. It is important to remember that out-of-sample prediction remains the gold standard, and sample splitting is one way to approximate that when only one sample is available."
  },
  {
    "objectID": "topics/what_is_a_model.html",
    "href": "topics/what_is_a_model.html",
    "title": "What is a model?",
    "section": "",
    "text": "This is the second part of lecture for Feb 11. Slides are here\nWhat is a model, and why would we use one? This page introduces ideas with two models: Ordinary Least Squares and logistic regression.\nAt a high level, a model is a tool to share information across units with different \\(\\vec{X}\\) values when estimating subgroup summaries, such as the conditional mean \\(\\text{E}(Y\\mid\\vec{X} = \\vec{x})\\) within the subgroup taking predictor value \\(\\vec{X} = \\vec{x}\\). By assuming that this conditional mean follows a particular shape defined by a small number of parameters, models can yield better predictions than the sample subgroup means. The advantages of models are particularly apparent when there aren’t very many units observed in each subgroup.\nLater in the course, we will expand our conception of models to include flexible statistical learning procedures. For now, we will focus on two models from classical statistics.",
    "crumbs": [
      " ",
      "Inference with Models",
      "What is a model?"
    ]
  },
  {
    "objectID": "topics/what_is_a_model.html#a-simple-example",
    "href": "topics/what_is_a_model.html#a-simple-example",
    "title": "What is a model?",
    "section": "A simple example",
    "text": "A simple example\nAs an example, we continue to use the data on baseball salaries, with a small twist. The file baseball_population.csv contains the following variables\n\npopulation &lt;- read_csv(\"https://soc114.github.io/data/baseball_population.csv\")\n\n\nplayer is the player name\nteam is the team name\nsalary is the 2023 salary\nteam_past_record is the 2022 proportion of games won by that team\nteam_past_salary is the 2022 mean salary in that team\n\nOur goal: using a sample, estimate the mean salary of all Dodger players in 2023. Because we have the population, we know the true mean is $6.23m. We will imagine that we don’t know this number. Instead of having the full population, we will imagine we have\n\ninformation on predictors for all players: position, team, team past record\ninformation on salary for a random sample of 5 players per team\n\nOur estimation task will be made difficult by a lack of data: we will work with a sample containing many teams (30), and few players per team (5). We will use statistical learning strategies to pool information from those other teams’ players to help us make a better estimate of the Dodger mean salary.\nOur predictor will be the team_past_salary from the previous year. We assume that a team’s past salary in 2022 tells us something about their mean salary in 2023.\nFor illustration, draw a sample of 5 players per team\n\nsample &lt;- population |&gt;\n  group_by(team) |&gt;\n  sample_n(5) |&gt;\n  ungroup()\n\nConstruct a tibble with the observations to be predicted: the Dodgers.\n\nto_predict &lt;- population |&gt;\n  filter(team == \"L.A. Dodgers\")",
    "crumbs": [
      " ",
      "Inference with Models",
      "What is a model?"
    ]
  },
  {
    "objectID": "topics/what_is_a_model.html#ordinary-least-squares",
    "href": "topics/what_is_a_model.html#ordinary-least-squares",
    "title": "What is a model?",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nWe could model salary next year as a linear function of team past salary by Ordinary Least Squares. In math, OLS produces a prediction \\[\\hat{Y}_i = \\hat\\alpha + \\hat\\beta X_i\\] with \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) chosen to minimize the sum of squared errors, \\(\\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2\\). Visually, it minimizes all the line segments below.\n\n\n\n\n\n\n\n\n\nHere is how to estimate an OLS model using R.\n\nmodel &lt;- lm(salary ~ team_past_salary, data = sample)\n\nThen we could predict the mean salary for the Dodgers.\n\npredicted &lt;- to_predict |&gt;\n  mutate(predicted = predict(model, newdata = to_predict))\n\nWe would report the mean predicted salary for the Dodgers.\n\npredicted |&gt;\n  summarize(estimated_Dodger_mean = mean(predicted))\n\n# A tibble: 1 × 1\n  estimated_Dodger_mean\n                  &lt;dbl&gt;\n1              7224025.\n\n\nOur model-based estimate compares to the true population mean of $6.23m.",
    "crumbs": [
      " ",
      "Inference with Models",
      "What is a model?"
    ]
  },
  {
    "objectID": "topics/what_is_a_model.html#logistic-regression",
    "href": "topics/what_is_a_model.html#logistic-regression",
    "title": "What is a model?",
    "section": "Logistic regression",
    "text": "Logistic regression\nSometimes the outcome is binary (taking the values {0,1} or {FALSE,TRUE}). One can model binary outcomes with linear regression, but sometimes the predicted values are negative or greater than 1. As an example, suppose we model the probability that a player is a Designated Hitter (position == \"DH\") as a linear function of player salary. For illustration, we do this on the full population.\n\nols_binary_outcome &lt;- lm(\n  position == \"C\" ~ salary,\n  data = population\n)\n\nCatchers tend to have low salaries, so the probability of being a catcher declines as player salary rises. But the linear model carries this trend perhaps further than it ought to: the estimated probability of being a catcher for a player making $40 million is -2%! This prediction doesn’t make a lot of sense.\n\n\n\n\n\n\n\n\n\nLogistic regression is simialr to OLS, except that it uses a nonlinear function (the logistic function) to convert between coefficients that can take any negative or positive values and predictions that always fall in the [0,1] interval.\n\n\n\n\n\n\n\n\n\nMathematically, logistic regression replaces \\(\\text{E}(Y\\mid\\vec{X})\\) on the left side of the equation with the logistic function.\n\\[\n\\underbrace{\\log\\left(\\frac{\\text{P}(Y\\mid\\vec{X})}{1 - \\text{P}(Y\\mid\\vec{X})}\\right)}_\\text{Logistic Function} = \\alpha + \\vec{X}'\\vec\\beta\n\\]\nIn our example with the catchers, we can use logistic regression to model the probability of being a catcher using the glm() function. The family = \"binomial\" line tells the function that we want to estimate logistic regression (since “binomial” is a distribution for outcomes drawn at random with a given probability).\n\nlogistic_regression &lt;- glm(\n  position == \"C\" ~ salary,\n  data = population,\n  family = \"binomial\"\n)\n\nWe can predict exactly as with OLS, except that we need to add the type = \"response\" argument to ensure that R transforms the predicted values into the space of predicted probabilities [0,1] instead of the space in which the coefficients are defined (\\(-\\inf,\\inf\\)).\n\n\n\n\n\n\n\n\n\nBelow is code to make a prediction for the L.A. Dodgers.\n\npredicted_logistic &lt;- predict(\n  logistic_regression,\n  newdata = to_predict,\n  type = \"response\"\n)\n\nTo summarize, linear regression and logistic regression both use an assumed model to share information across units with different values of \\(\\vec{X}\\) when estimating \\(\\text{E}(Y\\mid \\vec{X})\\) or \\(\\text{P}(Y = 1\\mid\\vec{X})\\). This is especially useful any time when we do not get to observe many units at each value of \\(\\vec{X}\\).",
    "crumbs": [
      " ",
      "Inference with Models",
      "What is a model?"
    ]
  },
  {
    "objectID": "topics/weights.html",
    "href": "topics/weights.html",
    "title": "Using weights",
    "section": "",
    "text": "When studying population-level inequality, our goal is to draw inference about all units in the population. We want to know about the people in the U.S., not just the people who answer the Current Population Survey. Drawing inference from a sample to a population is most straightforward for a simple random sample: when people are chosen at random with equal probabilities. For simple random samples, the sample average of any variable is an unbiased and consistent estimator of the population average.\nBut the Current Population Survey is not a simple random sample. Neither are most labor force samples! These samples still begin with a sampling frame, but people are chosen with unequal probabilities. We need sample weights to address this fact.\nIn the CPS, a key goal is to estimate unemployment in each state. Every state needs to have enough sample size—even tiny states like Wyoming. In order to make those estimates, the CPS oversamples people who live in small states.\nTo draw good population inference, our analysis must incorporate what we know about how the data were collected. If we ignore the weights, our sample will have too many people from Wyoming and too few people from California. Weights correct for this.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Using weights"
    ]
  },
  {
    "objectID": "topics/weights.html#how-survey-designers-create-weights",
    "href": "topics/weights.html#how-survey-designers-create-weights",
    "title": "Using weights",
    "section": "How survey designers create weights",
    "text": "How survey designers create weights\nTo calculate sampling weight on person \\(i\\), those who design survey samples take the ratio \\[\\text{weight on unit }i = \\frac{1}{\\text{probability of including person }i\\text{ in the sample}}\\] You can think of the sampling weight as the number of population members a given sample member represents. If there are 100 people with a 1% chance of inclusion, then on average 1 of them will be in the sample. That person represents \\(\\frac{1}{.01}=100\\) people.\n\n\n\n\n\n\nExample redux: California and Wyoming\n\n\n\nSuppose Californians are sampled with probability 0.0004. Then each Californian represents 1 / 0.0004 = 2,500 people. Each Californian should receive a weight of 2,500. Working out the same math for Wyoming, each Wyoming resident should receive a weight of 250. The total weight on these two samples will then be proportional to the sizes of these two populations.\n\n\nIn practice, weighting is more complicated: survey administrators adjust weights for differential nonresponse across population subgroups (a method called post-stratification). How to construct weights is beyond the scope of this course, and could be a whole course in itself!",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Using weights"
    ]
  },
  {
    "objectID": "topics/weights.html#point-estimates",
    "href": "topics/weights.html#point-estimates",
    "title": "Using weights",
    "section": "Point estimates",
    "text": "Point estimates\nWhen we download data, we typically download a column of weights. For simplicity, suppose we are given a sample of four people. The weight column tells us how many people in the population each person represents. The employed column tells us whether each person employed.\n\n\n     name weight employed\n1    Luis      4        1\n2 William      1        0\n3   Susan      1        0\n4  Ayesha      4        1\n\n\nIf we take an unweighted mean, we would conclude that only 50% of the population is employed. But with a weighted mean, we would conclude that 80% of the population is employed! This might be the case if the sample was designed to oversample people at a high risk of unemployment.\n\n\n\n\n\n\n\n\n\nEstimator\nMath\nExample\nResult\n\n\n\n\nUnweighted mean\n\\(=\\frac{\\sum_{i=1}^n Y_i}{n}\\)\n\\(=\\frac{1 + 0 + 0 + 1}{4}\\)\n= 50% employed\n\n\nWeighted mean\n\\(=\\frac{\\sum_{i=1}^n w_iY_i}{\\sum_{i=1}^n w_i}\\)\n\\(=\\frac{4*1 + 1*0 + 1*0 + 4*1}{4 + 1 + 1 + 4}\\)\n= 80% employed\n\n\n\nIn R, the weighted.mean(x, w) function will calculate weighted means where x is an argument for the outcome variable and w is an argument for the weight variable.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Using weights"
    ]
  },
  {
    "objectID": "topics/weights.html#standard-errors",
    "href": "topics/weights.html#standard-errors",
    "title": "Using weights",
    "section": "Standard errors",
    "text": "Standard errors\nAs you know from statistics, our sample mean is unlikely to equal the population mean. There is random variation in which people were chosen for inclusion in our sample, and this means that across hypothetical repeated samples we would get different sample means! You likely learned formulas to create a standard errors, which quantifies how much a sample estimator would move around across repeated samples.\nUnfortunately, the formula you learned doesn’t work for complex survey samples! Simple random samples (for which those formulas hold) are actually quite rare. When you face a complex survey sample, those who administer the survey might provide\n\na vector of \\(n\\) weights for making a point estimate\na matrix of \\(n\\times k\\) replicate weights for making standard errors\n\nBy providing \\(k\\) different ways to up- and down-weight various observations, the replicate weights enable you to generate \\(k\\) estimates that vary in a way that mimics how the estimator might vary if applied to different samples from the population. For instance, our employment sample might come with 3 replicate weights.\n\n\n     name weight employed repwt1 repwt2 repwt3\n1    Luis      4        1      3      5      3\n2 William      1        0      1      2      2\n3   Susan      1        0      3      1      1\n4  Ayesha      4        1      5      3      4\n\n\nThe procedure to use replicate weights depends on how they are constructed. Often, it is relatively straightforward:\n\nuse weight to create a point estimate \\(\\hat\\tau\\)\nuse repwt* to generate \\(k\\) replicate estimates \\(\\hat\\tau^*_1,\\dots,\\hat\\tau^*_k\\)\ncalculate the standard error of \\(\\hat\\tau\\) using the replicate estimates \\(\\hat\\tau^*\\). The formula will depend on how the replicate weights were constructed, but it will likely involve the standard deviation of the \\(\\hat\\tau^*\\) multiplied by some factor\nconstruct a confidence interval1 by a normal approximation \\[(\\text{point estimate}) \\pm 1.96 * (\\text{standard error estimate})\\]\n\nIn our concrete example, the point estimate is 80% employed. The replicate estimates are 0.67, 0.73, 0.70. Variation across the replicate estimates tells us something about how the estimate would vary across hypothetical repeated samples from the population.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Using weights"
    ]
  },
  {
    "objectID": "topics/weights.html#computational-strategy-for-replicate-weights",
    "href": "topics/weights.html#computational-strategy-for-replicate-weights",
    "title": "Using weights",
    "section": "Computational strategy for replicate weights",
    "text": "Computational strategy for replicate weights\nUsing replicate weights can be computationally tricky! It becomes much easier if you write an estimator() function. Your function accepts two arguments\n\ndata is the tibble containing the data\nweight_name is the name of a column containing the weight to be used (e.g., “repwt1”)\n\nExample. If our estimator is the weighted mean of employment,\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    summarize(\n      estimate = weighted.mean(\n        x = employed,\n        # extract the weight column\n        w = sim_rep |&gt; pull(weight_name)\n      )\n    ) |&gt; \n    # extract the scalar estimate\n    pull(estimate)\n}\n\nIn the code above, sim_rep |&gt; pull(weight_name) takes the data frame sim_rep and extracts the weight variable that is named weight_name. There are other ways to do this also.\nWe can now apply our estimator to get a point estimate with the main sampling weight,\n\nestimate &lt;- estimator(data = sim_rep, weight_name = \"weight\")\n\nwhich yields the point estimate 0.80. We can use the same function to produce the replicate estimates,\n\nreplicate_estimates &lt;- c(\n  estimator(data = sim_rep, weight_name = \"repwt1\"),\n  estimator(data = sim_rep, weight_name = \"repwt2\"),\n  estimator(data = sim_rep, weight_name = \"repwt3\")\n)\n\nyielding the three estimates: 0.67, 0.73, 0.70. In real data, you will want to apply this in a loop because there may be dozens of replicate weights.\nThe standard error of the estimator will be some function of the replicate estimates, likely involving the standard deviation of the replicate estimates. Check with the data distributor for a formula for your case. Once you estimate the standard error, a 95% confidence interval can be constructed with a Normal approximation, as discussed above.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Using weights"
    ]
  },
  {
    "objectID": "topics/weights.html#application-in-the-cps",
    "href": "topics/weights.html#application-in-the-cps",
    "title": "Using weights",
    "section": "Application in the CPS",
    "text": "Application in the CPS\nStarting in 2005, the CPS-ASEC samples include 160 replicate weights. If you download replicate weights for many years, the file size will be enormous. We illustrate the use of replicate weights with a question that can be explored with only one year of data: among 25-year olds in 2023, how did the proportion holding four-year college degrees differ across those identifying as male and female?\nWe first load some packages, including the foreach package which will be helpful when looping through replicate weights.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(foreach)\n\nTo answer our research question, we download 2023 CPS-ASEC data including the variables sex, educ, age, the weight variable asecwt, and the replicate weights repwtp*.\n\ncps_data &lt;- read_dta(\"../data_raw/cps_00079.dta\")\n\nWe then define an estimator to use with these data. It accepts a tibble data and a character weight_name identifying the name of the weight variable, and it returns a tibble with two columns: sex and estimate for the estimated proportion with a four-year degree.\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    # Define focal_weight to hold the selected weight\n    mutate(focal_weight = data |&gt; pull(weight_name)) |&gt;\n    # Restrict to those age 25+\n    filter(age &gt;= 25) |&gt;\n    # Restrict to valid reports of education\n    filter(educ &gt; 1 & educ &lt; 999) |&gt;\n    # Define a binary outcome: a four-year degree\n    mutate(college = educ &gt;= 110) |&gt;\n    # Estimate weighted means by sex\n    group_by(sex) |&gt;\n    summarize(estimate = weighted.mean(\n      x = college,\n      w = focal_weight\n    ))\n}\n\nWe produce a point estimate by applying that estimator with the asecwt.\n\nestimate &lt;- estimator(data = cps_data, weight_name = \"asecwt\")\n\n\n\n# A tibble: 2 × 2\n  sex        estimate\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;\n1 1 [male]      0.369\n2 2 [female]    0.397\n\n\nUsing the foreach package, we apply the estimator 160 times—once with each replicate weight—and use the argument .combine = \"rbind\" to stitch results together by rows.\n\nlibrary(foreach)\nreplicate_estimates &lt;- foreach(r = 1:160, .combine = \"rbind\") %do% {\n  estimator(data = cps_data, weight_name = paste0(\"repwtp\",r))\n}\n\n\n\n# A tibble: 320 × 2\n   sex        estimate\n   &lt;dbl+lbl&gt;     &lt;dbl&gt;\n 1 1 [male]      0.368\n 2 2 [female]    0.396\n 3 1 [male]      0.371\n 4 2 [female]    0.400\n 5 1 [male]      0.371\n 6 2 [female]    0.397\n 7 1 [male]      0.369\n 8 2 [female]    0.397\n 9 1 [male]      0.370\n10 2 [female]    0.398\n# ℹ 310 more rows\n\n\nWe estimate the standard error of our estimator by a formula \\[\\text{StandardError}(\\hat\\tau) = \\sqrt{\\frac{4}{160}\\sum_{r=1}^{160}\\left(\\hat\\tau^*_r - \\hat\\tau\\right)^2}\\] where the formula comes from the survey documentation. We carry out this procedure within groups defined by sex, since we are producing estimate for each sex.\n\nstandard_error &lt;- replicate_estimates |&gt;\n  # Denote replicate estimates as estimate_star\n  rename(estimate_star = estimate) |&gt;\n  # Merge in the point estimate\n  left_join(estimate,\n            by = join_by(sex)) |&gt;\n  # Carry out within groups defined by sex\n  group_by(sex) |&gt;\n  # Apply the formula from survey documentation\n  summarize(standard_error = sqrt(4 / 160 * sum((estimate_star - estimate) ^ 2)))\n\n\n\n# A tibble: 2 × 2\n  sex        standard_error\n  &lt;dbl+lbl&gt;           &lt;dbl&gt;\n1 1 [male]          0.00280\n2 2 [female]        0.00291\n\n\nFinally, we combine everything and construct a 95% confidence interval by a Normal approximation.\n\nresult &lt;- estimate |&gt;\n  left_join(standard_error, by = \"sex\") |&gt;\n  mutate(ci_min = estimate - 1.96 * standard_error,\n         ci_max = estimate + 1.96 * standard_error)\n\n\n\n# A tibble: 2 × 5\n  sex        estimate standard_error ci_min ci_max\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1 [male]      0.369        0.00280  0.364  0.375\n2 2 [female]    0.397        0.00291  0.391  0.403\n\n\nWe use ggplot() to visualize the result.\n\nresult |&gt;\n  mutate(sex = as_factor(sex)) |&gt;\n  ggplot(aes(\n    x = sex, \n    y = estimate,\n    ymin = ci_min, \n    ymax = ci_max,\n    label = scales::percent(estimate)\n  )) +\n  geom_errorbar(width = .2) +\n  geom_label() +\n  scale_x_discrete(\n    name = \"Sex\", \n    labels = str_to_title\n  ) +\n  scale_y_continuous(name = \"Proportion with 4-Year College Degree\") +\n  ggtitle(\n    \"Sex Disparities in College Completion\",\n    subtitle = \"Estimates from the 2023 CPS-ASEC among those age 25+\"\n  )\n\n\n\n\n\n\n\n\nWe conclude that those identifying as female are more likely to hold a college degree. Because we can see the confidence intervals generated using the replicate weights, we are reasonably confident in the statistical precision of our point estimates.",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Using weights"
    ]
  },
  {
    "objectID": "topics/weights.html#footnotes",
    "href": "topics/weights.html#footnotes",
    "title": "Using weights",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf we hypothetically drew many complex survey samples from the population in this way, an interval generated this way would contain the true population mean 95% of the time.↩︎",
    "crumbs": [
      " ",
      "Inference Without Models",
      "Using weights"
    ]
  },
  {
    "objectID": "topics/models_for_causal.html",
    "href": "topics/models_for_causal.html",
    "title": "Models for causal inference",
    "section": "",
    "text": "Here are slides on outcome modeling, a quick review and then on to treatment modeling, and slides (to come) on doubly-robust estimation.\nModels are useful when we need subgroup summaries but we do not observe very many units in each subgroup. This situation is common in causal inference: we assume that \\(\\vec{X}\\) is a sufficient adjustment set so that conditional exchangeability holds, and this allows us to identify the causal quantity \\(\\text{E}(Y^a\\mid \\vec{X} = \\vec{x})\\) by the statistical quantity \\(\\text{E}(Y\\mid A = a, \\vec{X} = \\vec{x})\\). But that empirical quantity—the subgroup mean among those with treatment value \\(a\\) and adjustment set value \\(\\vec{x}\\)—may be the mean of a subgroup that is unpopulated. This is especially true in practice because the adjustment set \\(\\vec{X}\\) is often most plausible when it includes many variables, leading to a curse of dimensionality and small subgroup sample sizes. For this reason, causal inference approaches that adjust for measured variables often require us to estimate the means in many subgroups that are sparsely populated.\nThis page introduces outcome models for causal inference. To run the code on this page, you will need the tidyverse.\nlibrary(tidyverse)",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for causal inference"
    ]
  },
  {
    "objectID": "topics/models_for_causal.html#motivating-example",
    "href": "topics/models_for_causal.html#motivating-example",
    "title": "Models for causal inference",
    "section": "Motivating example",
    "text": "Motivating example\nTo what extent does completing a four-year college degree by age 25 increase the probability of having a spouse or residential partner with a four-year college degree at age 35, among the population of U.S. residents who were ages 12–16 at the end of 1996?\nWe used this example on the Why Model? page and will continue with it here. For those jumping in on this page, here is a refresher.\nThis causal question draws on questions in sociology and demography about assortative mating: the tendency of people with high education, income, or status to form households together1. One reason to care about assortative mating is that it can contribute to inequality across households: if people with high earnings potential form households together, then income inequality across households will be greater than it would be if people formed households randomly.\nOur question is causal: to what extent is the probability of marrying a four-year college graduate higher if one were hypothetically to finish a four-year degree, versus if that same person were hypothetically to not finish a college degree? But in data that exist in the world, we see only one of these two potential outcomes. The people for whom we see the outcome under a college degree are systematically different from those for whom we see the outcome under no degree: college graduates come from families with higher incomes, higher wealth, and higher parental education, for example. All of these factors may directly shape the probability of marrying a college graduate even in the absence of college. Thus, it will be important to adjust for a set of measured confounders, represented by \\(\\vec{X}\\) in our DAG.\n\n\n\n\n\n\n\n\n\nBy adjusting for the variables \\(\\vec{X}\\), we block all non-causal paths between the treatment \\(A\\) and the outcome \\(Y\\) in the DAG. If this DAG is correct, then conditional exchangeability holds with this adjustment set: \\(\\{Y^1,Y^0\\}\\indep A \\mid\\vec{X}\\).\nTo estimate, we use data from the National Longitudinal Survey of Youth 1997, a probability sample of U.S. resident children who were ages 12–16 on Dec 31, 1996. The study followed these children and interviewed them every year through 2011 and then every other year after that.\nWe will analyze a simulated version of these data (nlsy97_simulated.csv), which you can access with this line of code.\n\ndata &lt;- read_csv(\"https://soc114.github.io/data/nlsy97_simulated.csv\")\n\n\n\n\n\n\n\nExpand to learn how to get the actual data\n\n\n\n\n\nTo access the actual data, you would need to register for an account, log in, upload the nlsy97.NLSY97 tagset that identifies our variables, and then download. Unzip the folder and put the contents in a directory on your computer. Then run our code file prepare_nlsy97.R in that folder. This will produce a new file d.RDS, contains the data. You could analyze that file. In the interest of transparency, we wrote the code nlsy97_simulated.R to convert these real data to simulated data that we can share.\n\n\n\nThe data contain several variables\n\nid is an individual identifier for each person\na is the treatment, containing the respondent’s education coded treated if the respondent completed a four-year college degree and untreated if not.\ny is the outcome: TRUE if has a spouse or residential partner at age 35 who holds a college degree, and FALSE if no spouse or partner or if the spouse or partner at age 35 does not have a degree.\nThere are several pre-treatment variables\n\nsex is coded Female and Male\nrace is race/ethnicity and is coded Hispanic, Non-Hispanic Black, and Non-Hispanic Non-Black.\nmom_educ is the respondent’s mother’s education as reported in 1997. It takes the value No mom if the child had no residential mother in 1997, and otherwise is coded with her education: &lt; HS, High school, Some college, or College.\ndad_educ is the respondent’s father’s education as reported in 1997. It takes the value No dad if the child had no residential father in 1997, and otherwise is coded with his education: &lt; HS, High school, Some college, or College.\nlog_parent_income is the log of gross household income in 1997\nlog_parent_wealth is the log of household net worth in 1997\ntest_percentile is the respondent’s percentile score on a test of math and verbal skills administered in 1999 (the Armed Services Vocational Aptitude Battery).\n\n\nWhen values are missing, we have replcaed them with predicted values. In the simulated data, no row represents a real person because values have been drawn randomly from a probability distribution designed to mimic what exists in the real data. As discussed above, we did this in order to share the file with you by a download on this website.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for causal inference"
    ]
  },
  {
    "objectID": "topics/models_for_causal.html#outcome-modeling",
    "href": "topics/models_for_causal.html#outcome-modeling",
    "title": "Models for causal inference",
    "section": "Outcome modeling",
    "text": "Outcome modeling\nBecause the causal effect of A on Y is identified by adjusting for the confounders, we can estimate by outcome modeling. There are three general steps.\n\nModel \\(E(Y\\mid A, \\vec{X})\\), the conditional mean of \\(Y\\) given the treatment and confounders\nPredict potential outcomes\n\nset A = 1 for every unit. Predict \\(Y^1\\)\nset A = 0 for every unit. Predict \\(Y^0\\)\n\nAggregate to the average causal effect\n\n\n1) Model factual outcomes\nThe code below uses Ordinary Least Squares to estimate an outcome model.\n\noutcome_model &lt;- lm(\n  y ~ a * (\n    sex + race + mom_educ + dad_educ + log_parent_income +\n      log_parent_wealth + test_percentile\n  ),\n  data = data\n)\n\nThe lm() function estimates a linear model, which is stored in the model object. The first argument is the model formula, which defines the function by which we model the conditional mean of the outcome given the predictors. The second argument is the data we use to learn the model.\nWhy did we choose this model formula? You can actually choose any model formula, but there are some reasons we chose this one. In our model formula, we begin with the treatment a and then we interact this treatment with an additive function of all confounders a * (...). This is equivalent to fitting two models: an additive OLS model for \\(Y^\\text{treated}\\) and an additive OLS model for \\(Y^\\text{untreated}\\), which is a desirable thing to do when we think the effect of college may differ for people with different values on the adjustment set. This type of model was proposed by Lin (2013) and is also known as a t-learner (Kunzel et al. 2019) because it is equivalent to estimating two separate regression models of outcome on confounder: one for the treated group and one for the untreated group. For a recent discussion of its advantages, see Hazlett & Shinkre (2024).\nThe model has a lot of terms! You can see them with summary(model). Thankfully, we won’t interpret any of them. We will just use the model as a tool to predict potential outcomes.\n\n\n2) Predict potential outcomes\nThe code below predicts the conditional average potential outcome under treatment and control at the confounder values of each observation.\nFirst, we create data with a set to the value treated for everyone.\n\ndata_if_treated &lt;- data |&gt;\n  mutate(a = \"treated\")\n\n\n\n# A tibble: 7,771 × 10\n     id a       y     sex   race             mom_educ dad_educ log_parent_income\n  &lt;dbl&gt; &lt;chr&gt;   &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1     1 treated TRUE  Male  Non-Hispanic No… &lt; HS     &lt; HS                  8.55\n2     2 treated FALSE Male  Non-Hispanic No… Some co… College              31.8 \n3     3 treated FALSE Male  Non-Hispanic Bl… College  High sc…             14.1 \n# ℹ 7,768 more rows\n# ℹ 2 more variables: log_parent_wealth &lt;dbl&gt;, test_percentile &lt;dbl&gt;\n\n\nThen, we create data with a set to the value untreated for everyone.\n\ndata_if_untreated &lt;- data |&gt;\n  mutate(a = \"untreated\")\n\n\n\n# A tibble: 7,771 × 10\n     id a         y     sex   race           mom_educ dad_educ log_parent_income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1     1 untreated TRUE  Male  Non-Hispanic … &lt; HS     &lt; HS                  8.55\n2     2 untreated FALSE Male  Non-Hispanic … Some co… College              31.8 \n3     3 untreated FALSE Male  Non-Hispanic … College  High sc…             14.1 \n# ℹ 7,768 more rows\n# ℹ 2 more variables: log_parent_wealth &lt;dbl&gt;, test_percentile &lt;dbl&gt;\n\n\nWe use our outcome model to predict the conditional mean of the potential outcome under each scenario.\n\npredicted_outcomes &lt;- data |&gt;\n  mutate(\n    y1_predicted = predict(outcome_model, newdata = data_if_treated),\n    y0_predicted = predict(outcome_model, newdata = data_if_untreated),\n    effect_predicted = y1_predicted - y0_predicted\n  ) |&gt;\n  select(id, a, y, y1_predicted, y0_predicted, effect_predicted)\n\n\n\n# A tibble: 7,771 × 6\n     id a         y     y1_predicted y0_predicted effect_predicted\n  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;        &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;\n1     1 treated   TRUE         0.507        0.318            0.188\n2     2 treated   FALSE        0.742        0.505            0.237\n3     3 untreated FALSE        0.391        0.159            0.232\n# ℹ 7,768 more rows\n\n\nIn the code above, the function call predict(model, newdata = data_if_treated) uses the model object to make predictions for the data in data_if_treated, which contains each person coded with the treatment set to treated. The predicted values y1_predicted are predictions \\(\\hat{Y}^\\text{treated}\\) of the potential outcome under a four-year college degree. Likewise, the function call predict(model, newdata = data_if_untreated) predicts the outcomes under no college degree. The effect_predicted variable contains the predicted causal effect at the adjustment set values of each person in the data.\n\n\n3) Aggregate\nThe final step is to aggregate to an average causal effect estimate.\n\noutcome_model_estimate &lt;- predicted_outcomes |&gt;\n  select(y1_predicted, y0_predicted, effect_predicted) |&gt;\n  summarize_all(.funs = mean)\n\n\n\n# A tibble: 1 × 3\n  y1_predicted y0_predicted effect_predicted\n         &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;\n1        0.432        0.160            0.272\n\n\nWe estimate that completing college increases the probability of having a college-educated by 0.272, from 0.16 to 0.432. This causal conclusion relies both on our causal assumptions (the DAG) and our statistical assumptions (the chosen model).",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for causal inference"
    ]
  },
  {
    "objectID": "topics/models_for_causal.html#treatment-modeling",
    "href": "topics/models_for_causal.html#treatment-modeling",
    "title": "Models for causal inference",
    "section": "Treatment modeling",
    "text": "Treatment modeling\nInstead of modeling the outcome, another way of using models for causal inference is to model the probability of treatment assignment. This approach is more analogous to sampling from a population.\nIn a probability sample, we observe the outcome \\(Y_i\\) for any sampled unit \\((S_i=1)\\) which is seen with some probability of sampling, \\(P(S=1\\mid\\vec{X} = \\vec{x}_i)\\) that may differ across subgroups with different values of some variables \\(\\vec{X}\\). As discussed in population sampling, the sampling weight is the inverse of these probabilities. A person who is sampled with a 20% probability represents 1 / .2 = 5 people in the population (the other 4 being unsampled).\nIn a conditionally randomized experiment, we observe the outcome under treatment \\(Y_i^1\\) for any treated unit \\(A_i=1\\), which might be assigned with some probability \\(P(A_i=1\\mid\\vec{X} = \\vec{x}_i)\\) that differs across subgroups defined by an adjustment set \\(\\vec{X}\\). In a conditionally randomized experiment, these probabilities are known and the overall expected outcome under treatment \\(\\E(Y^1)\\) can be estimated by the average of the observed outcomes under treatment, weighted by the inverse probability of being treated. A treated unit who had a 20% probability of being treated represents 1 / .2 = 5 people (the other 4 being untreated).\nIn an observational study, we don’t know the probability of being treated given the variables in our sufficient adjustment set. We need to model that probability. There are three general steps.\n\nModel treatment probabilities given an adjustment set\nConstruct a weight for each unit\nEstimate by weighted means within each treatment group\n\n\n1) Model treatment probabilities\nOne way to model the probability of treatment is with logistic regression. If logistic regression is new to you, see the bottom of What is a model?.\n\\[\n\\log\\left(\\frac{P(A = 1 \\mid\\vec{X})}{1-P(A = 1\\mid\\vec{X})}\\right) = \\alpha + \\vec{X}'\\vec\\beta\n\\]\n\ntreatment_model &lt;- glm(\n  I(a == \"treated\") ~ sex + race + mom_educ + dad_educ + log_parent_income +\n    log_parent_wealth + test_percentile,\n  family = binomial,\n  data = data\n)\n\nFor every unit, we can then predict the probability of being treated given the adjustment set.\n\npredicted_treatment_probabilities &lt;- data |&gt;\n  mutate(p_treated = predict(treatment_model, type = \"response\")) |&gt;\n  select(id, a, y, p_treated)\n\n\n\n# A tibble: 7,771 × 4\n     id a         y     p_treated\n  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;     &lt;dbl&gt;\n1     1 treated   TRUE      0.236\n2     2 treated   FALSE     0.860\n3     3 untreated FALSE     0.133\n# ℹ 7,768 more rows\n\n\nThe type = \"response\" argument is essential, because this tells R to predict the probability of treatment instead of the log odds of treatment.\n\n\n2) Construct weights\nFor each unit, we can construct a weight that is the inverse probability of that unit’s treatment assignment. Recall that if a unit is treated and had a 0.2 probability of treatment, then we could think of this unit as representing 1 / 0.2 = 5 units: itself and 4 others like it who were not treated. The weight on each unit is the inverse probability of the treatment value that happened for that unit.\n\\[\nw_i = \\begin{cases}\n\\frac{1}{\\P(A = 1\\mid \\vec{X} = \\vec{x}_i)} &\\text{if treated} \\\\\n\\frac{1}{1 - \\P(A = 1\\mid \\vec{X} = \\vec{x}_i)} &\\text{if untreated}\n\\end{cases}\n\\]\nIn code, we can use case_when() to assign this weight as 1 / p_treated for treated units and 1 / (1 - p_treated) for untreated units.\n\ninverse_probability_weights &lt;- predicted_treatment_probabilities |&gt;\n  mutate(\n    weight = case_when(\n      a == \"treated\" ~ 1 / p_treated,\n      a == \"untreated\" ~ 1 / (1 - p_treated)\n    )\n  )\n\n\n\n# A tibble: 7,771 × 5\n     id a         y     p_treated weight\n  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     1 treated   TRUE      0.236   4.24\n2     2 treated   FALSE     0.860   1.16\n3     3 untreated FALSE     0.133   1.15\n# ℹ 7,768 more rows\n\n\n\n\n3) Estimate by weighted means\nFinally, we use the weights to take the treated units and draw inference about what would happen to all units if they were hypothetically treated, and to use the untreated units and draw inference about what would happen to all units if they were hypothetically untreated.\n\ninverse_probability_weights |&gt;\n  # Within each treatment group\n  group_by(a) |&gt;\n  # Take the mean weighted by inverse probability of treatment weights\n  summarize(estimate = weighted.mean(y, w = weight)) |&gt;\n  # Pivot wider and difference to estimate the effect\n  pivot_wider(names_from = a, values_from = estimate, names_prefix = \"if_\") |&gt;\n  mutate(effect = if_treated - if_untreated)\n\n# A tibble: 1 × 3\n  if_treated if_untreated effect\n       &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1      0.425        0.167  0.259",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for causal inference"
    ]
  },
  {
    "objectID": "topics/models_for_causal.html#doubly-robust-estimation",
    "href": "topics/models_for_causal.html#doubly-robust-estimation",
    "title": "Models for causal inference",
    "section": "Doubly-robust estimation",
    "text": "Doubly-robust estimation\nWe don’t have to constrain ourselves to outcome modeling or treatment modeling. We can also use both together.\n\nModel outcomes and produce an initial ATE estimate\nModel treatment probabilities and produce inverse probability weights\nEstimate the weighted average error of your outcome model\n\nFor each unit, calculate the error \\(Y-\\hat{Y}\\)\nEach unit represents a number of units corresponding to its inverse probability weight\nEstimate the population-average error by the weighted mean of errors, within each treatment group\n\nImprove estimate (1) by subtracting the average error (3)\n\nThis estimator has some properties that make it superior to outcome or treatment modeling alone, as we will discuss at the end of this section.\n\n1) Model outcomes and produce an initial ATE estimate.\nWe already did this above! Our predictions are stored in an object already.\n\npredicted_outcomes |&gt; print(n = 3)\n\n# A tibble: 7,771 × 6\n     id a         y     y1_predicted y0_predicted effect_predicted\n  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;        &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;\n1     1 treated   TRUE         0.507        0.318            0.188\n2     2 treated   FALSE        0.742        0.505            0.237\n3     3 untreated FALSE        0.391        0.159            0.232\n# ℹ 7,768 more rows\n\n\n\n\n2) Model treatments to create weights\nWe already did this above! Our weights are stored in an object already.\n\ninverse_probability_weights |&gt; print(n = 3)\n\n# A tibble: 7,771 × 5\n     id a         y     p_treated weight\n  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     1 treated   TRUE      0.236   4.24\n2     2 treated   FALSE     0.860   1.16\n3     3 untreated FALSE     0.133   1.15\n# ℹ 7,768 more rows\n\n\n\n\n3) Estimate the weighted average error\nFor each unit, we can calculate the error as the difference between the actual outcome \\(Y\\) and the predicted outcome \\(\\hat{Y}\\) under the treatment value that actually happened for that unit.\n\nerrors &lt;- predicted_outcomes |&gt;\n  mutate(\n    error = case_when(\n      a == \"treated\" ~ y1_predicted - y,\n      a == \"untreated\" ~ y0_predicted - y\n    )\n  )\n\n\n\n# A tibble: 7,771 × 7\n     id a         y     y1_predicted y0_predicted effect_predicted  error\n  &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;        &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1     1 treated   TRUE         0.507        0.318            0.188 -0.493\n2     2 treated   FALSE        0.742        0.505            0.237  0.742\n3     3 untreated FALSE        0.391        0.159            0.232  0.159\n# ℹ 7,768 more rows\n\n\nWe then merge our errors with our weights, so that we can see how many total units each error should represent.\n\nerrors_with_weight &lt;- errors |&gt;\n  select(id, a, error) |&gt;\n  left_join(\n    inverse_probability_weights |&gt; select(id, p_treated, weight), \n    by = join_by(id)\n  )\n\n\n\n# A tibble: 7,771 × 5\n     id a          error p_treated weight\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     1 treated   -0.493     0.236   4.24\n2     2 treated    0.742     0.860   1.16\n3     3 untreated  0.159     0.133   1.15\n# ℹ 7,768 more rows\n\n\nAs a concrete example, the error when predicting the first person’s outcome was -0.493. This person’s treatment was treated, and that treatment occurred with treatment probability 0.236. Some units like this unit got treated, and others didn’t. When we take a weighted average within treatment groups to estimate the average over all people, this person’s error stands in for the errors of 4.244 units in total.\nWith our inverse probability weights, we can take the weighted average error within each treatment group as an estimate of the error that would persist if we hypothetically applied our model to all the \\(Y^1\\) values and all the \\(Y^0\\) values (even the ones we didn’t see).\n\nweighted_average_error &lt;- errors_with_weight |&gt;\n  group_by(a) |&gt;\n  summarize(average_outcome_error = weighted.mean(error, w = weight)) |&gt;\n  print()\n\n# A tibble: 2 × 2\n  a         average_outcome_error\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 treated                 0.00960\n2 untreated              -0.00416\n\n\nIn this case our model was very good—the weighted average errors are nearly 0! The weighted average error of (outcome under treatment) - (outcome under control) is calculated below.\n\nweighted_average_effect_error &lt;- weighted_average_error |&gt;\n  pivot_wider(\n    names_from = a, \n    values_from = average_outcome_error, \n    names_prefix = \"average_error_\"\n  ) |&gt;\n  mutate(effect_error = average_error_treated - average_error_untreated) |&gt;\n  pull(effect_error)\n\nWe estimate that our outcome model was mis-specified: we estimate that our outcome model estimate will be 0.014 away from the truth. We can improve our estimate by subtracting the estimated error from the original estimate.\n\\[\n\\text{Updated Estimate} = \\text{Outcome Model Estimate} - \\text{Estimated Error}\n\\]\n\nupdated_estimate &lt;- outcome_model_estimate |&gt;\n  mutate(estimated_error = weighted_average_effect_error) |&gt;\n  mutate(updated_estimate = effect_predicted - estimated_error) |&gt;\n  print()\n\n# A tibble: 1 × 5\n  y1_predicted y0_predicted effect_predicted estimated_error updated_estimate\n         &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1        0.432        0.160            0.272          0.0138            0.258\n\n\n\n\nWhy double robustness?\nThe doubly-robust estimator has a desirable property. We would like it to be the case that our estimator of the average causal effect is consistent: as the sample size grows to infinity, the estimator converges to the true average causal effect. Let \\(\\hat{f}()\\) be the estimated outcome model and \\(\\hat{m}()\\) be the estimated treatment model. The doubly-robust estimator is consistent for the average causal effect if either\n\nThe outcome model is consistent for the truth: \\(\\hat{f}(a,\\vec{x})\\rightarrow \\text{E}(Y\\mid A = a, \\vec{X} = \\vec{x})\\) for all values \\(a\\) and \\(\\vec{x}\\) OR\nThe treatment model is consistent for the truth: \\(\\hat{m}(\\vec{x})\\rightarrow \\text{P}(A = 1\\mid \\vec{X} = \\vec{x})\\) where the \\(\\rightarrow\\) indicates asymptotic convergence as the sample size grows.\n\nOur estimator is good if either (1) or (2) is true! When we aren’t sure how to specify the form of our model, it is good to have two chances.\nWhen both estimators are consistent, the doubly-robust estimator brings additional advantages such as a faster rate of convergence toward the truth as the sample size grows. For many statistical reasons, we should prefer the doubly robust estimator.\nA reason to choose outcome or treatment modeling on its own is that each of these alone may be easier to implement and explain to readers than the doubly-robust estimator. When presenting complex statistical results, being able to explain the procedure to your audience is an important consideration.",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for causal inference"
    ]
  },
  {
    "objectID": "topics/models_for_causal.html#concluding-thoughts",
    "href": "topics/models_for_causal.html#concluding-thoughts",
    "title": "Models for causal inference",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\nOutcome modeling is a powerful strategy because it bridges nonparametric causal identification to longstanding strategies where outcomes are modeled by parametric regression.\nInverse probability of treatment weighting is a powerful strategy because it bridges nonparametric causal identification to longstanding strategies from survey sampling where units from a population are sampled with known probabilities of inclusion. The analogy is that outcomes under treatment are sampled with estimated inclusion probabilities (the probability of treatment). Just as in a population sample we would need to think carefully about the probability of sampling, treatment modeling encourages us to model the probability of receiving the observed treatment.\nDoubly robust estimation brings the two together for an estimator that is statistically preferable, albeit conceptually more complicated!",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for causal inference"
    ]
  },
  {
    "objectID": "topics/models_for_causal.html#footnotes",
    "href": "topics/models_for_causal.html#footnotes",
    "title": "Models for causal inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor reviews, see Mare 1991 and Schwartz 2013.↩︎",
    "crumbs": [
      " ",
      "Inference with Models",
      "Models for causal inference"
    ]
  }
]