{
  "hash": "f8d4f20ddf1772249ffdf337a56fc75a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Trees\"\nformat: html\n---\n\n::: {.cell}\n\n:::\n\n\n\n\nRegression models perform well when the the response surface $E(Y\\mid\\vec{X})$ follows a line (or some other assumed shape). In some settings, however, the response surface may be more complex. There may be nonlinearities and interaction terms that the researcher may not know about in advance. In these settings, one might desire an estimator that adaptively learns the functional form from the data. Trees are one such approach.\n\n## A simulation to illustrate trees\n\nAs an example, the figure below presents some hypothetical data with a binary predictor $Z$, a numeric predictor $X$, and a numeric outcome $Y$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntrue_conditional_mean <- tibble(z = F, x = seq(0,1,.001)) |>\n  bind_rows(tibble(z = T, x = seq(0,1,.001))) |>\n  mutate(mu = z * plogis(10 * (x - .5)))\nsimulate <- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |>\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |>\n    mutate(mu = z * plogis(10 * (x - .5))) |>\n    slice_sample(n = sample_size, replace = T) |>\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data <- simulate(1000)\np_no_points <- true_conditional_mean |>\n  ggplot(aes(x = x, color = z, y = mu)) +\n  geom_line(linetype = \"dashed\", size = 1.2) +\n  labs(\n    x = \"Numeric Predictor X\",\n    y = \"Numeric Outcome Y\",\n    color = \"Binary Predictor Z\"\n  ) +\n  theme_bw()\np <- p_no_points +\n  geom_point(data = simulated_data, aes(y = y), size = .2, alpha = .3)\np\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nIf we tried to approximate these conditional means with an additive linear model,\n$$\\hat{E}_\\text{Linear}(Y\\mid X,Z) = \\hat\\alpha + \\hat\\beta X + \\hat\\gamma Z$$\nthen the model approximation error would be very large.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbest_linear_fit <- lm(mu ~ x + z, data = true_conditional_mean)\np +\n  geom_line(\n    data = true_conditional_mean |>\n      mutate(mu = predict(best_linear_fit))\n  ) +\n  theme_bw() +\n  ggtitle(\"An additive linear model (solid lines) poorly approximates\\nthe true conditional mean function (dashed lines)\")\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n## How a tree works\n\nA regression tree begins from a radically different place than regression. Instead of assuming that the response follows some assumed pattern, trees proceed by a much more inductive process: recursive splits.\n\n### Trees repeatedly split the data\n\nWith no model at all, suppose we were to **split** the sample into two subgroups. For example, we might choose to split on $Z$ and say that all units with `z = TRUE` are one subgroup while all units with `z = FALSE` are another subgroup. Or we might split on $X$ and say that all units with `x <= .23` are one subgroup and all units with `x > .23` are another subgroup. After choosing a way to split the dataset into two subgroups, we would then make a prediction rule: for each unit, predict the mean value of all sampled units who fall in their subgroup. This rule would produce only two predicted values: one prediction per resulting subgroup.\n\nIf you were designing an algorithm to predict this way, how would you choose to define the split?\n\nIn regression trees to estimate conditional means, the split is often chosen to minimize the resulting sum of squared prediction errors. Suppose we choose this rule. Suppose we consider splitting on $X$ being above or below each decile of its empirical distribution. Suppose we consider splitting on $Z$ being `FALSE` or `TRUE`. The graph below shows the sum of squared prediction error resulting from each rule.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nx_split_candidates <- quantile(simulated_data$x, seq(.1,.9,.1))\nz_split_candidates <- .5\nby_z <- simulated_data |>\n  group_by(z) |>\n  mutate(yhat = mean(y)) |>\n  ungroup() |>\n  summarize(sum_squared_error = sum((yhat - y) ^ 2))\nby_x <- foreach(x_split = x_split_candidates, .combine = \"rbind\") %do% {\n  simulated_data |>\n    mutate(left = x <= x_split) |>\n    group_by(left) |>\n    mutate(yhat = mean(y)) |>\n    ungroup() |>\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |>\n    mutate(x_split = x_split)\n}\n\nby_x |>\n  mutate(split = \"If Splitting on X\") |>\n  rename(split_value = x_split) |>\n  bind_rows(\n    by_z |>\n      mutate(split = \"If Splitting on Z\") |>\n      mutate(split_value = .5)\n  ) |>\n  ggplot(aes(x = split_value, y = sum_squared_error)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~split) +\n  labs(\n    x = \"Value on Which to Split into Two Subgroups\",\n    y = \"Resulting Sum of Squared Error\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nWith the results above, we would choose to split on $Z$, creating a subpopulation with $Z \\leq .5$ and a subgroup with $Z\\geq .5$. Our prediction function would look like this. Our split very well approximates the true conditional mean function when `Z = FALSE`, but is still a poor approximator when `Z = TRUE`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np +\n  geom_line(\n    data = simulated_data |>\n      group_by(z) |>\n      mutate(mu = mean(y))\n  ) +\n  ggtitle(\"Solid lines represent predicted values\\nafter one split on Z\")\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nWhat if we make a second split? A regression tree repeats the process and considers making a further split within each subpopulation. The graph below shows the sum of squared error in the each subpopulation of `Z` when further split at various candidate values of `X`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Split 2: After splitting by Z, only X remains on which to split\nleft_side <- simulated_data |> filter(!z)\nright_side <- simulated_data |> filter(z)\n\nleft_split_candidates <- quantile(left_side$x, seq(.1,.9,.1))\nright_split_candidates <- quantile(right_side$x, seq(.1,.9,.1))\n\nleft_split_results <- foreach(x_split = left_split_candidates, .combine = \"rbind\") %do% {\n  left_side |>\n    mutate(left = x <= x_split) |>\n    group_by(z,left) |>\n    mutate(yhat = mean(y)) |>\n    ungroup() |>\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |>\n    mutate(x_split = x_split)\n} |>\n  mutate(chosen = sum_squared_error == min(sum_squared_error))\n\nright_split_results <- foreach(x_split = right_split_candidates, .combine = \"rbind\") %do% {\n  right_side |>\n    mutate(left = x <= x_split) |>\n    group_by(z,left) |>\n    mutate(yhat = mean(y)) |>\n    ungroup() |>\n    summarize(sum_squared_error = sum((yhat - y) ^ 2)) |>\n    mutate(x_split = x_split)\n} |>\n  mutate(chosen = sum_squared_error == min(sum_squared_error))\n\nsplit2_results <- left_split_results |> mutate(split1 = \"Among Z = FALSE\") |>\n  bind_rows(right_split_results |> mutate(split1 = \"Among Z = TRUE\"))\n\nsplit2_results |>\n  ggplot(aes(x = x_split, y = sum_squared_error)) +\n  geom_line(color = 'gray') +\n  geom_point(aes(color = chosen)) +\n  scale_color_manual(values = c(\"gray\",\"blue\")) +\n  facet_wrap(~split1) +\n  theme_bw() +\n  labs(\n    x = \"X Value on Which to Split into Two Subgroups\",\n    y = \"Resulting Sum of Squared Error\"\n  )\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\nThe resulting prediction function is a step function that begins to more closely approximate the truth.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsplit2_for_graph <- split2_results |>\n  filter(chosen) |>\n  mutate(z = as.logical(str_remove(split1,\"Among Z = \"))) |>\n  select(z, x_split) |>\n  right_join(simulated_data, by = join_by(z)) |>\n  mutate(x_left = x <= x_split) |>\n  group_by(z, x_left) |>\n  mutate(yhat = mean(y))\n\np +\n  geom_line(\n    data = split2_for_graph,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines represent predicted values\\nafter two splits on (Z,X)\")\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\nHaving made one and then two splits, the figure below shows what happens when each subgroup is the created by 4 sequential splits of the data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(rpart)\nrpart.out <- rpart(\n  y ~ x + z, data = simulated_data, \n  control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4)\n)\np +\n  geom_step(\n    data = true_conditional_mean |>\n      mutate(mu_hat = predict(rpart.out, newdata = true_conditional_mean)),\n    aes(y = mu_hat)\n  ) +\n  ggtitle(\"Prediction from regression tree grown to depth 4\")\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\nThis prediction function is called a **regression tree** because of how it looks when visualized a different way. One begins with a full sample which then \"branches\" into a left and right part, which further \"branch\" off in subsequent splits. The terminal nodes of the tree---subgroups defined by all prior splits---are referred to as \"leaves.\" Below is the prediction function from above, visualized as a tree. This visualization is made possible with the [`rpart.plot`](https://cran.r-project.org/web/packages/rpart.plot) package which we practice further down the page.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(rpart.plot)\nrpart.plot::rpart.plot(rpart.out)\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n## Your turn: Fit a regression tree\n\nUsing the `rpart` package, fit a regression tree like the one above. First, load the package.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\n```\n:::\n\n\n\n\nThen use this code to simulate data.\n<!-- If you are a Stata user, download this [simulated data file](data/simulated_data_for_trees.dta) from the website. -->\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate <- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |>\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |>\n    mutate(mu = z * plogis(10 * (x - .5))) |>\n    slice_sample(n = sample_size, replace = T) |>\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data <- simulate(1000)\n```\n:::\n\n\n\n\nUse the `rpart` function to grow a tree.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.out <- rpart(y ~ x + z, data = simulated_data)\n```\n:::\n\n\n\n\nFinally, we can define a series of predictor values at which to make predictions,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- tibble(z = F, x = seq(0,1,.001)) |>\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n```\n:::\n\n\n\nand then make predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted <- predict(rpart.out, newdata = to_predict)\n```\n:::\n\n\n\nand visualize in a plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict |>\n  mutate(yhat = predicted) |>\n  ggplot(aes(x = x, y = yhat, color = z)) +\n  geom_step()\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\nWhen you succeed, there are a few things you can try:\n\n* Visualize the tree using the `rpart.plot()` function applied to your `rpart.out` object\n* Attempt a regression tree using the [`baseball_population.csv`](https://ilundberg.github.io/soc212b/data/baseball_population.csv) data\n* Try different specifications of the tuning parameters. See the `control` argument of `rpart`, explained at `?rpart.control`. To produce a model with depth 4, we previously used the argument `control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4)`.\n\n## Tuning parameter: Depth\n\nHow deep should one make a tree? Recall that the depth of the tree is the number of sequential splits that define a leaf. The figure below shows relatively shallow trees (depth = 2) and relatively deep trees (depth = 4) learned over repeated samples. What do you notice about performance with each choice?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nestimator <- function(maxdepth) {\n  foreach(rep = 1:3, .combine = \"rbind\") %do% {\n    this_sample <- simulate(100)\n    rpart.out <- rpart(y ~ x + z, data = this_sample, control = rpart.control(minsplit = 2, cp = 0, maxdepth = maxdepth))\n    true_conditional_mean |>\n      mutate(yhat = predict(rpart.out, newdata = true_conditional_mean),\n             maxdepth = maxdepth,\n             rep = rep)\n  }\n}\nresults <- foreach(maxdepth_value = c(2,5), .combine = \"rbind\") %do% estimator(maxdepth = maxdepth_value)\np_no_points +\n  geom_line(\n    data = results |> mutate(maxdepth = case_when(maxdepth == 2 ~ \"Shallow Trees\\nDepth = 2\", maxdepth == 5 ~ \"Deep Trees\\nDepth = 5\")),\n    aes(group = interaction(z,rep), y = yhat)\n  ) +\n  facet_wrap(\n    ~maxdepth\n  )\n```\n\n::: {.cell-output-display}\n![](trees_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\nShallow trees yield predictions that tend to be more biased because the terminal nodes are large. At the far right when `z = TRUE` and `x` is large, the predictions from the shallow trees are systematically lower than the true conditional mean.\n\nDeep trees yield predictions that tend to be high variance because the terminal nodes are small. While the flexibility of deep trees yields predictions that are less biased, the high variance can make deep trees poor predictors.\n\nThe balance between shallow and deep trees can be chosen by various rules of thumb or out-of-sample performance metrics, many of which are built into functions like `rpart`. Another way out is to move beyond trees to forests, which involve a simple extension that yields substantial improvements in performance.\n\n## Trees for causal inference\n\nTBD\n\n## What to read\n\nTo read more on trees, see Ch 8.4 of [Efron \\& Hastie (2016)](https://hastie.su.domains/CASI/).",
    "supporting": [
      "trees_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}