{
  "hash": "874555a5838365f458eb18b46b6b2505",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression\"\nsubtitle: \"UCLA Soc 114\"\nformat:\n  revealjs: default # For the web slides\n  #beamer: default      # To generate the PDF version\nexecute:\n  echo: true\nresources: \n   - lec06_logistic_regression.pdf\n---\n\n\n::: {.cell}\n\n:::\n\n\n## Logistic regression: Learning goals\n\nSome things you may know\n\n- Logistic regression is good for binary outcomes\n- Coefficients are hard to interpret\n\nData science ideas\n\n- Predicted values make logistic regression easy to use\n\n## Logistic regression\n\n- A type of model for a binary outcome\n     * $Y$ taking the values `{0,1}` or `{FALSE,TRUE}`\n- Modeled as a function of predictor variables $\\vec{X}$\n\n## A data example\n\n[`baseball_population.csv`](../../data/baseball_population.csv)\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopulation <- read_csv(\"https://soc114.github.io/data/baseball_population.csv\")\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 944 × 6\n  player               salary position team    team_past_record team_past_salary\n  <chr>                 <dbl> <chr>    <chr>              <dbl>            <dbl>\n1 Bumgarner, Madison 21882892 LHP      Arizona            0.457         2794887.\n2 Marte, Ketel       11600000 2B       Arizona            0.457         2794887.\n3 Ahmed, Nick        10375000 SS       Arizona            0.457         2794887.\n4 Kelly, Merrill      8500000 RHP      Arizona            0.457         2794887.\n5 Walker, Christian   6500000 1B       Arizona            0.457         2794887.\n# ℹ 939 more rows\n```\n\n\n:::\n:::\n\n\n## A data example\n\n- `player` is the player name\n- `salary` is the 2023 salary\n- `position` is the position played (e.g., `LHP` for left-handed pitcher)\n- `team` is the team name\n- `team_past_record` was the team's win percentage in the previous season\n- `team_past_salary` was the team's average salary in the previous season\n\n## A binary outcome\n\n- You see a player's `salary`\n- Are they a catcher?\n     - `position == \"C\"`\n     \n## Linear probability model\n\nWe can model with `lm()` for a linear fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_binary_outcome <- lm(\n  position == \"C\" ~ salary,\n  data = population\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npopulation |>\n  mutate(yhat = predict(ols_binary_outcome)) |>\n  ggplot(aes(x = salary, y = yhat)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_x_continuous(\n    name = \"Player Salary\",\n    labels = label_currency(scale = 1e-6, suffix = \"m\")\n  ) +\n  scale_y_continuous(\n    name = \"Predicted Probability\\nof Being a Catcher\"\n  ) +\n  ggtitle(\"Modeling a binary outcome with a line\")\n```\n\n::: {.cell-output-display}\n![](lec06_logistic_regression_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## Goal: Avoid illogical predictions\n\nIn OLS, there is a linear predictor\n$$\n\\mu = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 + \\cdots\n$$\nthat can take any numeric value. Possibly $\\mu <0$ or $\\mu > 1$.\n\n## From $\\mu$ to $\\pi$\n\nLogistic regression passes the linear predictor \n$$\\mu = \\beta_0 + X_1\\beta_1 + X_2\\beta_2 + \\cdots$$\nthrough a nonlinear function to force it between 0 and 1.\n\n$$\n\\pi = \\text{logit}^{-1}\\left(\\beta_0 + X\\beta_1\\right) = \\frac{e^{\\beta_0 + X\\beta_1}}{1 + e^{\\beta_0 + X\\beta_1}}\n$$\n\n## From $\\mu$ to $\\pi$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec06_logistic_regression_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n. . .\n\n- At linear predictor 0, what is the predicted probability?\n- At linear predictor 2.5, what is the predicted probability?\n- At linear predictor $\\infty$, what is the predicted probability?\n\n## From $\\pi$ to $\\mu$\n\nYou can also think from $\\pi$ to $\\mu$.\n\n$$\n\\begin{aligned}\n\\text{logit}(\\pi) &= \\mu = \\beta_0 + X\\beta_1 \\\\\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) &= \\mu = \\beta_0 + X\\beta_1\n\\end{aligned}\n$$\n\n## From $\\pi$ to $\\mu$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec06_logistic_regression_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Logistic regression in R\n\nThe `glm()` function (for logistic regression) works exactly like the `lm()` function (for linear regression)\n\n## Logistic regression in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_regression <- glm(\n  position == \"C\" ~ salary,\n  data = population,\n  family = \"binomial\"\n)\n```\n:::\n\n\n- `position == \"C\"` is our outcome: the binary indicator that the `position` variable takes the value `\"C\"`\n- `salary` is a predictor variable\n- `family = \"binomial\"` specifies logistic regression (since \"binomial\" is a distribution for binary outcomes)\n\n## Coefficients: A word of warning\n\nHard to interpret. Not probabilities. Use predicted values instead.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(logistic_regression)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = position == \"C\" ~ salary, family = \"binomial\", \n    data = population)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.268e+00  1.500e-01 -15.126   <2e-16 ***\nsalary      -5.599e-08  2.599e-08  -2.154   0.0312 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 508.94  on 943  degrees of freedom\nResidual deviance: 502.85  on 942  degrees of freedom\nAIC: 506.85\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n## Predicted values\n\nBe sure to use `type = \"response\"` predict probabilities (between 0 and 1) instead of log odds\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(\n  logistic_regression,\n  type = \"response\"\n)\n```\n:::\n\n\n## Predicted values\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npopulation |>\n  mutate(yhat = predict(logistic_regression, type = \"response\")) |>\n  distinct(salary, yhat) |>\n  ggplot(aes(x = salary, y = yhat)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_x_continuous(\n    name = \"Player Salary\",\n    labels = label_currency(scale = 1e-6, suffix = \"m\")\n  ) +\n  scale_y_continuous(\n    name = \"Predicted Probability\\nof Being a Catcher\"\n  ) +\n  ggtitle(\"Modeling a binary outcome with logistic regression\")\n```\n\n::: {.cell-output-display}\n![](lec06_logistic_regression_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n## Predicted values with `newdata`\n\n- New player: salary is \\$5 million.\n- What is the probability that this player is a catcher?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- tibble(salary = 5e6)\n```\n:::\n\n\n. . .\n\nMake the predicted value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(\n  logistic_regression,\n  newdata = to_predict,\n  type = \"response\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1 \n0.07255671 \n```\n\n\n:::\n:::\n\n\n## Linear and logistic regression\n\nWhat is the same? What is different?\n\n. . .\n\n* Same\n     * Takes $X$ and predicts $Y$\n     * Involves $\\beta_0 + \\beta_1 X$\n* Different\n     * Logistic regression predicts a probability $0\\leq\\pi\\leq 1$\n\n$$\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + X_1\\beta_1\n$$\n\n## Logistic regression: Learning goals\n\nSome things you may know\n\n- Logistic regression is good for binary outcomes\n- Coefficients are hard to interpret\n\nData science ideas\n\n- Predicted values make logistic regression easy to use\n\n",
    "supporting": [
      "lec06_logistic_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}