{
  "hash": "f920a8423c8e1933dc0dd614b5da8ffe",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Matching\"\nformat: \n  html:\n    fig-height: 3\n---\n\n> Here are [slides](../slides/matching/matching.pdf) on matching.\n\nMatching is a method for causal inference that is analogous to model-based estimation, but is often easier to explain. Suppose we have a set of 5 units, of whom 2 are treated.\n\n![](../assets/illustrations/matching_visual.png){width=50%}\n\nWe would like to infer that treatment causes higher outcomes, but the units also differ along a confounding variable $L$. How can we infer the average treatment effect for units 1 and 2?\n\nOne way to draw inference is by a model: model $Y^0$ as a function of $L$ among the untreated units, and then use our model to predict for the treated units.\n\nAnother strategy might involve no model at all. We notice that unit 3 is very similar to unit 1 along $L$. Likewise, unit 2 is very similar to unit 5. We could match these units together and use the matches to infer the unobserved potential outcomes.\n\n![](../assets/illustrations/matched_visual.png){width=50%}\n\nWe then estimate the average effect for units 1 and 2 by the difference of\n\n1. The average outcome among treated units: $\\frac{1}{2}(Y_1 + Y_2)$ and\n2. The average outcome among matched controls: $\\frac{1}{2}(Y_3 + Y_5)$\n\n## Matching in math\n\nFormally, let $\\text{match}(i)$ denote the index of the match for unit $i$. Our goal is to estimate the average treatment effect on the treated,\n\n$$\\tau = \\frac{1}{n_1}\\sum_{i:A_i=1}\\left(Y_i^1 - Y_i^0\\right)$$\n\nwhere $n_1$ is the number of treated units and the sum is taken over all units $i$ such that the treatment took the value 1 for those units.\n\nThe fundamental problem of causal inference is that for these units $Y_i^1$ is observed but $Y_i^0$ is not. But for each treated unit $i$, we find an untreated match $j = \\text{match}(i)$ who is very simialr to $i$ but for whom $Y_j^0$ is observed. We then estimate by the mean difference between the treated units and their matched controls.\n\n$$\\hat\\tau_\\text{Matching} = \\frac{1}{n_1}\\sum_{i:A_i=1}\\left(Y_i^1 - Y_{\\text{match}(i)}^0\\right)$$\n\n## Matching vs. regression\n\nWhy should we prefer matching or regression?\n\nWe have already learned a regression solution to this problem: assume a sufficient adjustment set $\\vec{X}$, model the $Y_i^0$ outcomes as a function of $\\vec{X}$ for a set of units who factually were untreated, and use the model to predict what would happen for the treated units if they had been untreated $(\\hat{Y}_i^0)$. Then our estimator of the ATT would be:\n\n$$\\hat\\tau_\\text{Regression} = \\frac{1}{n_1}\\sum_{i:A_i=1}\\left(Y_i^1 - \\hat{\\text{E}}(Y\\mid\\vec{X} = \\vec{x}_i, A = 0)\\right)$$\n\nMatching is actually doing the same thing---we are just using the outcome of unit $j$ as an estimator of $\\hat{\\text{E}}(Y\\mid\\vec{X} = \\vec{x}_i, A = 0)$.\n\nWhy would we then prefer matching? One reason is explainability. A model is easy to explain to social scientists and statisticians who are familiar with models. It isn't as good when you are speaking to policymakers and others who are unfamiliar with models.\n\n## Distances for multivariate matching\n\nSuppose we have treated and untreated units that differ along two confounding variables, $L_1$ and $L_2$.\n\n![](../assets/illustrations/matching_multivariate){width=50%}\n\nWhich control unit should be chosen as the match? With more than one variable, it is not immediately obvious which control points is \"closest\" to the treated point---we first need to define \"closest.\" Which one we would choose requires one to choose a distance metric.\n\n> **Distance metric.** A function $d()$ that takes two vectors $\\vec{x}$ and $\\vec{x}'$ and returns a scalar numeric distance $d(\\vec{x},\\vec{x}')$.\n\n![](../assets/illustrations/matching_multivariate_distances){width=50%}\n\n### Manhattan distance\n\nImagine that the points are places in Manhattan, where the streets are arranged in a grid. The way to travel from `Treated` to `Untreated 1` is by traveling 4 blocks north and then 3 blocks east, for a total distance of 7 units. This distance metric is called Manhattan distance.\n\n> **Manhattan distance.** The distance between two vectors $\\vec{x}$ and $\\vec{x}'$ is the sum of their absolute differences on each element: $$d_\\text{Manhattan}(\\vec{x},\\vec{x}') = \\sum_p \\lvert x_p - x'_p \\rvert $$\n\nBy Manhattan distance, we might determine that the unit (Untreated 2) is closest to (Treated) because its Manhattan distance from the treated unit is 6 instead of 7.\n\n### Euclidean distance\n\nImagine instead that the points are in a field, and you are a crow. The distance that is relevant to you is the most direct line---the distance as the crow flies! This is Euclidean distance.\n\n> **Euclidean distance.** The distance between two vectors $\\vec{x}$ and $\\vec{x}'$ is the square root of the sum of their squared differences on each element: $$d_\\text{Euclidean}(\\vec{x},\\vec{x}') = \\sqrt{\\sum_p \\left( x_p - x'_p \\right)^2} $$\n\nBy Euclidean distance, we would choose a different matched control unit! The unit (Untreated 2) is 6 units away from the (Treated) unit in terms of Euclidean distance, and the unit (Untreated 1) is only 5 units away. By Euclidean distance, the match to choose is (Untreated 1).\n\nThe comparison between Euclidean and Manhattan distances shows that our choice of distance metric can shape who we choose as matches.\n\nIn practice, neither Manhattan nor Euclidean distance is commonly used for matching. Instead, researchers often use Mahalanobis distance, which is a generalization of Euclidean distance that takes into account the variance and covariance of $\\vec{X}$. And an even more common distance metric is propensity score distance, which we discuss next.\n\n### Propensity score distance\n\nA distance metric requires us to map a pair of vectors $(\\vec{x},\\vec{x}')$ into a single-number distance. Thankfully, there is already a way we often map a vector of confounders to a number: predict the probability of treatment.\n\nSuppose we estimate each unit's probability of being treated, $\\text{P}(A = 1\\mid \\vec{X} = \\vec{x}_i)$. We might then define the distance between two units as the distance between their predicted probabilities of being treated.\n\n> **Propensity score distance.** The distance between two vectors $\\vec{x}$ and $\\vec{x}'$ is the squared difference in the probability of treatment under these two vectors. $$d_\\text{PropensityScore}(\\vec{x},\\vec{x}') = \\left(\\text{P}(A = 1\\mid \\vec{X} = \\vec{x}) - \\text{P}(A = 1\\mid \\vec{X} = \\vec{x}')\\right)^2$$\n\nOften, the propensity score $\\text{P}(A = 1\\mid \\vec{X})$ is estimated by a logistic regression model, but one could estimate by any machine learning strategy or nonparametrically if $\\vec{X}$ is discrete.\n\nPropensity score matching is especially intuitive. With propensity score matching, the researcher matches each treated unit to a control unit who had a similar probability of being treated.\n\n## Choices after the distance\n\nAfter you define the distance, there are many additional choices for how to conduct matching! These choices often involve a tradeoff between bias and variance.\n\n### With and without replacement\n\nBelow are two treated and two untreated units who differ on one confounding variable $L$. The left-most treated unit is matched to the left-most control unit. The algorithm then moves to the right-most treated unit: which control unit should serve as its match?\n\n![](../assets/illustrations/matching_replacement.png)\n\nOne argument is that the left-most control unit should again serve as the match---it is clearly the closest control unit. But it has already been used! If we want to enforce that each treated unit gets its own unique untreated match, then we should match to the unit at the right.\n\nThis is the choice between matching with and without replacement.\n\n* With replacement: After an untreated unit is used as a match for one treated unit, it is returned to the pool of untreated units to be considered for future matches.\n* Without replacement: After an untreated unit is used as a match for one treated unit, it is never again used as a match.\n\nMatching with replacement yields the closest possible matches: each treated unit gets paired with the closest control unit that can be found. In this sense, matching with replacement reduces bias.\n\nBut matching with replacement can also produce a high-variance estimator. Suppose you have 50 treated units who all get matched to a single untreated unit---the random chance that included that particular untreated unit in the sample has huge influence on the resulting estimate!\n\nThe choice of with and without replacement has no correct answer; whether one or the other is better will depend on the bias and variance in a particular research setting.\n\n### k:1 matching\n\nShould each treated unit be matched to only one untreated unit, or should we match to more than one untreated unit and take the average? A k:1 matching algorithm matches each treated unit to $k$ untreated units.\n\n![](../assets/illustrations/matching_k1.png)\n\nThe advantage of k:1 matching is a reduction in variance: by averaging over a larger number of untreated units, the resulting estimator will vary less from sample to sample. But the cost of k:1 matching bias: the two closest matches are not generally going to be collectively as close to the treated unit as the single closest match. \n\n### Calipers\n\nSuppose we have a treated unit, and there seem to be no comparable control units. Is there a point at which we give up the search?\n\n![](../assets/illustrations/matching_calipers.png)\n\nIn our illustration, the treated point at the far right is very far from both untreated units. It is not clear that we should try to match this unit. To formalize that it is too far, we might define the black bars as the farthest distance we are willing to look for a match. The width of these bars is known as a caliper.\n\n> **Caliper.** The maximum distance between a treated and untreated unit such that we will consider them possible matches.\n\nIn caliper matching, we would not match the right-most treated unit to any untreated unit. Instead, we would update the estimand to be the average treatment effect on the treated among those within the caliper distance from the control units.\n\nCaliper matching can be good because it avoids bad matches. But caliper matching comes with a cost---it changes the causal estimand to the causal effect in a subgroup who can be hard to explain!\n\n## Regression after matching\n\nAfter matching, there are two estimators we could consider.\n\n1. Mean $Y$ among treated units - mean $Y$ of matched control units\n2. Coefficient on $A$ in a regression of $Y$ on $A$ and $\\vec{X}$ among matches\n\nEstimator (2) is preferable in the sense that the regression model can correct for imperfections in our matching. Despite our best efforts, the matched controls will not be quite equal to the treated units along $\\vec{X}$! Regression can fix this. In this sense, we can think of matching as a preprocessing step before regression.\n\n## Matching in code\n\nThe `MatchIt` package in R carries out all kinds of matching applications. You can do all of the above using `MatchIt`.\n\nHere is one example, using our data from the model-based inference page.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(MatchIt)\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_csv(\"https://soc114.github.io/data/nlsy97_simulated.csv\")\n```\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmatched <- matchit(\n  # A formula for treatment given confounders.\n  # Treatment must be a binary or logical variable.\n  formula = (a == \"treated\") ~ sex + race + mom_educ + dad_educ +\n    log_parent_income + log_parent_wealth + test_percentile,\n  # Data containing variables\n  data = data,\n  # Conduct propensity score matching\n  distance = \"glm\",\n  link = \"logit\",\n  estimand = \"ATT\"\n) |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA `matchit` object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 7688 (original), 2958 (matched)\n - target estimand: ATT\n - covariates: sex, race, mom_educ, dad_educ, log_parent_income, log_parent_wealth, test_percentile\n```\n\n\n:::\n:::\n\n\nWe see that this carried out 1:1 nearest neighbor matching without replacement, with distance estimated by the propensity score estimated with logistic regression. We can extract the resulting matches with the `match.data()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmatches <- match.data(matched)\n```\n:::\n\n\nThen we can estimate by the mean difference across matched treated and control units. In the event of matching with replacement or k:1 matching, it is important to include `weights` since each unit may be used as a match multiple times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmatches |>\n  group_by(a) |>\n  summarize(estimate = weighted.mean(y, w = weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 2\n  a         estimate\n  <chr>        <dbl>\n1 treated      0.518\n2 untreated    0.254\n```\n\n\n:::\n:::\n\n\nAlternatively, we can carry out regression after matching.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_after_matching <- lm(\n  y ~ (a == \"treated\") + sex + race + mom_educ + dad_educ +\n    log_parent_income + log_parent_wealth + test_percentile,\n  data = matches,\n  weights = weights\n)\n```\n:::\n\n\nThen our estimate could be the coefficient on the treatment variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm_after_matching)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ (a == \"treated\") + sex + race + mom_educ + dad_educ + \n    log_parent_income + log_parent_wealth + test_percentile, \n    data = matches, weights = weights)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7731 -0.3592 -0.1666  0.4412  1.2364 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                -0.1667709  0.0904700  -1.843  0.06537 .  \na == \"treated\"TRUE          0.2350511  0.0169415  13.874  < 2e-16 ***\nsexMale                     0.1119653  0.0170433   6.569 5.95e-11 ***\nraceNon-Hispanic Black     -0.1501000  0.0341049  -4.401 1.12e-05 ***\nraceNon-Hispanic Non-Black -0.0232965  0.0275414  -0.846  0.39769    \nmom_educCollege            -0.0421786  0.0442361  -0.953  0.34042    \nmom_educHigh school        -0.1016445  0.0422265  -2.407  0.01614 *  \nmom_educNo mom             -0.1125961  0.0654962  -1.719  0.08570 .  \nmom_educSome college       -0.0691847  0.0427370  -1.619  0.10559    \ndad_educCollege             0.0872352  0.0451752   1.931  0.05357 .  \ndad_educHigh school         0.0010773  0.0448006   0.024  0.98082    \ndad_educNo dad              0.0713750  0.0452732   1.577  0.11501    \ndad_educSome college        0.0662214  0.0449126   1.474  0.14047    \nlog_parent_income           0.0058689  0.0063126   0.930  0.35260    \nlog_parent_wealth           0.0155895  0.0049802   3.130  0.00176 ** \ntest_percentile             0.0029745  0.0004176   7.122 1.33e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4505 on 2942 degrees of freedom\nMultiple R-squared:  0.1482,\tAdjusted R-squared:  0.1438 \nF-statistic: 34.11 on 15 and 2942 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nUsing the regression-after-matching strategy, we estimate that going to college leads to a 0.24 increase in the probability of having a college-educated spouse or residential partner.\n\n## What to read\n\nTo learn more about matching, a good article is:\n\n* Stuart, Elizabeth. 2010. [\"Matching Methods for Causal Inference: A Review and a Look Forward.\"](https://projecteuclid.org/journals/statistical-science/volume-25/issue-1/Matching-Methods-for-Causal-Inference--A-Review-and-a/10.1214/09-STS313.full). Statistical Science 25(1):1-21.\n\nTo see how matching methods have been used in questions of social stratification, see this book and papers cited in it.\n\n* Brand, Jennie E. 2023. [Overcoming the Odds: The Benefits of Completing College for Unlikely Graduates.](https://www.russellsage.org/publications/overcoming-odds) Russell Sage Foundation. Here is a link to read online through the [UCLA Library](https://search.library.ucla.edu/permalink/01UCS_LAL/17p22dp/alma9919490742806531).\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}