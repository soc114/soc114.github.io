{
  "hash": "df7f583629253159656cb93031df2dd9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression\"\nformat: \n  html:\n    fig-height: 3\n---\n\n\n::: {.cell}\n\n:::\n\n\n> Here are slides in [website](../slides/lec05_linear_regression/lec05_linear_regression.qmd) and [pdf](../slides/lec05_linear_regression/lec05_linear_regression.pdf) format.\n\nAs an example, we will work with U.S. adult income by sex (male, female), age (30--50), and year (2010--2019). We will focus on the target population of those working 35+ hours per week for 50+ weeks per year. Data are simulated based on the 2010–2019 American Community Survey (ACS).\n\nThe function below will simulate data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate <- function(n = 100) {\n  read_csv(\"https://ilundberg.github.io/description/assets/truth.csv\") |>\n    slice_sample(n = n, weight_by = weight, replace = T) |>\n    mutate(income = exp(rnorm(n(), meanlog, sdlog))) |>\n    select(year, age, sex, income)\n}\n```\n:::\n\n\nBelow you can see this function in action.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated <- simulate(n = 3e4)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 420 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (5): year, age, meanlog, sdlog, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30,000 × 4\n   year   age sex    income\n  <dbl> <dbl> <chr>   <dbl>\n1  2011    48 female 93676.\n2  2012    38 female 98805.\n3  2013    38 female 52330.\n# ℹ 29,997 more rows\n```\n\n\n:::\n:::\n\n\n## Conditional expectation\n\nA key goal with linear regression is the conditional expectation: the mean of an outcome within a population subgroup.\n\n- **expectation** refers to taking a mean\n- **conditional** refers to within a subgroup\n\nExample: Mean income among females age 47 in 2019 \n\nSuppose we want to estimate that conditional mean in our data. One way is to first create the subgroup and take the mean among people in that subgroup in our sample.\n\n`filter()` restricts our data to cases meeting requirements:\n\n- the `sex` variable equals the value `female`\n- the `age` variable equals the value `47`\n- the `year` variable equals the value `2019`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubgroup <- simulated |>\n  filter(sex == \"female\") |>\n  filter(age == 47) |>\n  filter(year == 2019)\n```\n:::\n\n\n`summarize()` aggregates to the mean\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubgroup |>\n  summarize(conditional_expectation = mean(income))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  conditional_expectation\n                    <dbl>\n1                  71530.\n```\n\n\n:::\n:::\n\n\nOften, we want to study many conditional expectations: the mean outcome in many subgroups. In previous homework, we have seen how `group_by` and `summarize` can yield the mean in many subgroups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated |>\n  group_by(sex, age, year) |>\n  summarize(conditional_expectation = mean(income))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'sex', 'age'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 420 × 4\n# Groups:   sex, age [42]\n  sex      age  year conditional_expectation\n  <chr>  <dbl> <dbl>                   <dbl>\n1 female    30  2010                  45928.\n2 female    30  2011                  43688.\n3 female    30  2012                  42714.\n# ℹ 417 more rows\n```\n\n\n:::\n:::\n\n\nIn math, the **conditional expectation function** is the subgroup mean of $Y$ within a subgroup with the predictor values $\\vec{X} = \\vec{x}$. We use $\\text{E}$ to denote the expectation operator. For simplicity, we will let $f()$ refer to the conditional expectation function, which has input $\\vec{x}$ and outputs a conditional mean among those with $\\vec{X} = \\vec{x}$.\n\n$$\nf(\\vec{x}) = \\text{E}(Y\\mid\\vec{X} = \\vec{x})\n$$\n\nTo learn $f(\\vec{x})$ from data is a central task in statistical learning.\n\n#$ Statistical learning by pooling information\n\nA common problem of statistical inference is that we want to study a subgroup, but there are few cases within the subgroup. For example, female respondents age 47 in 2019 in our simulated data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated |>\n  filter(sex == \"female\") |>\n  filter(year == 2019) |>\n  filter(age == 47)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 67 × 4\n   year   age sex    income\n  <dbl> <dbl> <chr>   <dbl>\n1  2019    47 female 15761.\n2  2019    47 female 32995.\n3  2019    47 female 83967.\n# ℹ 64 more rows\n```\n\n\n:::\n:::\n\n\nFew cases in a subgroup leads to statistical uncertainty about the mean in the subgroup. How can we better estimate this conditional mean?\n\nOne strategy is to **pool information** using a model. We have many female respondents in 2019. The only problem is that few are age 47.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated |>\n  filter(sex == \"female\") |>\n  filter(year == 2019)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,427 × 4\n   year   age sex    income\n  <dbl> <dbl> <chr>   <dbl>\n1  2019    32 female 52130.\n2  2019    46 female 17465.\n3  2019    41 female 66012.\n# ℹ 1,424 more rows\n```\n\n\n:::\n:::\n\n\nWe might think that these other respondents (e.g., those age 46 and 48) are informative about the outcomes of 47-year-olds. There are many ways to pool information. A linear regression model is one strategy that pools information across people of all ages, to estimate a conditional mean at any particular age.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_linear_regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThe model assumes that all of the conditional means fall along a line. Then, it estimates the intercept $\\beta_0$ and slope $\\beta_1$ of this line to best fit those conditional means. Finally, one can use the model to make a prediction at any particular $X$ value.\n\n### Practice question\n\n$$\n\\text{E}(Y\\mid X) = \\beta_0 + \\beta_1 X\n$$\n\nSuppose $\\beta_0 = 5$ and $\\beta_1 = 3$\n\n1. What is the conditional mean when $X = 0$?\n2. What is the conditional mean when $X = 1$?\n3. What is the conditional mean when $X = 2$?\n4. How much does the conditional mean change for each unit increase in $X$?\n\n## Coding a linear model\n\nCoding a linear model in R is easy. First, generate some data (get the `simulate()` function from futher up this page).\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated <- simulate(n = 3e4)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 420 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (5): year, age, meanlog, sdlog, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\nFor our example, restrict to female respondents in 2019.\n\n::: {.cell}\n\n```{.r .cell-code}\nfemale_2019 <- simulated |>\n  filter(sex == \"female\") |>\n  filter(year == 2019)\n```\n:::\n\n\nThen learn a model from the data with the `lm()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(\n  formula = income ~ age, \n  data = female_2019\n)\n```\n:::\n\n\nHere is how that code worked:\n\n- `model` is an object of class `lm` for **l**inear **m**odel\n- `lm()` function creates this object\n- `formula` argument is a model formula\n     - `outcome ~ predictor` is the syntax\n- `data` is a dataset containing `outcome` and `predictor`\n\nWe can look at the learned model with the `summary()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = income ~ age, data = female_2019)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-52689 -29518 -12682  16013 400507 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  47242.6     7518.3   6.284 4.37e-10 ***\nage            233.7      185.7   1.259    0.208    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 43360 on 1437 degrees of freedom\nMultiple R-squared:  0.001102,\tAdjusted R-squared:  0.0004064 \nF-statistic: 1.585 on 1 and 1437 DF,  p-value: 0.2083\n```\n\n\n:::\n:::\n\n\nFinally, we might predict at a new X value. First, define the data at which to make the prediction: a person age 47.\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- tibble(age = 47)\n```\n:::\n\n\nPredict for that subgroup\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(model, newdata = to_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n58228.57 \n```\n\n\n:::\n:::\n\n\nTo review, our model **pooled information**:\n\n- People of all ages contributed to `model`\n- Then we predicted at a single age\n\n### Practice question\n\nBelow is the line fit to the population data. Suppose we want to learn $\\text{E}(\\log(Y)\\mid X = 30)$.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 420 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (5): year, age, meanlog, sdlog, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](5_linear_regression_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n1. Why might this model make a misleading estimate?\n2. Why might the model still be useful?\n\n## Additive vs interactive models\n\nBelow, we visualize two models: one for male and one for female respondents.\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_linear_regression_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nThere are two equivalent ways to describe these two models. The first is by thinking of them as separate linear regressions.\n\n$$\n\\begin{aligned}\n\\text{E}(Y\\mid X, \\text{Female}) &= \\beta_0^\\text{Female} + \\beta_1^\\text{Female}\\times \\text{Age} \\\\\n\\text{E}(Y\\mid X, \\text{Male}) &= \\beta_0^\\text{Male} + \\beta_1^\\text{Male}\\times \\text{Age} \\\\\n\\end{aligned}\n$$\n\nThe second way is to think of them as one pooled linear regression that **interacts** age and sex to allow the slope on age to differ by sex.\n\n$$\\text{E}(Y \\mid X, \\text{Sex}) = \\gamma_0 + \\gamma_1(\\text{Female}) + \\gamma_2(\\text{Age}) + \\gamma_3 (\\text{Age} \\times \\text{Female})$$\n\nNote that both approaches summarize the conditional mean function with 4 parameters. The two approaches are actually equivalent, as you can show with some algebra.\n\n$$\\begin{aligned}\n\\gamma_0 &= \\beta_0^\\text{Male} \n&\\gamma_1 &= \\beta_0^\\text{Female} - \\beta_0^\\text{Male} \\\\\n\\gamma_2 &= \\beta_1^\\text{Male} \n&\\gamma_3 &= \\beta_1^\\text{Female} - \\beta_1^\\text{Male}\n\\end{aligned}$$\n\nBelow, we practice writing an interaction in code. First generate data in 2019 that vary in both `sex` and `age`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_2019 <- simulated |>\n  filter(year == 2019)\n```\n:::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3,204 × 4\n   year   age sex   income\n  <dbl> <dbl> <chr>  <dbl>\n1  2019    41 male  50285.\n2  2019    45 male  31057.\n3  2019    34 male  66166.\n# ℹ 3,201 more rows\n```\n\n\n:::\n:::\n\n\nThe `*` operator allows slopes to differ across groups\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(\n  formula = income ~ sex * age,\n  data = all_2019\n)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_linear_regression_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n## Two models: Additive model in R\n\nThe `+` operator assumes slopes are the same across groups\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(\n  formula = income ~ sex + age,\n  data = all_2019\n)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_linear_regression_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nWhen you have many interactions, the model starts to have lots of terms! This can make interpretation hard. But, you can always use the model to predict any conditional mean you want, even if there are many interactions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(\n  formula = income ~ sex * age * year,\n  data = simulated\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = income ~ sex * age * year, data = simulated)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-81158 -33849 -14946  15839 972817 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)\n(Intercept)       1.387e+06  2.343e+06   0.592    0.554\nsexmale          -1.943e+06  3.117e+06  -0.623    0.533\nage              -6.273e+04  5.760e+04  -1.089    0.276\nyear             -6.680e+02  1.163e+03  -0.574    0.566\nsexmale:age       6.646e+04  7.675e+04   0.866    0.386\nsexmale:year      9.519e+02  1.547e+03   0.615    0.538\nage:year          3.130e+01  2.859e+01   1.095    0.274\nsexmale:age:year -3.247e+01  3.809e+01  -0.852    0.394\n\nResidual standard error: 57790 on 29992 degrees of freedom\nMultiple R-squared:  0.03332,\tAdjusted R-squared:  0.03309 \nF-statistic: 147.7 on 7 and 29992 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe many terms of the model correspond to many slopes in subgroups, as visualized below.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_linear_regression_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n## Penalized Regression\n\nWe close with a more advanced data science topic: penalized regression. We will discuss a method known as ridge regression or L2 penalized regression. This data science approach is a linear model just like OLS, but estimates the coefficients slightly differently.\n\nBefore defining this type of penalized regression, below we show what happens when we use it. Each line is a regression estimated on a different sample from the population. What similarities and differences do you notice between the penalized and unpenalized regressions?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_linear_regression_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nWe could use each approach to predict the mean income among 47-year-olds. Below are those predictions, with one dot from each simulated sample. How do the patterns below align with what you noticed in the graph above?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_linear_regression_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\nThe reason one uses penalized regression is to reduce the sampling variance of estimates, at the cost of some bias (estimates are generally drawn in toward the overall sample mean). To understand how, it is useful to see some math.\n\nOLS chose $\\alpha, \\vec\\beta$ to minimize this function:\n$$\n\\begin{aligned}\n\\underbrace{\\sum_i\\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Sum of Squared Error}\n\\end{aligned}\n$$\nwhere $\\hat{Y}_i = \\hat\\alpha + \\sum_j X_j \\hat\\beta_j$\n\nPenalized (ridge) regression chose $\\alpha, \\vec\\beta$ to minimize this function:\n$$\n\\begin{aligned}\n\\underbrace{\\sum_i\\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Sum of Squared Error} + \\underbrace{\\lambda \\sum_{j} \\beta_j^2}_\\text{Penalty Term}\n\\end{aligned}\n$$\nwhere $\\hat{Y}_i = \\hat\\alpha + \\sum_j X_j \\hat\\beta_j$\n\nThe only difference between the two is that penalized regression seeks to avoid having a large value of $\\sum_j \\beta_j^2$. Thus, it prefers to estimate models with coefficients near zero. In practice, researchers often mean-center covariates and outcomes for penalized regression so that this pulls all estimates toward the overall mean.\n\n### Penalized regression in code\n\nPenalized regression is available through many R packages. Here we illustrate with the `glmnet` package. First, simulate a large sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated <- simulate(n = 1e5)\n```\n:::\n\n\nLoad the package.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n:::\n\n\nCreate a model matrix of predictors\n\n- This converts the predictor data into matrix form\n- Each column will correspond to a coefficient\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(~ age * sex * year, data = simulated)\n```\n:::\n\n\nCreate a vector of the outcomes\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- simulated |> pull(income)\n```\n:::\n\n\nUse the `cv.glmnet` function to call the package. For now, we will leave as a black box how it chooses the penalty parameter $\\lambda$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenalized <- cv.glmnet(\n  x = X,    # model matrix we created\n  y = y,    # outcome vector we created\n  alpha = 0 # penalize sum of beta ^ 2\n)\n```\n:::\n\n\nFinally, make predictions from the model at each observed data point.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat <- predict(\n  penalized,\n  newx = X\n)\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(yhat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   lambda.1se   \n Min.   :60582  \n 1st Qu.:62568  \n Median :65476  \n Mean   :65063  \n 3rd Qu.:67425  \n Max.   :69405  \n```\n\n\n:::\n:::\n\n\nWhen should you use penalized regression? The key reasons are motivated by the original illustration figures above. You should use penalized regression to reduce the variance of your estimates. This may occur in settings where you have many predictors and few observations, for example. However, there is a cost: penalized regression generally yields biased estimates of conditional means, so the model will be wrong on average.\n\nIn future classes, we will discuss data-driven ways to choose among the many statistical and machine-learning approaches to estimate conditional mean functions.",
    "supporting": [
      "5_linear_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}