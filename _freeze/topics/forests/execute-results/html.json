{
  "hash": "de0d4f71e86cd5f7b254705c82b7e078",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Forests\"\nformat: html\n---\n\n::: {.cell}\n\n:::\n\n\n\n\nWe saw previously that a deep tree is a highly flexible learner, but one that may have poor predictive performance due to its high sampling variance. Random forests ([Breiman 2001](https://link.springer.com/article/10.1023/a:1010933404324)) resolve this problem in a simple but powerful way: reduce the variance by averaging the predictions from many trees. The forest is the average of the trees.\n\nIf one simply estimated a regression tree many times on the same data, every tree would be the same. Instead, each time a random forest grows a tree it proceeds by:\n\n1) bootstrap a sample $n$ of the $n$ observations chosen with replacement\n2) randomly sample some number $m$ of the variables to consider for splitting\n\nThere is an art to selection of the tuning parameter $m$, as well as the parameters of the tree-growing algorithm. But most packages can select these tuning parameters automatically. The more trees you grow, the less the forest-based predictions will be sensitive to the stochastic variability that comes from the random sampling of data for each tree.\n\n### Illustration with bagged forest\n\nTo illustrate, we generate data by the same process as on the [trees](trees.qmd) page.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntrue_conditional_mean <- tibble(z = F, x = seq(0,1,.001)) |>\n  bind_rows(tibble(z = T, x = seq(0,1,.001))) |>\n  mutate(mu = z * plogis(10 * (x - .5)))\nsimulate <- function(sample_size) {\n  tibble(z = F, x = seq(0,1,.001)) |>\n    bind_rows(tibble(z = T, x = seq(0,1,.001))) |>\n    mutate(mu = z * plogis(10 * (x - .5))) |>\n    slice_sample(n = sample_size, replace = T) |>\n    mutate(y = mu + rnorm(n(), sd = .1))\n}\nsimulated_data <- simulate(1000)\n```\n:::\n\n\n\n\nFor illustration, we will first consider a simple version of random forest that is a bagging estimator: all predictors are included in every tree and variance is created through bagging, or **b**ootstrap **aggregating**. The code below builds intuition, and the code later using the `regression_forest` function from the `grf` package is one way we would actually recommend learning a forest in practice.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_estimates <- foreach(tree_index = 1:100, .combine = \"rbind\") %do% {\n  # Draw a bootstrap sample of the data\n  simulated_data_star <- simulated_data |>\n    slice_sample(prop = 1, replace = T)\n  # Learn the tree\n  rpart.out <- rpart(\n    y ~ x + z, data = simulated_data_star, \n    # Set tuning parameters to grow a deep tree\n    control = rpart.control(minsplit = 2, cp = 0, maxdepth = 4)\n  )\n  # Define data to predict\n  to_predict <- tibble(z = F, x = seq(0,1,.001)) |>\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n  # Make predictions\n  predicted <- to_predict |>\n    mutate(\n      yhat = predict(rpart.out, newdata = to_predict),\n      tree_index = tree_index\n    )\n  return(predicted)\n}\n```\n:::\n\n\n\n\nWe can then aggregate the tree estimates into a forest prediction by averaging over trees.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforest_estimate <- tree_estimates |>\n  group_by(z,x) |>\n  summarize(yhat = mean(yhat), .groups = \"drop\")\n```\n:::\n\n\n\n\nThe forest is very good at approximating the true conditional mean.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np_no_points <- true_conditional_mean |>\n  ggplot(aes(x = x, color = z, y = mu)) +\n  geom_line(linetype = \"dashed\", size = 1.2) +\n  labs(\n    x = \"Numeric Predictor X\",\n    y = \"Numeric Outcome Y\",\n    color = \"Binary Predictor Z\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\np_no_points +\n  geom_line(\n    data = forest_estimate,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are forest predictions.\\nDashed lines are the true conditional mean.\")\n```\n\n::: {.cell-output-display}\n![](forests_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n## Your turn: A random forest with `grf`\n\nIn practice, it is helpful to work with a function that can choose the tuning parameters of the forest for you. One such function is the `regression_forest()` function in the [`grf`](https://grf-labs.github.io/grf/) package.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(grf)\n```\n:::\n\n\n\n\nTo illustrate its use, we first produce a matrix `X` of predictors and a vector `Y` of outcome values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(~ x + z, data = simulated_data)\nY <- simulated_data |> pull(y)\n```\n:::\n\n\n\n\nWe then estimate the forest with the `regression_forest()` function, here using the `tune.parameters = \"all\"` argument to allow automated tuning of all parameters.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforest <- regression_forest(\n  X = X, Y = Y, tune.parameters = \"all\"\n)\n```\n:::\n\n\n\n\nWe can extract one tree from the forest with the `get_tree()` function and then visualize with the `plot()` function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_tree <- get_tree(forest, index = 1)\nplot(first_tree)\n```\n:::\n\n\n\n\nTo predict in a new dataset requires a new X matrix,\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- tibble(z = F, x = seq(0,1,.001)) |>\n    bind_rows(tibble(z = T, x = seq(0,1,.001)))\n\nX_to_predict <- model.matrix(~ x + z, data = to_predict)\n```\n:::\n\n\n\n\nwhich can then be used to make predictions.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforest_predicted <- to_predict |>\n  mutate(\n    yhat = predict(forest, newdata = X_to_predict) |> \n      pull(predictions)\n  )\n```\n:::\n\n\n\n\nWhen we visualize, we see that the forest from the package is also a good approximator of the conditional mean function. It is possible that the bias of this estimated forest arises from tuning parameters that did not grow sufficiently deep trees.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np_no_points +\n  geom_line(\n    data = forest_predicted,\n    aes(y = yhat)\n  ) +\n  ggtitle(\"Solid lines are grf::regression_forest() predictions.\\nDashed lines are the true conditional mean.\")\n```\n\n::: {.cell-output-display}\n![](forests_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\nOnce you have learned a forest yourself, you might try a regression forest using the [`baseball_population.csv`](https://ilundberg.github.io/soc212b/data/baseball_population.csv) data or another dataset of your choosing.\n\n## Forests as adaptive nearest neighbors\n\nA regression tree can be interpreted as an adaptive nearest-neighbor estimator: the prediction at predictor value $\\vec{x}$ is the average outcome of all its neighbors, where neighbors are defined as all sampled data points that fall in the same leaf as $\\vec{x}$. The estimator is adaptive because the definition of the neighborhood around $\\vec{x}$ was learned from the data.\n\nRandom forests can likewise be interpreted as weighted adaptive nearest-neighbor estimators. For each unit $i$, the predicted value is the average outcome of all other units where each unit $j$ is weighted by the frequency with which it falls in the same leaf as unit $i$. Seeing forest-based predictions as a weighted average of other units' outcomes is a powerful perspective that has led to new advances in forests for uses that go beyond standard regression [(Athey, Tibshirani, \\& Wager 2019)](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full).\n\n## Forests for causal inference\n\nTBD\n\n## What to read\n\nTo read more on trees, see Ch 17 of [Efron \\& Hastie (2016)](https://hastie.su.domains/CASI/).",
    "supporting": [
      "forests_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}