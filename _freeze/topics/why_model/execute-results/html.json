{
  "hash": "ed4aedb7966e4c1ddbf8c842a0ba3a46",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Why model?\"\nformat: \n  html:\n    fig-height: 3\n---\n\n\n\n\n> Covered Feb 11. Here are [slides](../slides/lec11a_whymodel/lec11a_whymodel.pdf). Lecture for today also includes the next page.\n\n<!-- > The reading with this class is [Berk 2020 Ch 1](https://link.springer.com/book/10.1007/978-3-030-40189-4) p. 1--5, stopping at paragraph ending \"...is nonlinear.\" Then p. 14--17 \"Model misspecification...\" through \"...will always be in play.\" -->\n\nThis page will help you\n\n- explain the curse of dimensionality\n- recognize the possible futility of nonparametric estimation\n\nIt motivates the **model-based estimation** that we will use in the next part of the course.\n\n## A motivating example\n\nSociologists who study household income inequality often focus on two mechanisms. First, incomes vary across individuals. Second, individuals pool together into households. The tendency of high-income individuals to marry other high-income individuals is thus an important process through which household income inequality arises.^[[Mare 1991](https://doi.org/10.2307/2095670), [Schwartz 2013](https://doi.org/10.1146/annurev-soc-071312-145544)]\n\nThis page asks a question about the pooling of people together into households: To what degree does finishing college increase the probability of having a spouse who finished college?\n\n## Data\n\nWe use data from the [National Longitudinal Survey of Youth 1997](https://www.bls.gov/nls/nlsy97.htm) (NLSY97). The NLSY97 is a probability sample of U.S. non-institutional civilian youths age 12--16 on Dec 31 1996 ($n$ = 8,984). Sampled individuals were surveyed annually 1997--2011, then biennially.\n\nIf you would like to work with the data, you should first prepare your computer with some files from us:\n\n* set your working directory where you will be working\n* download two supporting files from us\n     1. [`nlsy97.NLSY97`](https://drive.google.com/file/d/1YcyaC1R5u9_d0-AX4ivqDiU0bRnEsAsJ/view?usp=sharing) is a tagset file containing the variable names\n     2. [`prepare_nlsy97.R`](https://drive.google.com/file/d/1fUdlhEsGGC0shREvu4znPPmJjtNi--Ko/view?usp=sharing) is an R script to prepare the data\n\nNow go to the data distributor\n\n1. [Register](https://nlsinfo.org/investigator/pages/register) with the survey\n2. [Log in](https://nlsinfo.org/investigator/pages/login) to the NLS Investigator\n3. Choose the NLSY97 study\n4. Upload the tagset [`nlsy97.NLSY97`](https://drive.google.com/file/d/1YcyaC1R5u9_d0-AX4ivqDiU0bRnEsAsJ/view?usp=sharing) that you downloaded from us\n5. In the Investigator, download the data. Type to change the file name from `default` to `nlsy97`\n6. Unzip the file. Drag `nlsy97.dat` into the folder you will work in\n7. In your R console, run the line of code below\n     * this will take about 30 seconds to run\n     * you will need these R packages: `tidyverse` and `Amelia`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"prepare_nlsy97.R\")\n```\n:::\n\n\n\nIn the future, you can now load the data with \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- readRDS(\"d.RDS\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n## A DAG with one confounder\n\nTo draw a DAG, we first define our treatment and outcome variables\n\n- Treatment $A$: Finished BA by age 25\n- Outcome $Y$: Spouse or partner at age 30--40 holds a BA\n     * 0 if no spouse or partner, or partner with no BA\n     * 1 if spouse or partner holds a BA\n     \nWith these two variables, we ask whether there are any other variables that are common ancestors of these two. One possibility is the sex of the respondent, $L$.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nIf this were the only confounder, our adjustment procedure would be simple:\n\n1) Estimate within subgroups defined by \\{sex\\}\n2) Aggregate over the subgroups\n\nThe code below estimates within the subgroups\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nybar_in_subgroups <- d |>\n  # Group by confounders and treatment\n  group_by(sex, a) |>\n  # Summarize mean outcomes and nber of cases\n  summarize(ybar = mean(y),\n            n = n(),\n            .groups = \"drop\") |>\n  pivot_wider(names_from = a,\n              values_from = c(\"ybar\",\"n\")) |>\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  sex    ybar_college ybar_no_college n_college n_no_college\n  <chr>         <dbl>           <dbl>     <int>        <int>\n1 Female        0.467           0.102       896         2953\n2 Male          0.614           0.174       637         3285\n```\n\n\n:::\n:::\n\n\n\n\nThen we can aggregate over the subgroups, weighted by size.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nybar_in_subgroups |>\n  mutate(\n    conditional_effect = ybar_college - ybar_no_college,\n    n_in_stratum = n_college + n_no_college\n  ) |>\n  select(sex, conditional_effect, n_in_stratum) |>\n  summarize(\n    population_average_effect = weighted.mean(\n      conditional_effect,\n      w = n_in_stratum\n    )\n  ) |>\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  population_average_effect\n                      <dbl>\n1                     0.403\n```\n\n\n:::\n:::\n\n\n\n\nThe visualization below seeks to build intuitiomn for what we have just done. We split the population into two subgroups (left and right of the solid line). Within each subgroup, we took the mean outcome among those with and without college degrees (above and below dashed line), taking the difference as the effect of a college degree. Then we took the average across the left and right subgroups.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n## With two confounders\n\nSex may not be the only confounder. What if race also shapes access to college and the probability of having a college-educated spouse at age 35?\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\nOur procedure would be analogous to the above:\n\n1) Estimate effects within subgroups defined by \\{sex, race\\}\n2) Aggregate over subgroups\n\nThe only difference is that now there are 6 subgroups (3 race categories $\\times$ 2 sex categories). Moving from one to two confounders multiplies the number of subgroups.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\nBelow are the unadjusted estimates and the estimates adjusted for sex and race.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n## With three confounders\n\nSex and race are not likely to be a sufficient adjustment set. Our DAG should really include at least three confounders:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\nProceeding as above, we can make a nonparametric estimate:\n\n1) Estimate effects within subgroups defined by \\{race,sex, mom education\\}\n2) Aggregate over subgroups\n\nThe number of subgroups is starting to get quite large!\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n## With four confounders\n\nNow we suppose that dad's education is also a confounder.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\nThe procedure still holds:\n\n1) Estimate effects within subgroups defined by \\{race,sex, mom education, dad education\\}\n2) Aggregate over subgroups\n\nbut the number of subgroups is now *very* many.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nNow we have a problem. Some subgroups (highlighted in red) are unpopulated: there are no sampled units in these subgroups. The problem of unpopulated cells happens because we have many confounders, so that there are a huge number of subgroups. This problem is known as the curse of dimensionality.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n## Curse of dimensionality: Unpopulated cells\n\nThe curse of dimensionality is that the number of subgroups grows multiplicatively with the number of predictors. Nonparametric estimation (estimation by subgroup means) quickly becomes infeasible as the number of subgroups explodes.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nIn fact, in our example, **4.2%** of the sample is in a subgroup where there are either zero people with college degrees or zero people without college degrees.\n\nTo make conditional exchangeability plausible, social scientists often have to draw a DAG with numerous confounding variables.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](why_model_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nBut when we add tons of variables, we have an ever-increasing problem of empty cells. With the above DAG, **100%** of the sample falls in a cell where either everyone is treated or everyone is untreated.\n\n## Models to the rescue\n\nIn many real research settings, the curse of dimensionality makes it impossible to estimate nonparametrically by subgroup means. With more than a few confounders, there are simply too many subgroups and not enough sampled cases to populate them.\n\nA **model** is the solution to this problem. A model is a statistical approach to pool information across units who fall in different subgroups, thereby allowing us to make inference even in subgroups where no units are observed. The next page introduces models through a simple descriptive example, and then we will return to causal examples to see how models can help us to answer causal questions.\n\nIf you would like to read more, you might see [Hernán \\& Robins Ch 11](https://miguelhernan.org/whatifbook).\n     ",
    "supporting": [
      "why_model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}